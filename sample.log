🟩 Transformers version: 4.57.1
🟩 Numpy version: 2.1.2
🟩 BitsAndBytes version: 0.48.1
🟩 loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/tokenizer.model
🟩 loading file added_tokens.json from cache at None
🟩 loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/special_tokens_map.json
🟩 loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/tokenizer_config.json
🟩 loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/tokenizer.json
🟩 loading file chat_template.jinja from cache at None
🟩 LlamaTokenizerの初期化開始 vocab_file=/root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/tokenizer.model unk_token=<unk> bos_token=<s> eos_token=</s> pad_token=None legacy=True add_prefix_space=True
🟩 SentencePieceProcessorの取得 from_slow=False legacy=True
🟩 語彙の取得開始
🟩 語彙の取得完了 vocab_size=32000
🟩 語彙の取得開始
🟩 語彙の取得完了 vocab_size=32000
🟩 LlamaTokenizerの初期化完了
🟨 The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
🟩 loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/config.json
🟩 Model config MixtralConfig {
  "architectures": [
    "MixtralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "head_dim": null,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mixtral",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": false,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.02,
  "router_jitter_noise": 0.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "vocab_size": 32000
}

🟦 Multi-backend validation successful.
🟩 loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/model.safetensors.index.json
🟩 Instantiating MixtralForCausalLM model under default dtype torch.bfloat16.
🟩 MixtralForCausalLMの初期化開始
🟩 Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

🟩 MixtralModelの初期化開始 config.vocab_size=32000, config.hidden_size=4096, config.num_hidden_layers=32, config.pad_token_id=None
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=0
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=0
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=1
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=1
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=2
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=2
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=3
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=3
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=4
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=4
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=5
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=5
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=6
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=6
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=7
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=7
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=8
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=8
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=9
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=9
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=10
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=10
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=11
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=11
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=12
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=12
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=13
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=13
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=14
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=14
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=15
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=15
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=16
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=16
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=17
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=17
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=18
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=18
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=19
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=19
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=20
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=20
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=21
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=21
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=22
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=22
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=23
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=23
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=24
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=24
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=25
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=25
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=26
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=26
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=27
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=27
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=28
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=28
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=29
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=29
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=30
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=30
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=31
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=31
🟦 self.head_dim=128
🟦 self.num_key_value_groups=4
🟦 self.scaling=0.08838834764831845
🟦 self.attention_dropout=0.0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRotaryEmbeddingの初期化開始 config.max_position_embeddings=32768
🟦 self.rope_type='default'
🟦 inv_freq.shape=torch.Size([64]), self.attention_scaling=1.0
🟩 MixtralRotaryEmbeddingの初期化完了
🟩 MixtralModelの初期化完了
🟩 MixtralForCausalLMの初期化完了
🟩 target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization
🟦 Multi-backend validation successful.
🟩 loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/generation_config.json
🟩 Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

🟩 Could not locate the custom_generate/generate.py inside mistralai/Mixtral-8x7B-v0.1.
🟩 入力プロンプト text='Hello'
🟩 トークン化開始 text=Hello legacy=True add_prefix_space=True
🟩 _tokenizeの開始 text=Hello legacy=True
🟩 _tokenizeの完了（legacy） tokens=['▁Hello']
🟩 トークン化完了（legacy） tokens=['▁Hello']
🟩 _convert_token_to_idの開始 token=▁Hello
🟩 _convert_token_to_idの完了 token=▁Hello id=22557
🟩 特殊トークン付き入力の構築開始 token_ids_0=[] token_ids_1=None
🟩 特殊トークン付き入力の構築完了 output=[1]
🟩 特殊トークン付き入力の構築開始 token_ids_0=[22557] token_ids_1=None
🟩 特殊トークン付き入力の構築完了 output=[1, 22557]
🟩 シーケンスからトークンタイプIDの作成開始 token_ids_0=[22557] token_ids_1=None
🟩 シーケンスからトークンタイプIDの作成完了 output=[0, 0]
🟨 Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
🟩 MixtralForCausalLMの順伝播開始 torch.Size([1, 2]) torch.Size([1, 2]) torch.Size([1, 2]) past_key_values is not None=True None None use_cache=True output_router_logits=None torch.Size([2]) logits_to_keep=1
🟦 output_router_logits=False
🟩 MixtralModelの順伝播開始 input_ids.shape=torch.Size([1, 2]) attention_mask.shape=torch.Size([1, 2]) position_ids.shape=torch.Size([1, 2]) past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]) None use_cache=True cache_position=tensor([0, 1], device='cuda:0')
🟦 入力の埋め込みを取得 inputs_embeds.shape=torch.Size([1, 2, 4096])
🟦 マスク関数を選択 mask_function=<function create_causal_mask at 0x7637441ade40>
🟦 因果マスクを作成 causal_mask=None
🟩 MixtralRotaryEmbeddingの順伝播開始 x.shape=torch.Size([1, 2, 4096]) position_ids.shape=torch.Size([1, 2])
🟦 inv_freq_expanded.shape=torch.Size([1, 64, 1])
🟦 position_ids_expanded.shape=torch.Size([1, 1, 2])
🟦 freqs.shape=torch.Size([1, 2, 64])
🟦 emb.shape=torch.Size([1, 2, 128])
🟩 MixtralRotaryEmbeddingの順伝播完了 cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128])
🟦 位置埋め込みを作成 position_embeddings[0].shape=torch.Size([1, 2, 128]) position_embeddings[1].shape=torch.Size([1, 2, 128])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([4, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([4, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([4, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([4, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([3, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([1, 1], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([2, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([4, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([4, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([4, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([3, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([0, 0], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([2, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([4, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([3, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0, 0], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([2, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([4, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([4, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([4, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([4, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([4, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([3, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0, 1], device='cuda:0') top_x=tensor([1, 0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([2, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([3, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([0, 1], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([2, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([2, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([0, 1], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([2, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0, 1], device='cuda:0') top_x=tensor([1, 0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([2, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([4, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([4, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([2, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([0, 1], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([2, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0, 1], device='cuda:0') top_x=tensor([1, 0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([2, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([3, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([0, 0], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([2, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([3, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0, 0], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([2, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([3, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0, 0], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([2, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([4, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([4, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([4, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([3, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([0, 1], device='cuda:0') top_x=tensor([1, 0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([2, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([3, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([0, 1], device='cuda:0') top_x=tensor([1, 0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([2, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([3, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0, 1], device='cuda:0') top_x=tensor([1, 0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([2, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟦 hidden_shape=(1, 2, -1, 128)
🟦 query_states.shape=torch.Size([1, 32, 2, 128])
🟦 key_states.shape=torch.Size([1, 8, 2, 128])
🟦 value_states.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 32, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 32, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟦 x1を取得 x1.shape=torch.Size([1, 8, 2, 64])
🟦 x2を取得 x2.shape=torch.Size([1, 8, 2, 64])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟦 過去のキー・バリューを更新 key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
🟦 アテンションの実装を選択 attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
🟦 アテンションを計算開始 query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
🟦 アテンションを計算完了 attn_output.shape=torch.Size([1, 2, 32, 128]) None
🟦 アテンション出力を整形 attn_output.shape=torch.Size([1, 2, 4096])
🟦 出力のプロジェクションを適用 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 hidden_statesを整形 hidden_states.shape=torch.Size([2, 4096])
🟦 ルーターのロジットを計算 router_logits.shape=torch.Size([2, 8])
🟦 ルーティングの重みを計算 routing_weights.shape=torch.Size([2, 8])
🟦 上位2つのエキスパートを選択 routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
🟦 ルーティングの重みを正規化 routing_weights.shape=torch.Size([2, 2])
🟦 最終的なhidden_statesを初期化 final_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートマスクを作成 expert_mask.shape=torch.Size([8, 2, 2])
🟦 エキスパートヒットを計算 expert_hit.shape=torch.Size([3, 1])
🟦 マスクの値を取得 expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([0, 0], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([2, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 マスクの値を取得 expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
🟦 エキスパートの現在のhidden_statesを取得 current_state.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟦 エキスパートの順伝播を実行 current_hidden_states.shape=torch.Size([1, 4096])
🟦 最終的なhidden_statesに加算 final_hidden_states.shape=torch.Size([2, 4096])
🟦 hidden_statesの形状を元に戻す final_hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟦 残差接続を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 分散を計算 variance.shape=torch.Size([1, 2, 1])
🟦 正規化を適用 hidden_states.shape=torch.Size([1, 2, 4096])
🟦 ゲインを適用 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralModelの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralForCausalLMの順伝播完了 logits.shape=torch.Size([1, 1, 32000])
🟩 トークン列の文字列変換開始 tokens=['▁Hello', ','] legacy=True add_prefix_space=True
🟦 最初のトークンから接頭辞スペースを削除 tokens=['Hello', ',']
🟦 特殊トークンではないのでバッファに追加 current_sub_tokens=['Hello']
🟦 特殊トークンではないのでバッファに追加 current_sub_tokens=['Hello', ',']
🟦 最後のバッファ current_sub_tokens=['Hello', ','] をデコードし集約 out_string='Hello,'
🟩 トークン列の文字列変換完了 out_string='Hello,'
🟩 生成されたテキスト Hello,
🟨 Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
