🟩 Transformers version: 4.57.1
🟩 Numpy version: 2.1.2
🟩 BitsAndBytes version: 0.48.1
🟩 loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/tokenizer.model
🟩 loading file added_tokens.json from cache at None
🟩 loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/special_tokens_map.json
🟩 loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/tokenizer_config.json
🟩 loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/tokenizer.json
🟩 loading file chat_template.jinja from cache at None
🟩 LlamaTokenizerの初期化開始 vocab_file=/root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/tokenizer.model unk_token=<unk> bos_token=<s> eos_token=</s> pad_token=None legacy=True add_prefix_space=True
🟩 SentencePieceProcessorの取得 from_slow=False legacy=True
🟩 語彙の取得開始
🟩 語彙の取得完了 vocab_size=32000
🟩 語彙の取得開始
🟩 語彙の取得完了 vocab_size=32000
🟩 LlamaTokenizerの初期化完了
🟨 The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
🟩 loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/config.json
🟩 Model config MixtralConfig {
  "architectures": [
    "MixtralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "head_dim": null,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mixtral",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": false,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.02,
  "router_jitter_noise": 0.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "vocab_size": 32000
}

🟦 Multi-backend validation successful.
🟩 loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/model.safetensors.index.json
🟩 Instantiating MixtralForCausalLM model under default dtype torch.bfloat16.
🟩 MixtralForCausalLMの初期化開始
🟩 Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

🟩 MixtralModelの初期化開始
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=0
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=0
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=1
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=1
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=2
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=2
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=3
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=3
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=4
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=4
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=5
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=5
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=6
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=6
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=7
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=7
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=8
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=8
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=9
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=9
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=10
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=10
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=11
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=11
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=12
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=12
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=13
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=13
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=14
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=14
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=15
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=15
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=16
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=16
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=17
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=17
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=18
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=18
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=19
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=19
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=20
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=20
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=21
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=21
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=22
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=22
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=23
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=23
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=24
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=24
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=25
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=25
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=26
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=26
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=27
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=27
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=28
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=28
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=29
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=29
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=30
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=30
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralDecoderLayerの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=31
🟩 MixtralAttentionの初期化開始 config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=31
🟩 MixtralAttentionの初期化完了
🟩 MixtralSparseMoeBlockの初期化開始 config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralBlockSparseTop2MLPの初期化開始 config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
🟩 MixtralBlockSparseTop2MLPの初期化完了
🟩 MixtralSparseMoeBlockの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralDecoderLayerの初期化完了
🟩 MixtralRMSNormの初期化開始 hidden_size=4096, eps=1e-05
🟩 MixtralRMSNormの初期化完了
🟩 MixtralRotaryEmbeddingの初期化開始 config.max_position_embeddings=32768
🟩 MixtralRotaryEmbeddingの初期化完了
🟩 MixtralModelの初期化完了
🟩 MixtralForCausalLMの初期化完了
🟩 target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization
🟦 Multi-backend validation successful.
🟩 loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/generation_config.json
🟩 Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

🟩 Could not locate the custom_generate/generate.py inside mistralai/Mixtral-8x7B-v0.1.
🟩 トークン化開始 text=Hello legacy=True add_prefix_space=True
🟩 _tokenizeの開始 text=Hello legacy=True
🟩 _tokenizeの完了（legacy） tokens=['▁Hello']
🟩 トークン化完了（legacy） tokens=['▁Hello']
🟩 _convert_token_to_idの開始 token=▁Hello
🟩 _convert_token_to_idの完了 token=▁Hello id=22557
🟩 特殊トークン付き入力の構築開始 token_ids_0=[] token_ids_1=None
🟩 特殊トークン付き入力の構築完了 output=[1]
🟩 特殊トークン付き入力の構築開始 token_ids_0=[22557] token_ids_1=None
🟩 特殊トークン付き入力の構築完了 output=[1, 22557]
🟩 シーケンスからトークンタイプIDの作成開始 token_ids_0=[22557] token_ids_1=None
🟩 シーケンスからトークンタイプIDの作成完了 output=[0, 0]
🟨 Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
🟩 MixtralForCausalLMの順伝播開始 torch.Size([1, 2]) torch.Size([1, 2]) torch.Size([1, 2]) past_key_values is not None=True None None use_cache=True output_router_logits=None torch.Size([2]) logits_to_keep=1
🟩 MixtralModelの順伝播開始 torch.Size([1, 2]) torch.Size([1, 2]) torch.Size([1, 2]) past_key_values is not None=True None use_cache=True torch.Size([2])
🟩 MixtralRotaryEmbeddingの順伝播開始 x.shape=torch.Size([1, 2, 4096]) position_ids.shape=torch.Size([1, 2])
🟩 MixtralRotaryEmbeddingの順伝播完了 cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟨 Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
🟩 MixtralForCausalLMの順伝播開始 torch.Size([1, 2]) torch.Size([1, 2]) torch.Size([1, 2]) past_key_values is not None=True None None use_cache=True output_router_logits=None torch.Size([2]) logits_to_keep=1
🟩 MixtralModelの順伝播開始 torch.Size([1, 2]) torch.Size([1, 2]) torch.Size([1, 2]) past_key_values is not None=True None use_cache=True torch.Size([2])
🟩 MixtralRotaryEmbeddingの順伝播開始 x.shape=torch.Size([1, 2, 4096]) position_ids.shape=torch.Size([1, 2])
🟩 MixtralRotaryEmbeddingの順伝播完了 cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralDecoderLayerの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralAttentionの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
🟩 apply_rotary_pos_embの開始 q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
🟩 rotate_halfの開始 x.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 32, 2, 128])
🟩 rotate_halfの開始 x.shape=torch.Size([1, 8, 2, 128])
🟩 rotate_halfの完了 res.shape=torch.Size([1, 8, 2, 128])
🟩 apply_rotary_pos_embの完了 q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
🟩 MixtralAttentionの順伝播完了 attn_output.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralSparseMoeBlockの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([2, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([2, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([2, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([2, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播開始 hidden_states.shape=torch.Size([1, 4096])
🟦 ゲートを計算 gate.shape=torch.Size([1, 14336])
🟦 アッププロジェクションを計算 up.shape=torch.Size([1, 14336])
🟦 ダウンプロジェクションを計算 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralBlockSparseTop2MLPの順伝播完了 current_hidden_states.shape=torch.Size([1, 4096])
🟩 MixtralSparseMoeBlockの順伝播完了 final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
🟩 MixtralDecoderLayerの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播開始 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralRMSNormの順伝播完了 res.shape=torch.Size([1, 2, 4096])
🟩 MixtralModelの順伝播完了 hidden_states.shape=torch.Size([1, 2, 4096])
🟩 MixtralForCausalLMの順伝播完了 logits.shape=torch.Size([1, 1, 32000])
🟩 トークン列の文字列変換開始 tokens=['▁Hello', ','] legacy=True add_prefix_space=True
🟩 トークン列の文字列変換完了 string=Hello,
🟨 Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
🟨 Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
🟨 Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
