ğŸŸ© Transformers version: 4.57.1
ğŸŸ© Numpy version: 2.1.2
ğŸŸ© BitsAndBytes version: 0.48.1
ğŸŸ© loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/tokenizer.model
ğŸŸ© loading file added_tokens.json from cache at None
ğŸŸ© loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/special_tokens_map.json
ğŸŸ© loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/tokenizer_config.json
ğŸŸ© loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/tokenizer.json
ğŸŸ© loading file chat_template.jinja from cache at None
ğŸŸ© LlamaTokenizerã®åˆæœŸåŒ–é–‹å§‹ vocab_file=/root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/tokenizer.model unk_token=<unk> bos_token=<s> eos_token=</s> pad_token=None legacy=True add_prefix_space=True
ğŸŸ© SentencePieceProcessorã®å–å¾— from_slow=False legacy=True
ğŸŸ© èªå½™ã®å–å¾—é–‹å§‹
ğŸŸ© èªå½™ã®å–å¾—å®Œäº† vocab_size=32000
ğŸŸ© èªå½™ã®å–å¾—é–‹å§‹
ğŸŸ© èªå½™ã®å–å¾—å®Œäº† vocab_size=32000
ğŸŸ© LlamaTokenizerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¨ The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
ğŸŸ© loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/config.json
ğŸŸ© Model config MixtralConfig {
  "architectures": [
    "MixtralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "head_dim": null,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mixtral",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": false,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.02,
  "router_jitter_noise": 0.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "vocab_size": 32000
}

ğŸŸ¦ Multi-backend validation successful.
ğŸŸ© loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/model.safetensors.index.json
ğŸŸ© Instantiating MixtralForCausalLM model under default dtype torch.bfloat16.
ğŸŸ© MixtralForCausalLMã®åˆæœŸåŒ–é–‹å§‹
ğŸŸ© Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

ğŸŸ© MixtralModelã®åˆæœŸåŒ–é–‹å§‹ config.vocab_size=32000, config.hidden_size=4096, config.num_hidden_layers=32, config.pad_token_id=None
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=0
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=1
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=1
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=2
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=2
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=3
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=3
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=4
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=4
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=5
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=5
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=6
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=6
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=7
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=7
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=8
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=8
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=9
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=9
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=10
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=10
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=11
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=11
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=12
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=12
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=13
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=13
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=14
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=14
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=15
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=15
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=16
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=16
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=17
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=17
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=18
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=18
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=19
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=19
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=20
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=20
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=21
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=21
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=22
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=22
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=23
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=23
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=24
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=24
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=25
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=25
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=26
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=26
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=27
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=27
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=28
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=28
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=29
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=29
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=30
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=30
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=31
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=31
ğŸŸ¦ self.head_dim=128
ğŸŸ¦ self.num_key_value_groups=4
ğŸŸ¦ self.scaling=0.08838834764831845
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRotaryEmbeddingã®åˆæœŸåŒ–é–‹å§‹ config.max_position_embeddings=32768
ğŸŸ¦ self.rope_type='default'
ğŸŸ¦ inv_freq.shape=torch.Size([64]), self.attention_scaling=1.0
ğŸŸ© MixtralRotaryEmbeddingã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralModelã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralForCausalLMã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization
ğŸŸ¦ Multi-backend validation successful.
ğŸŸ© loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/generation_config.json
ğŸŸ© Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

ğŸŸ© Could not locate the custom_generate/generate.py inside mistralai/Mixtral-8x7B-v0.1.
ğŸŸ© å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ text='Hello'
ğŸŸ© ãƒˆãƒ¼ã‚¯ãƒ³åŒ–é–‹å§‹ text=Hello legacy=True add_prefix_space=True
ğŸŸ© _tokenizeã®é–‹å§‹ text=Hello legacy=True
ğŸŸ© _tokenizeã®å®Œäº†ï¼ˆlegacyï¼‰ tokens=['â–Hello']
ğŸŸ© ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å®Œäº†ï¼ˆlegacyï¼‰ tokens=['â–Hello']
ğŸŸ© _convert_token_to_idã®é–‹å§‹ token=â–Hello
ğŸŸ© _convert_token_to_idã®å®Œäº† token=â–Hello id=22557
ğŸŸ© ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ä»˜ãå…¥åŠ›ã®æ§‹ç¯‰é–‹å§‹ token_ids_0=[] token_ids_1=None
ğŸŸ© ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ä»˜ãå…¥åŠ›ã®æ§‹ç¯‰å®Œäº† output=[1]
ğŸŸ© ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ä»˜ãå…¥åŠ›ã®æ§‹ç¯‰é–‹å§‹ token_ids_0=[22557] token_ids_1=None
ğŸŸ© ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ä»˜ãå…¥åŠ›ã®æ§‹ç¯‰å®Œäº† output=[1, 22557]
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚¿ã‚¤ãƒ—IDã®ä½œæˆé–‹å§‹ token_ids_0=[22557] token_ids_1=None
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚¿ã‚¤ãƒ—IDã®ä½œæˆå®Œäº† output=[0, 0]
ğŸŸ¨ Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
ğŸŸ© MixtralForCausalLMã®é †ä¼æ’­é–‹å§‹ torch.Size([1, 2]) torch.Size([1, 2]) torch.Size([1, 2]) past_key_values is not None=True None None use_cache=True output_router_logits=None torch.Size([2]) logits_to_keep=1
ğŸŸ¦ output_router_logits=False
ğŸŸ© MixtralModelã®é †ä¼æ’­é–‹å§‹ input_ids.shape=torch.Size([1, 2]) attention_mask.shape=torch.Size([1, 2]) position_ids.shape=torch.Size([1, 2]) past_key_values=DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer]) None use_cache=True cache_position=tensor([0, 1], device='cuda:0')
ğŸŸ¦ å…¥åŠ›ã®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾— inputs_embeds.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯é–¢æ•°ã‚’é¸æŠ mask_function=<function create_causal_mask at 0x7637441ade40>
ğŸŸ¦ å› æœãƒã‚¹ã‚¯ã‚’ä½œæˆ causal_mask=None
ğŸŸ© MixtralRotaryEmbeddingã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 2, 4096]) position_ids.shape=torch.Size([1, 2])
ğŸŸ¦ inv_freq_expanded.shape=torch.Size([1, 64, 1])
ğŸŸ¦ position_ids_expanded.shape=torch.Size([1, 1, 2])
ğŸŸ¦ freqs.shape=torch.Size([1, 2, 64])
ğŸŸ¦ emb.shape=torch.Size([1, 2, 128])
ğŸŸ© MixtralRotaryEmbeddingã®é †ä¼æ’­å®Œäº† cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128])
ğŸŸ¦ ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’ä½œæˆ position_embeddings[0].shape=torch.Size([1, 2, 128]) position_embeddings[1].shape=torch.Size([1, 2, 128])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([4, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([4, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([4, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([4, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([3, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([1, 1], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([4, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([4, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([4, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([3, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([0, 0], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([4, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([3, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0, 0], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([4, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([4, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([4, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([4, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([4, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([3, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0, 1], device='cuda:0') top_x=tensor([1, 0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([3, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([0, 1], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([2, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([0, 1], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0, 1], device='cuda:0') top_x=tensor([1, 0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([4, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([4, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([2, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([0, 1], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0, 1], device='cuda:0') top_x=tensor([1, 0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([3, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([0, 0], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([3, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0, 0], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([3, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0, 0], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([4, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([4, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([4, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([3, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([0, 1], device='cuda:0') top_x=tensor([1, 0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([3, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(3, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([0, 1], device='cuda:0') top_x=tensor([1, 0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(5, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([3, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(0, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(4, device='cuda:0') idx=tensor([0], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(6, device='cuda:0') idx=tensor([0, 1], device='cuda:0') top_x=tensor([1, 0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ¦ hidden_shape=(1, 2, -1, 128)
ğŸŸ¦ query_states.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ key_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 32, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 32, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ x1ã‚’å–å¾— x1.shape=torch.Size([1, 8, 2, 64])
ğŸŸ¦ x2ã‚’å–å¾— x2.shape=torch.Size([1, 8, 2, 64])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ attention_interface=<function sdpa_attention_forward at 0x7636efd25f80>
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ query_states.shape=torch.Size([1, 32, 2, 128]) key_states.shape=torch.Size([1, 8, 2, 128]) value_states.shape=torch.Size([1, 8, 2, 128]) None self.scaling=0.08838834764831845 sliding_window=None dropout=0.0
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† attn_output.shape=torch.Size([1, 2, 32, 128]) None
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ hidden_statesã‚’æ•´å½¢ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— routing_weights.shape=torch.Size([2, 8])
ğŸŸ¦ ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ routing_weights.shape=torch.Size([2, 2]) selected_experts.shape=torch.Size([2, 2])
ğŸŸ¦ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– routing_weights.shape=torch.Size([2, 2])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ expert_mask.shape=torch.Size([8, 2, 2])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— expert_hit.shape=torch.Size([3, 1])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(1, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([0], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(2, device='cuda:0') idx=tensor([0, 0], device='cuda:0') top_x=tensor([0, 1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— expert_idx[0]=tensor(7, device='cuda:0') idx=tensor([1], device='cuda:0') top_x=tensor([1], device='cuda:0')
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— current_state.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— final_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ final_hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ¦ æ®‹å·®æ¥ç¶šã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ åˆ†æ•£ã‚’è¨ˆç®— variance.shape=torch.Size([1, 2, 1])
ğŸŸ¦ æ­£è¦åŒ–ã‚’é©ç”¨ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¦ ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralModelã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralForCausalLMã®é †ä¼æ’­å®Œäº† logits.shape=torch.Size([1, 1, 32000])
ğŸŸ© ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®æ–‡å­—åˆ—å¤‰æ›é–‹å§‹ tokens=['â–Hello', ','] legacy=True add_prefix_space=True
ğŸŸ¦ æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰æ¥é ­è¾ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤ tokens=['Hello', ',']
ğŸŸ¦ ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã§ã¯ãªã„ã®ã§ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ  current_sub_tokens=['Hello']
ğŸŸ¦ ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã§ã¯ãªã„ã®ã§ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ  current_sub_tokens=['Hello', ',']
ğŸŸ¦ æœ€å¾Œã®ãƒãƒƒãƒ•ã‚¡ current_sub_tokens=['Hello', ','] ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—é›†ç´„ out_string='Hello,'
ğŸŸ© ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®æ–‡å­—åˆ—å¤‰æ›å®Œäº† out_string='Hello,'
ğŸŸ© ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ Hello,
ğŸŸ¨ Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
