ğŸŸ© Transformers version: 4.57.1
ğŸŸ© Numpy version: 2.1.2
ğŸŸ© BitsAndBytes version: 0.48.1
ğŸŸ© loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/tokenizer.model
ğŸŸ© loading file added_tokens.json from cache at None
ğŸŸ© loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/special_tokens_map.json
ğŸŸ© loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/tokenizer_config.json
ğŸŸ© loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/tokenizer.json
ğŸŸ© loading file chat_template.jinja from cache at None
ğŸŸ© LlamaTokenizerã®åˆæœŸåŒ–é–‹å§‹ vocab_file=/root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/tokenizer.model unk_token=<unk> bos_token=<s> eos_token=</s> pad_token=None legacy=True add_prefix_space=True
ğŸŸ© SentencePieceProcessorã®å–å¾— from_slow=False legacy=True
ğŸŸ© èªå½™ã®å–å¾—é–‹å§‹
ğŸŸ© èªå½™ã®å–å¾—å®Œäº† vocab_size=32000
ğŸŸ© èªå½™ã®å–å¾—é–‹å§‹
ğŸŸ© èªå½™ã®å–å¾—å®Œäº† vocab_size=32000
ğŸŸ© LlamaTokenizerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¨ The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
ğŸŸ© loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/config.json
ğŸŸ© Model config MixtralConfig {
  "architectures": [
    "MixtralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "dtype": "bfloat16",
  "eos_token_id": 2,
  "head_dim": null,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mixtral",
  "num_attention_heads": 32,
  "num_experts_per_tok": 2,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "num_local_experts": 8,
  "output_router_logits": false,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "router_aux_loss_coef": 0.02,
  "router_jitter_noise": 0.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "vocab_size": 32000
}

ğŸŸ¦ Multi-backend validation successful.
ğŸŸ© loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/model.safetensors.index.json
ğŸŸ© Instantiating MixtralForCausalLM model under default dtype torch.bfloat16.
ğŸŸ© MixtralForCausalLMã®åˆæœŸåŒ–é–‹å§‹
ğŸŸ© Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

ğŸŸ© MixtralModelã®åˆæœŸåŒ–é–‹å§‹
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=0
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=1
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=1
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=2
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=2
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=3
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=3
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=4
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=4
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=5
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=5
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=6
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=6
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=7
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=7
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=8
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=8
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=9
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=9
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=10
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=10
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=11
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=11
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=12
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=12
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=13
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=13
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=14
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=14
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=15
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=15
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=16
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=16
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=17
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=17
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=18
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=18
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=19
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=19
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=20
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=20
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=21
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=21
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=22
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=22
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=23
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=23
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=24
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=24
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=25
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=25
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=26
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=26
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=27
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=27
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=28
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=28
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=29
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=29
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=30
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=30
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.intermediate_size=14336, layer_idx=31
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ config.hidden_size=4096, config.num_attention_heads=32, config.num_key_value_heads=8, getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=128, config.attention_dropout=0.0, layer_idx=31
ğŸŸ© MixtralAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ config.num_local_experts=8, config.num_experts_per_tok=2, config.hidden_size=4096, config.intermediate_size=14336, config.router_jitter_noise=0.0
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=14336, config.hidden_size=4096, config.hidden_act='silu'
ğŸŸ© MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ hidden_size=4096, eps=1e-05
ğŸŸ© MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralRotaryEmbeddingã®åˆæœŸåŒ–é–‹å§‹ config.max_position_embeddings=32768
ğŸŸ© MixtralRotaryEmbeddingã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralModelã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© MixtralForCausalLMã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization
ğŸŸ¦ Multi-backend validation successful.
ğŸŸ© loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mixtral-8x7B-v0.1/snapshots/fc7ac94680e38d7348cfa806e51218e6273104b0/generation_config.json
ğŸŸ© Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

ğŸŸ© Could not locate the custom_generate/generate.py inside mistralai/Mixtral-8x7B-v0.1.
ğŸŸ© ãƒˆãƒ¼ã‚¯ãƒ³åŒ–é–‹å§‹ text=Hello legacy=True add_prefix_space=True
ğŸŸ© _tokenizeã®é–‹å§‹ text=Hello legacy=True
ğŸŸ© _tokenizeã®å®Œäº†ï¼ˆlegacyï¼‰ tokens=['â–Hello']
ğŸŸ© ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å®Œäº†ï¼ˆlegacyï¼‰ tokens=['â–Hello']
ğŸŸ© _convert_token_to_idã®é–‹å§‹ token=â–Hello
ğŸŸ© _convert_token_to_idã®å®Œäº† token=â–Hello id=22557
ğŸŸ© ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ä»˜ãå…¥åŠ›ã®æ§‹ç¯‰é–‹å§‹ token_ids_0=[] token_ids_1=None
ğŸŸ© ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ä»˜ãå…¥åŠ›ã®æ§‹ç¯‰å®Œäº† output=[1]
ğŸŸ© ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ä»˜ãå…¥åŠ›ã®æ§‹ç¯‰é–‹å§‹ token_ids_0=[22557] token_ids_1=None
ğŸŸ© ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ä»˜ãå…¥åŠ›ã®æ§‹ç¯‰å®Œäº† output=[1, 22557]
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚¿ã‚¤ãƒ—IDã®ä½œæˆé–‹å§‹ token_ids_0=[22557] token_ids_1=None
ğŸŸ© ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚¿ã‚¤ãƒ—IDã®ä½œæˆå®Œäº† output=[0, 0]
ğŸŸ¨ Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
ğŸŸ© MixtralForCausalLMã®é †ä¼æ’­é–‹å§‹ torch.Size([1, 2]) torch.Size([1, 2]) torch.Size([1, 2]) past_key_values is not None=True None None use_cache=True output_router_logits=None torch.Size([2]) logits_to_keep=1
ğŸŸ© MixtralModelã®é †ä¼æ’­é–‹å§‹ torch.Size([1, 2]) torch.Size([1, 2]) torch.Size([1, 2]) past_key_values is not None=True None use_cache=True torch.Size([2])
ğŸŸ© MixtralRotaryEmbeddingã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 2, 4096]) position_ids.shape=torch.Size([1, 2])
ğŸŸ© MixtralRotaryEmbeddingã®é †ä¼æ’­å®Œäº† cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ¨ Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
ğŸŸ© MixtralForCausalLMã®é †ä¼æ’­é–‹å§‹ torch.Size([1, 2]) torch.Size([1, 2]) torch.Size([1, 2]) past_key_values is not None=True None None use_cache=True output_router_logits=None torch.Size([2]) logits_to_keep=1
ğŸŸ© MixtralModelã®é †ä¼æ’­é–‹å§‹ torch.Size([1, 2]) torch.Size([1, 2]) torch.Size([1, 2]) past_key_values is not None=True None use_cache=True torch.Size([2])
ğŸŸ© MixtralRotaryEmbeddingã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 2, 4096]) position_ids.shape=torch.Size([1, 2])
ğŸŸ© MixtralRotaryEmbeddingã®é †ä¼æ’­å®Œäº† cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None torch.Size([1, 2]) past_key_values is not None=True torch.Size([2])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096]) None past_key_values is not None=True torch.Size([2])
ğŸŸ© apply_rotary_pos_embã®é–‹å§‹ q.shape=torch.Size([1, 32, 2, 128]) k.shape=torch.Size([1, 8, 2, 128]) cos.shape=torch.Size([1, 2, 128]) sin.shape=torch.Size([1, 2, 128]) unsqueeze_dim=1
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 32, 2, 128])
ğŸŸ© rotate_halfã®é–‹å§‹ x.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© rotate_halfã®å®Œäº† res.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© apply_rotary_pos_embã®å®Œäº† q_embed.shape=torch.Size([1, 32, 2, 128]) k_embed.shape=torch.Size([1, 8, 2, 128])
ğŸŸ© MixtralAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([2, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([2, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([2, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([2, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 4096])
ğŸŸ¦ ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— gate.shape=torch.Size([1, 14336])
ğŸŸ¦ ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— up.shape=torch.Size([1, 14336])
ğŸŸ¦ ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† current_hidden_states.shape=torch.Size([1, 4096])
ğŸŸ© MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† final_hidden_states.shape=torch.Size([1, 2, 4096]) router_logits.shape=torch.Size([2, 8])
ğŸŸ© MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralRMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralModelã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 2, 4096])
ğŸŸ© MixtralForCausalLMã®é †ä¼æ’­å®Œäº† logits.shape=torch.Size([1, 1, 32000])
ğŸŸ© ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®æ–‡å­—åˆ—å¤‰æ›é–‹å§‹ tokens=['â–Hello', ','] legacy=True add_prefix_space=True
ğŸŸ© ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®æ–‡å­—åˆ—å¤‰æ›å®Œäº† string=Hello,
ğŸŸ¨ Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
ğŸŸ¨ Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
ğŸŸ¨ Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
