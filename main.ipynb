{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c211c7ff",
   "metadata": {},
   "source": [
    "# Mixtral-8x7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3094ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- [Ë´ñÊñá](https://arxiv.org/abs/2401.04088)\n",
    "- [ÂÆüË£Ö](https://github.com/mistralai/mistral-inference)\n",
    "- [„Ç¶„Çß„Éñ„Çµ„Ç§„Éà](https://mistral.ai/news/mixtral-of-experts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d52dffc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Ê¶ÇË¶Å"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9a70e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Mixtral 8x7B** „ÅØ„ÄÅ„Çπ„Éë„Éº„ÇπÊ∑∑Âêà„Ç®„Ç≠„Çπ„Éë„Éº„ÉàÔºàSparse Mixture of Experts: **SMoE**ÔºâË®ÄË™û„É¢„Éá„É´\n",
    "\n",
    "Mistral 7B„Å®Áï∞„Å™„Çä„ÄÅÂêÑÂ±§„Åå8„Å§„ÅÆ„Éï„Ç£„Éº„Éâ„Éï„Ç©„ÉØ„Éº„Éâ„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÔºà**„Ç®„Ç≠„Çπ„Éë„Éº„Éà**Ôºâ„ÅßÊßãÊàê„Åï„Çå„Çã\n",
    "\n",
    "„Éà„Éº„ÇØ„É≥ÊØé„Å´ **„É´„Éº„Çø„Éº„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ** „Åå2„Å§„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÇíÈÅ∏Êäû„Åó„ÄÅ„Åù„ÅÆÂá∫Âäõ„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Å¶Âá¶ÁêÜ„Åô„Çã\n",
    "\n",
    "„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÇíÂãïÁöÑ„Å´ÈÅ∏Êäû„Åô„Çã‰ªïÁµÑ„Åø„Å´„Çà„Çä„ÄÅ47BÔºà470ÂÑÑÔºâ„ÅÆ„ÅÜ„Å°13BÔºà130ÂÑÑÔºâ„Éë„É©„É°„Éº„Çø„Åó„Åã‰ΩøÁî®„Åó„Å™„ÅÑ\n",
    "\n",
    "Ë®àÁÆó„Ç≥„Çπ„Éà„Å®„É¨„Ç§„ÉÜ„É≥„Ç∑„ÇíÊäë„Åà„Å¶„ÄÅLlama 2 70B„ÇÑGPT-3.5„Å´ÂåπÊïµ„Åô„ÇãÊÄßËÉΩ„ÇíÂÆüÁèæ\n",
    "\n",
    "SFTÔºàSupervised Fine-TuningÔºâ„Å®DPOÔºàDirect Preference OptimizationÔºâ„ÅßË®ìÁ∑¥„Åó„Åü **Mixtral 8x7B Instruct** „ÇÇÂÖ¨Èñã"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8243ed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## „Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7e1b24",
   "metadata": {},
   "source": [
    "Mixtral„ÅØ„ÄÅMistral 7B„Åã„Çâ‰ª•‰∏ã„ÅÆÂ§âÊõ¥„ÇíÂä†„Åà„Å¶„ÅÑ„Çã:\n",
    "\n",
    "- Sliding Window AttentionÔºàSWAÔºâ„Åã„ÇâFully Dense Attention„Å´Â§âÊõ¥\n",
    "- „Éï„Ç£„Éº„Éâ„Éï„Ç©„ÉØ„Éº„Éâ„Éñ„É≠„ÉÉ„ÇØ„ÇíÊ∑∑Âêà„Ç®„Ç≠„Çπ„Éë„Éº„ÉàÂ±§ÔºàMoEÂ±§Ôºâ„Å´Â§âÊõ¥ \n",
    "\n",
    "![](image/table_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c8739",
   "metadata": {},
   "source": [
    "### „Çπ„Éë„Éº„ÇπÊ∑∑Âêà„Ç®„Ç≠„Çπ„Éë„Éº„ÉàÔºàSparse Mixture of ExpertsÔºâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56350ed",
   "metadata": {},
   "source": [
    "Ê∑∑Âêà„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆË©≥Á¥∞„ÅØÂà•„ÅÆ[Ë´ñÊñá](https://arxiv.org/pdf/2401.04088)„ÇíÂèÇÁÖß\n",
    "\n",
    "ÂÖ•Âäõ $x$ „Å´ÂØæ„Åô„ÇãMoE„ÅÆÂá∫Âäõ„ÅØ„ÄÅ **„Ç®„Ç≠„Çπ„Éë„Éº„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÂá∫Âäõ„ÅÆÈáç„Åø‰ªò„ÅçÂíå** „ÅßÊ±∫„Åæ„Çã\n",
    "\n",
    "„Ç®„Ç≠„Çπ„Éë„Éº„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÈáç„Åø„ÅØ„ÄÅ **„Ç≤„Éº„ÉÜ„Ç£„É≥„Ç∞„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÂá∫Âäõ** „ÅßÊ±∫„Åæ„Çã\n",
    "\n",
    "„Çπ„Éë„Éº„ÇπÊ∑∑Âêà„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆÊ¶ÇË¶ÅÂõ≥:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3996b6a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![](image/figure_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bfd050",
   "metadata": {},
   "source": [
    "$n$ ÂÄã„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ $\\{E_0, E_1, ..., E_{n-1}\\}$ „Åå‰∏é„Åà„Çâ„Çå„Åü„Å®„Åç„ÅÆÂá∫Âäõ„ÅÆÈáç„Åø‰ªò„ÅçÂíå:\n",
    "\n",
    "$$\n",
    "\\sum_{i=0}^{n-1}G(x)_{i}\\cdot E_{i}(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667dfb07",
   "metadata": {},
   "source": [
    "- $G(x)_i$: ÂÖ•Âäõ $x$ „Å´ÂØæ„Åô„Çã $i$ Áï™ÁõÆ„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆÈáç„ÅøÔºà„Ç≤„Éº„ÉÜ„Ç£„É≥„Ç∞„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÔºâ\n",
    "- $E_i(x)$: ÂÖ•Âäõ $x$ „Çí $i$ Áï™ÁõÆ„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅåÂá¶ÁêÜ„Åó„ÅüÂá∫ÂäõÔºà„Ç®„Ç≠„Çπ„Éë„Éº„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÔºâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951187ee",
   "metadata": {},
   "source": [
    "Mixtral„ÅÆ„Ç≤„Éº„ÉÜ„Ç£„É≥„Ç∞„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅØ„ÄÅ‰∏ä‰Ωç $K$ ÂÄãÔºàTop-KÔºâ„ÅÆ„É≠„Ç∏„ÉÉ„Éà„Å´ÂØæ„Åó„Å¶„ÇΩ„Éï„Éà„Éû„ÉÉ„ÇØ„Çπ„ÇíÈÅ©Áî®„Åó„ÅüÈñ¢Êï∞:\n",
    "\n",
    "$$\n",
    "G(x) := \\text{Softmax}(\\text{TopK}(x \\cdot W_g))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d03c1",
   "metadata": {},
   "source": [
    "- $W_g$: „Ç≤„Éº„ÉÜ„Ç£„É≥„Ç∞„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÈáç„ÅøË°åÂàó\n",
    "- $x\\cdot W_g$: ÂêÑ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆ„Çπ„Ç≥„Ç¢Ôºà„É≠„Ç∏„ÉÉ„ÉàÔºâ\n",
    "- $\\text{TopK}$: ‰∏ä‰Ωç$K$ÂÄã„Å´Âê´„Åæ„Çå„Å™„ÅÑ„É≠„Ç∏„ÉÉ„Éà„Çí„Éû„Ç§„Éä„ÇπÁÑ°ÈôêÂ§ß„Å´„Åô„ÇãÈñ¢Êï∞\n",
    "- $\\text{Softmax}(\\cdot)$: „Éû„Ç§„Éä„ÇπÁÑ°ÈôêÂ§ß„Å´„Å™„Å£„Åü„É≠„Ç∏„ÉÉ„Éà„ÇíÈô§„ÅÑ„Å¶ÂêàË®à $1.0$ „ÅÆÁ¢∫ÁéáÂàÜÂ∏É„Å´Â§âÊèõ„Åô„ÇãÈñ¢Êï∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104779c1",
   "metadata": {},
   "source": [
    "‰ΩøÁî®„Åô„Çã„Ç®„Ç≠„Çπ„Éë„Éº„ÉàÊï∞ $K$ „ÇíÂõ∫ÂÆö„Åó„ÄÅ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆÁ∑èÊï∞ $n$ „ÇíÂ¢ó„ÇÑ„Åô„Åì„Å®„Åß„ÄÅÂäπÁéáÁöÑ„Å´„Éë„É©„É°„Éº„ÇøÁ∑èÊï∞„ÇíÂ¢óÂä†„Åß„Åç„Çã:\n",
    "\n",
    "- „Éà„Éº„ÇØ„É≥ÊØé„Å´‰ΩøÁî®„Åô„Çã„Éë„É©„É°„Éº„ÇøÊï∞„Çí **„Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Éë„É©„É°„Éº„ÇøÊï∞** Ôºàactive parameter countÔºâ„Å®Âëº„Å∂\n",
    "- „É¢„Éá„É´„ÅÆ„Éë„É©„É°„Éº„ÇøÁ∑èÊï∞„Çí **„Çπ„Éë„Éº„Çπ„Éë„É©„É°„Éº„ÇøÊï∞** Ôºàsparse parameter countÔºâ„Å®Âëº„Å∂"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630079c7",
   "metadata": {},
   "source": [
    "MoEÂ±§„ÅØ„ÄÅÂçò‰∏Ä„ÇÇ„Åó„Åè„ÅØË§áÊï∞„ÅÆGPU„ÅßÂäπÁéáÁöÑ„Å´ÂÆüË°å„Åß„Åç„Çã:\n",
    "\n",
    "- Âçò‰∏ÄGPU„Åß„ÅÆÂäπÁéáÂåñÊâãÊ≥ï\n",
    "    - [Megablocks][1]: MoE„ÅÆFFN„ÅÆÊìç‰Ωú„ÇíÂ§ß„Åç„Å™„Çπ„Éë„Éº„ÇπË°åÂàó‰πóÁÆó„Å®„Åó„Å¶Êâ±„ÅÑ„ÄÅÂÆüË°åÈÄüÂ∫¶„ÇíÂêë‰∏ä„Åï„Åõ„Çã\n",
    "- Ë§áÊï∞GPU„Åß„ÅÆÂäπÁéáÂåñÊâãÊ≥ï\n",
    "    - „É¢„Éá„É´‰∏¶ÂàóÂåñÔºàModel Parallelism techniquesÔºâ: „É¢„Éá„É´„ÇíÂ±§„Åî„Å®„Å´ÂàÜ„Åë„Å¶Ë§áÊï∞„ÅÆGPU„Å´Â±ïÈñã„Åô„Çã\n",
    "    - [„Ç®„Ç≠„Çπ„Éë„Éº„Éà‰∏¶ÂàóÂåñ][2]ÔºàExpert Parallelism: EPÔºâ: „Ç®„Ç≠„Çπ„Éë„Éº„Éà„Çí„Ç∞„É´„Éº„Éó„Å´ÂàÜ„Åë„Å¶Ë§áÊï∞„ÅÆGPU„Å´Â±ïÈñã„Åô„Çã\n",
    "\n",
    "[1]: https://proceedings.mlsys.org/paper_files/paper/2023/hash/5a54f79333768effe7e8927bcccffe40-Abstract-mlsys2023.html\n",
    "[2]: https://arxiv.org/abs/1701.06538\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d208d",
   "metadata": {},
   "source": [
    "Mixtral„Åß„ÅØ„ÄÅ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÇíSwiGLU„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅßÂÆüË£Ö„Åó„ÄÅ‰ΩøÁî®„Åô„Çã„Ç®„Ç≠„Çπ„Éë„Éº„ÉàÊï∞„Çí $K=2$ „Å®„Åô„Çã:\n",
    "\n",
    "$$\n",
    "y = \\sum_{i=0}^{n-1} \\text{Softmax}(\\text{Top2}(x\\cdot W_g))_i \\cdot \\text{SwiGLU}_i(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc29f0bc",
   "metadata": {},
   "source": [
    "- $\\text{Softmax}(\\text{Top2}(x\\cdot W_g))_i$: $i$ Áï™ÁõÆ„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„Å´ÂØæ„Åô„ÇãÈáç„Åø\n",
    "- $\\text{SwiGLU}_i(x)$: $i$ Áï™ÁõÆ„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆÂá∫Âäõ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4e45c",
   "metadata": {},
   "source": [
    "## „Éô„É≥„ÉÅ„Éû„Éº„ÇØÁµêÊûú"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946da742",
   "metadata": {},
   "source": [
    "Mixtral„Å®Llama„Çí„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÅßË©ï‰æ°„ÅóÊØîËºÉ:\n",
    "\n",
    "- Â∏∏Ë≠òÊé®Ë´ñÔºà0-shotÔºâ\n",
    "    - Hellaswag: ÊñáËÑà„Åã„ÇâËá™ÁÑ∂„Å´Á∂ö„ÅèÁµêÊú´„ÇíÈÅ∏„Å∂\n",
    "    - Winogrande: ‰ª£ÂêçË©û„ÅåÊåá„Åó„Å¶„ÅÑ„ÇãÂçòË™û„ÇíÈÅ∏„Å∂\n",
    "    - PIQAÔºàPhysical Interaction Question AnsweringÔºâ: Áâ©ÁêÜÁöÑ„Å™ÁêÜËß£„ÅåÂøÖË¶Å„Å™ÈÅ∏ÊäûËÇ¢„ÇíÈÅ∏„Å∂\n",
    "    - SIQAÔºàSocial Interaction QAÔºâ:‰∫∫„ÅÆÊÑüÊÉÖ„ÅÆÁêÜËß£„ÅåÂøÖË¶Å„Å™ÈÅ∏ÊäûËÇ¢„ÇíÈÅ∏„Å∂\n",
    "    - OpenbookQA: ‰∏ÄËà¨ÁöÑ„Å™ÁßëÂ≠¶ÁöÑ‰∫ãÂÆüÔºàOpen BookÔºâ„ÅÆÁêÜËß£„ÅåÂøÖË¶Å„Å™ÈÅ∏ÊäûËÇ¢„ÇíÈÅ∏„Å∂\n",
    "    - ARC-EasyÔºàAI2 Reasoning ChallengeÔºâ: Â∞èÂ≠¶Áîü„É¨„Éô„É´„ÅÆÁßëÂ≠¶„ÅÆÁêÜËß£„ÅåÂøÖË¶Å„Å™ÈÅ∏ÊäûËÇ¢„ÇíÈÅ∏„Å∂\n",
    "    - ARC-Challenge: ‰∏≠Â≠¶Áîü„É¨„Éô„É´„ÅÆÁßëÂ≠¶„ÅÆÁêÜËß£„ÅåÂøÖË¶Å„Å™ÈÅ∏ÊäûËÇ¢„ÇíÈÅ∏„Å∂\n",
    "    - CommonsenseQA: Á§æ‰ºöÂ∏∏Ë≠ò„ÅÆÁêÜËß£„ÅåÂøÖË¶Å„Å™ÈÅ∏ÊäûËÇ¢„ÇíÈÅ∏„Å∂\n",
    "- ‰∏ñÁïåÁü•Ë≠òÔºà5-shotÔºâ\n",
    "    - NaturalQuestions: ‰∏é„Åà„Çâ„Çå„ÅüWikipedia„ÅÆ„Éö„Éº„Ç∏„Åã„ÇâÈï∑„ÅÑÂõûÁ≠î„Å®Áü≠„ÅÑÂõûÁ≠î„ÇíÊäΩÂá∫„Åô„Çã\n",
    "    - TriviaQA: „Ç¶„Çß„Éñ„Éö„Éº„Ç∏„ÇÑWikipedia„ÅÆ„Éö„Éº„Ç∏„Åå‰∏é„Åà„Çâ„Çå„ÄÅ„Åù„Çå„Çâ„ÇíÁµ±Âêà„Åô„ÇãÂøÖË¶Å„ÅÆ„ÅÇ„ÇãÂõûÁ≠î„ÇíÊäΩÂá∫„Åô„Çã\n",
    "- Ë™≠Ëß£Ôºà0-shotÔºâ\n",
    "    - BoolQ: ‰∏é„Åà„Çâ„Çå„ÅüÊñáÁ´†„Å´ÂØæ„Åó„Å¶„ÄÅ„ÅØ„ÅÑ/„ÅÑ„ÅÑ„Åà„ÅßÂõûÁ≠î„Åô„Çã\n",
    "    - QuACÔºàQuestion Answering in ContextÔºâ: ‰∏Ä‰∫∫„ÅÆ„É¶„Éº„Ç∂„Éº„ÅåÈÄ£Á∂ö„Åó„Å¶Ë≥™Âïè„Åó„Åù„Çå„Å´ÂØæ„Åó„Å¶ÂõûÁ≠î„ÅóÁ∂ö„Åë„Çã\n",
    "- Êï∞Â≠¶\n",
    "    - GSM8KÔºà8-shotÔºâ: Â∞èÂ≠¶Áîü„É¨„Éô„É´„ÅÆÁÆóÊï∞ÂïèÈ°å\n",
    "    - MATHÔºà4-shotÔºâ: Á´∂ÊäÄÊï∞Â≠¶„É¨„Éô„É´„ÅÆÈõ£„Åó„ÅÑÊï∞Â≠¶ÂïèÈ°å\n",
    "- „Ç≥„Éº„Éâ\n",
    "    - HumanevalÔºà0-shotÔºâ: ‰∫∫„Åå‰ΩúÊàê„Åó„ÅüPythonÈñ¢Êï∞„Çí„Éí„É≥„Éà„Åã„ÇâÂÆåÊàê„Åï„Åõ„Çã\n",
    "    - MBPPÔºàMostly Basic Python ProgrammingÔºâÔºà3-shotÔºâ: ÂàùÂøÉËÄÖÂêë„Åë„ÅÆÂü∫Êú¨ÁöÑ„Å™Python„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞ÂïèÈ°å„ÇíËß£„Åè\n",
    "- Á∑èÂêà\n",
    "    - MMLUÔºàMassive Multitask Language UnderstandingÔºâÔºà5-shotÔºâ: 57„ÅÆÁï∞„Å™„ÇãÂàÜÈáé„ÅÆÈÅ∏ÊäûÂïèÈ°å\n",
    "    - BBHÔºàBig-Bench HardÔºâÔºà3-shotÔºâ: ÁèæÂú®„ÅÆ„É¢„Éá„É´„ÅåËã¶Êâã„Å®„Åô„Çã23„ÅÆÊåëÊà¶ÁöÑ„Å™„Çø„Çπ„ÇØ\n",
    "    - AGI EvalÔºà3-5-shotÔºâ: Á±≥ÂõΩ„ÅÆÂ§ßÂ≠¶ÂÖ•Â≠¶Ë©¶È®ì„ÄÅÊ≥ïÁßëÂ§ßÂ≠¶Èô¢Ë©¶È®ì„ÄÅÂåªÂ∏´ÂõΩÂÆ∂Ë©¶È®ì„Å™„Å©„ÅÆÈÅ∏ÊäûÂïèÈ°å"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b071e43",
   "metadata": {},
   "source": [
    "Mixtral„ÅØ„ÄÅ„Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Éë„É©„É°„Éº„ÇøÊï∞„Åå5ÂÄçÂ§ö„ÅÑLlama2 70B„ÇíÂ§ö„Åè„ÅÆ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Åß‰∏äÂõû„Å£„ÅüÔºà„Ç≥„Éº„Éâ„Å®Êï∞Â≠¶„ÅåÂº∑„ÅÑÔºâ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ec94f",
   "metadata": {},
   "source": [
    "![](image/figure_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc4ba4",
   "metadata": {},
   "source": [
    "![](image/table_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54bdb7d",
   "metadata": {},
   "source": [
    "Mixtral„ÅØ„ÄÅ„Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Éë„É©„É°„Éº„ÇøÊï∞„ÅåÂ∞ë„Å™„ÅèÊÄßËÉΩ„ÅåÈ´ò„ÅÑ:\n",
    "\n",
    "![](image/figure_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd406ac",
   "metadata": {},
   "source": [
    "Mixtral„ÅØ„ÄÅLlaMA 2 70B„Çà„ÇäÂÑ™„Çå„ÄÅGPT-3.5ÔºàGPT-3.5-TurboÔºâ„Å´ÂåπÊïµ„Åô„ÇãÊÄßËÉΩ„ÇíÁ§∫„Åó„Åü:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe974526",
   "metadata": {},
   "source": [
    "![](image/table_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b6938",
   "metadata": {},
   "source": [
    "Mixtral„ÅØ„ÄÅËã±Ë™û„ÅÆ‰ªñ„Å´„Éï„É©„É≥„ÇπË™û„Éª„Éâ„Ç§„ÉÑË™û„Éª„Çπ„Éö„Ç§„É≥Ë™û„Éª„Ç§„Çø„É™„Ç¢Ë™û„ÅßLlama2 70 B„ÇíÂ§ßÂπÖ„Å´‰∏äÂõû„ÇãÊÄßËÉΩ:\n",
    "\n",
    "![](image/table_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00793e7",
   "metadata": {},
   "source": [
    "Èï∑„ÅÑÊñáÁ´†„Åã„ÇâÊé¢„ÅóÂá∫„Åô„Çø„Çπ„ÇØÔºà[passkey retrieval][1]Ôºâ„Åß„ÅØ„ÄÅ100%„ÅÆÊ§úÁ¥¢ÊÄßËÉΩ„ÇíÁ§∫„Åó„ÄÅÂõ∞ÊÉëÂ∫¶ÔºàperplexityÔºâ„ÇÇÈï∑„Åï„Å´Âøú„Åò„Å¶Ê∏õÂ∞ë:\n",
    "\n",
    "![](image/figure_4.png)\n",
    "\n",
    "[1]: https://arxiv.org/abs/2305.16300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e849f",
   "metadata": {},
   "source": [
    "Llama 2„Å®ÊØîËºÉ„Åó„Å¶„ÄÅBBQÔºàBias Benchmark for QAÔºâ„Åß‰Ωé„ÅÑ„Éê„Ç§„Ç¢„Çπ„ÇíÁ§∫„Åó„Åü:\n",
    "\n",
    "![](image/figure_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11470c2d",
   "metadata": {},
   "source": [
    "ÊåáÁ§∫„ÉÅ„É•„Éº„Éã„É≥„Ç∞Ê∏à„Åø„É¢„Éá„É´„ÅØ„ÄÅMT-Bench„Åß„ÅØ„Ç™„Éº„Éó„É≥„Ç¶„Çß„Ç§„Éà„É¢„Éá„É´„ÅÆ‰∏≠„ÅßÊúÄ„ÇÇÈ´ò„ÅÑ:\n",
    "\n",
    "![](image/figure_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbd7cd",
   "metadata": {},
   "source": [
    "## „É´„Éº„ÉÜ„Ç£„É≥„Ç∞ÂàÜÊûê"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af09458c",
   "metadata": {},
   "source": [
    "The Pile„ÅÆÊ§úË®º„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí‰Ωø„ÅÑ„ÄÅ„Éà„Éî„ÉÉ„ÇØÊØé„Å´0Â±§ÁõÆ„Éª15Â±§ÁõÆ„Éª31Â±§ÁõÆ„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„ÉàÈÅ∏ÊäûÁä∂ÊÖã„ÇíÊ∏¨ÂÆö\n",
    "\n",
    "„Éà„Éî„ÉÉ„ÇØ„Å´Âü∫„Å•„ÅÑ„Åü„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆÈÅ∏Êäû„Å´ÊòéÁ¢∫„Å™„Éë„Çø„Éº„É≥„ÅØË¶ã„Çâ„Çå„Å™„Åã„Å£„ÅüÔºàÊï∞Â≠¶„ÅÆ„Åø„Çè„Åö„Åã„Å´ÂèçÂøúÔºâ:\n",
    "\n",
    "![](image/figure_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8077637",
   "metadata": {},
   "source": [
    "![](image/table_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d1b54",
   "metadata": {},
   "source": [
    "„Éà„Éº„ÇØ„É≥„Åî„Å®„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆÂâ≤ÂΩì„Åß„ÅØ„ÄÅ`self`„Éª`Question`„Éª„Ç§„É≥„Éá„É≥„Éà„ÉªÈÄ£Á∂ö„Åó„Åü„Éà„Éº„ÇØ„É≥„ÅåÂêå„Åò„É´„Éº„ÉÜ„Ç£„É≥„Ç∞:\n",
    "\n",
    "![](image/figure_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8549895d",
   "metadata": {},
   "source": [
    "## ÂÆüË£Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d377de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentencepiece in /opt/miniconda/envs/py312/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: protobuf in /opt/miniconda/envs/py312/lib/python3.12/site-packages (6.33.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/py312/lib/python3.12/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "CUDA„Çí‰ΩøÁî®„Åß„Åç„Åæ„Åõ„Çì",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m torch.cuda.is_available(), \u001b[33m\"\u001b[39m\u001b[33mCUDA„Çí‰ΩøÁî®„Åß„Åç„Åæ„Åõ„Çì\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ACT2FN\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcache_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Cache, DynamicCache\n",
      "\u001b[31mAssertionError\u001b[39m: CUDA„Çí‰ΩøÁî®„Åß„Åç„Åæ„Åõ„Çì"
     ]
    }
   ],
   "source": [
    "%pip install -qU transformers==4.57.1\n",
    "%pip install sentencepiece protobuf\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "except ImportError:\n",
    "    from dotenv import load_dotenv\n",
    "    import os\n",
    "    load_dotenv()\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "assert HF_TOKEN\n",
    "\n",
    "import os\n",
    "import logging as logging_\n",
    "import transformers\n",
    "from transformers import PretrainedConfig\n",
    "from transformers.utils import logging\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "# „Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº\n",
    "\n",
    "import json\n",
    "import os\n",
    "import unicodedata\n",
    "from functools import lru_cache\n",
    "from typing import Optional\n",
    "\n",
    "import regex as re\n",
    "\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "from transformers.utils import logging\n",
    "\n",
    "# „É¢„Éá„É´\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA„Çí‰ΩøÁî®„Åß„Åç„Åæ„Åõ„Çì\"\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.cache_utils import Cache, DynamicCache\n",
    "from transformers.generation import GenerationMixin\n",
    "from transformers.integrations import use_kernel_forward_from_hub\n",
    "from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask\n",
    "from transformers.modeling_flash_attention_utils import FlashAttentionKwargs\n",
    "from transformers.modeling_layers import (\n",
    "    GenericForQuestionAnswering,\n",
    "    GenericForSequenceClassification,\n",
    "    GenericForTokenClassification,\n",
    "    GradientCheckpointingLayer,\n",
    ")\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n",
    "from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n",
    "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n",
    "from transformers.processing_utils import Unpack\n",
    "from transformers.utils import TransformersKwargs, auto_docstring, can_return_tuple\n",
    "from transformers.utils.deprecation import deprecate_kwarg\n",
    "from transformers.utils.generic import check_model_inputs\n",
    "from transformers.models.qwen2.configuration_qwen2 import Qwen2Config\n",
    "\n",
    "\n",
    "# „Éá„Éê„Ç§„ÇπË®≠ÂÆö\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "# „É≠„Ç∞Ë®≠ÂÆö\n",
    "\n",
    "if os.path.exists('debug.log'):\n",
    "    os.remove('debug.log')\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging_.DEBUG:\n",
    "            level = 'üü¶'\n",
    "        case logging_.INFO:\n",
    "            level = 'üü©'\n",
    "        case logging_.WARNING:\n",
    "            level = 'üü®'\n",
    "        case logging_.ERROR:\n",
    "            level = 'üü•'\n",
    "        case logging_.CRITICAL:\n",
    "            level = 'üõë'\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logging.set_verbosity_debug()\n",
    "logger = logging.get_logger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging_.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging_.FileHandler('debug.log')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging_.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.info(f\"Transformers version: {transformers.__version__}\")\n",
    "logger.info(f\"Numpy version: {np.__version__}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
