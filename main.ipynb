{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c211c7ff",
   "metadata": {},
   "source": [
    "# Mixtral-8x7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3094ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- [論文](https://arxiv.org/abs/2401.04088)\n",
    "- [実装](https://github.com/mistralai/mistral-inference)\n",
    "- [ウェブサイト](https://mistral.ai/news/mixtral-of-experts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d52dffc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 概要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9a70e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Mixtral 8x7B** は、スパース混合エキスパート（Sparse Mixture of Experts: **SMoE**）言語モデル\n",
    "\n",
    "Mistral 7Bと異なり、各層が8つのフィードフォワードネットワーク（**エキスパート**）で構成される\n",
    "\n",
    "トークン毎に **ルーターネットワーク** が2つのエキスパートを選択し、その出力を組み合わせて処理する\n",
    "\n",
    "エキスパートを動的に選択する仕組みにより、47B（470億）のうち13B（130億）パラメータしか使用しない\n",
    "\n",
    "計算コストとレイテンシを抑えて、Llama 2 70BやGPT-3.5に匹敵する性能を実現\n",
    "\n",
    "SFT（Supervised Fine-Tuning）とDPO（Direct Preference Optimization）で訓練した **Mixtral 8x7B Instruct** も公開"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8243ed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## アーキテクチャ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7e1b24",
   "metadata": {},
   "source": [
    "Mixtralは、Mistral 7Bから以下の変更を加えている:\n",
    "\n",
    "- Sliding Window Attention（SWA）からFully Dense Attentionに変更\n",
    "- フィードフォワードブロックを混合エキスパート層（MoE層）に変更 \n",
    "\n",
    "![](image/table_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c8739",
   "metadata": {},
   "source": [
    "### スパース混合エキスパート（Sparse Mixture of Experts）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56350ed",
   "metadata": {},
   "source": [
    "混合エキスパートの詳細は別の[論文](https://arxiv.org/pdf/2401.04088)を参照\n",
    "\n",
    "入力 $x$ に対するMoEの出力は、 **エキスパートネットワークの出力の重み付き和** で決まる\n",
    "\n",
    "エキスパートネットワークの重みは、 **ゲーティングネットワークの出力** で決まる\n",
    "\n",
    "スパース混合エキスパートの概要図:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3996b6a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![](image/figure_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bfd050",
   "metadata": {},
   "source": [
    "$n$ 個のエキスパートネットワーク $\\{E_0, E_1, ..., E_{n-1}\\}$ が与えられたときの出力の重み付き和:\n",
    "\n",
    "$$\n",
    "\\sum_{i=0}^{n-1}G(x)_{i}\\cdot E_{i}(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667dfb07",
   "metadata": {},
   "source": [
    "- $G(x)_i$: 入力 $x$ に対する $i$ 番目のエキスパートの重み（ゲーティングネットワーク）\n",
    "- $E_i(x)$: 入力 $x$ を $i$ 番目のエキスパートが処理した出力（エキスパートネットワーク）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951187ee",
   "metadata": {},
   "source": [
    "Mixtralのゲーティングネットワークは、上位 $K$ 個（Top-K）のロジットに対してソフトマックスを適用した関数:\n",
    "\n",
    "$$\n",
    "G(x) := \\text{Softmax}(\\text{TopK}(x \\cdot W_g))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d03c1",
   "metadata": {},
   "source": [
    "- $W_g$: ゲーティングネットワークの重み行列\n",
    "- $x\\cdot W_g$: 各エキスパートのスコア（ロジット）\n",
    "- $\\text{TopK}$: 上位$K$個に含まれないロジットをマイナス無限大にする関数\n",
    "- $\\text{Softmax}(\\cdot)$: マイナス無限大になったロジットを除いて合計 $1.0$ の確率分布に変換する関数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104779c1",
   "metadata": {},
   "source": [
    "使用するエキスパート数 $K$ を固定し、エキスパートの総数 $n$ を増やすことで、効率的にパラメータ総数を増加できる:\n",
    "\n",
    "- トークン毎に使用するパラメータ数を **アクティブパラメータ数** （active parameter count）と呼ぶ\n",
    "- モデルのパラメータ総数を **スパースパラメータ数** （sparse parameter count）と呼ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630079c7",
   "metadata": {},
   "source": [
    "MoE層は、単一もしくは複数のGPUで効率的に実行できる:\n",
    "\n",
    "- 単一GPUでの効率化手法\n",
    "    - [Megablocks][1]: MoEのFFNの操作を大きなスパース行列乗算として扱い、実行速度を向上させる\n",
    "- 複数GPUでの効率化手法\n",
    "    - モデル並列化（Model Parallelism techniques）: モデルを層ごとに分けて複数のGPUに展開する\n",
    "    - [エキスパート並列化][2]（Expert Parallelism: EP）: エキスパートをグループに分けて複数のGPUに展開する\n",
    "\n",
    "[1]: https://proceedings.mlsys.org/paper_files/paper/2023/hash/5a54f79333768effe7e8927bcccffe40-Abstract-mlsys2023.html\n",
    "[2]: https://arxiv.org/abs/1701.06538\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d208d",
   "metadata": {},
   "source": [
    "Mixtralでは、エキスパートをSwiGLUアーキテクチャで実装し、使用するエキスパート数を $K=2$ とする:\n",
    "\n",
    "$$\n",
    "y = \\sum_{i=0}^{n-1} \\text{Softmax}(\\text{Top2}(x\\cdot W_g))_i \\cdot \\text{SwiGLU}_i(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc29f0bc",
   "metadata": {},
   "source": [
    "- $\\text{Softmax}(\\text{Top2}(x\\cdot W_g))_i$: $i$ 番目のエキスパートに対する重み\n",
    "- $\\text{SwiGLU}_i(x)$: $i$ 番目のエキスパートの出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4e45c",
   "metadata": {},
   "source": [
    "## ベンチマーク結果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946da742",
   "metadata": {},
   "source": [
    "MixtralとLlamaをベンチマークで評価し比較:\n",
    "\n",
    "- 常識推論（0-shot）\n",
    "    - Hellaswag: 文脈から自然に続く結末を選ぶ\n",
    "    - Winogrande: 代名詞が指している単語を選ぶ\n",
    "    - PIQA（Physical Interaction Question Answering）: 物理的な理解が必要な選択肢を選ぶ\n",
    "    - SIQA（Social Interaction QA）:人の感情の理解が必要な選択肢を選ぶ\n",
    "    - OpenbookQA: 一般的な科学的事実（Open Book）の理解が必要な選択肢を選ぶ\n",
    "    - ARC-Easy（AI2 Reasoning Challenge）: 小学生レベルの科学の理解が必要な選択肢を選ぶ\n",
    "    - ARC-Challenge: 中学生レベルの科学の理解が必要な選択肢を選ぶ\n",
    "    - CommonsenseQA: 社会常識の理解が必要な選択肢を選ぶ\n",
    "- 世界知識（5-shot）\n",
    "    - NaturalQuestions: 与えられたWikipediaのページから長い回答と短い回答を抽出する\n",
    "    - TriviaQA: ウェブページやWikipediaのページが与えられ、それらを統合する必要のある回答を抽出する\n",
    "- 読解（0-shot）\n",
    "    - BoolQ: 与えられた文章に対して、はい/いいえで回答する\n",
    "    - QuAC（Question Answering in Context）: 一人のユーザーが連続して質問しそれに対して回答し続ける\n",
    "- 数学\n",
    "    - GSM8K（8-shot）: 小学生レベルの算数問題\n",
    "    - MATH（4-shot）: 競技数学レベルの難しい数学問題\n",
    "- コード\n",
    "    - Humaneval（0-shot）: 人が作成したPython関数をヒントから完成させる\n",
    "    - MBPP（Mostly Basic Python Programming）（3-shot）: 初心者向けの基本的なPythonプログラミング問題を解く\n",
    "- 総合\n",
    "    - MMLU（Massive Multitask Language Understanding）（5-shot）: 57の異なる分野の選択問題\n",
    "    - BBH（Big-Bench Hard）（3-shot）: 現在のモデルが苦手とする23の挑戦的なタスク\n",
    "    - AGI Eval（3-5-shot）: 米国の大学入学試験、法科大学院試験、医師国家試験などの選択問題"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b071e43",
   "metadata": {},
   "source": [
    "Mixtralは、アクティブパラメータ数が5倍多いLlama2 70Bを多くのベンチマークで上回った（コードと数学が強い）:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ec94f",
   "metadata": {},
   "source": [
    "![](image/figure_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc4ba4",
   "metadata": {},
   "source": [
    "![](image/table_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54bdb7d",
   "metadata": {},
   "source": [
    "Mixtralは、アクティブパラメータ数が少なく性能が高い:\n",
    "\n",
    "![](image/figure_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd406ac",
   "metadata": {},
   "source": [
    "Mixtralは、LlaMA 2 70Bより優れ、GPT-3.5（GPT-3.5-Turbo）に匹敵する性能を示した:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe974526",
   "metadata": {},
   "source": [
    "![](image/table_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b6938",
   "metadata": {},
   "source": [
    "Mixtralは、英語の他にフランス語・ドイツ語・スペイン語・イタリア語でLlama2 70 Bを大幅に上回る性能:\n",
    "\n",
    "![](image/table_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00793e7",
   "metadata": {},
   "source": [
    "長い文章から探し出すタスク（[passkey retrieval][1]）では、100%の検索性能を示し、困惑度（perplexity）も長さに応じて減少:\n",
    "\n",
    "![](image/figure_4.png)\n",
    "\n",
    "[1]: https://arxiv.org/abs/2305.16300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e849f",
   "metadata": {},
   "source": [
    "Llama 2と比較して、BBQ（Bias Benchmark for QA）で低いバイアスを示した:\n",
    "\n",
    "![](image/figure_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11470c2d",
   "metadata": {},
   "source": [
    "指示チューニング済みモデルは、MT-Benchではオープンウェイトモデルの中で最も高い:\n",
    "\n",
    "![](image/figure_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbd7cd",
   "metadata": {},
   "source": [
    "## ルーティング分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af09458c",
   "metadata": {},
   "source": [
    "The Pileの検証データセットを使い、トピック毎に0層目・15層目・31層目のエキスパート選択状態を測定\n",
    "\n",
    "トピックに基づいたエキスパートの選択に明確なパターンは見られなかった（数学のみわずかに反応）:\n",
    "\n",
    "![](image/figure_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8077637",
   "metadata": {},
   "source": [
    "![](image/table_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d1b54",
   "metadata": {},
   "source": [
    "トークンごとのエキスパートの割当では、`self`・`Question`・インデント・連続したトークンが同じルーティング:\n",
    "\n",
    "![](image/figure_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8549895d",
   "metadata": {},
   "source": [
    "## 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d377de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentencepiece in /opt/miniconda/envs/py312/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: protobuf in /opt/miniconda/envs/py312/lib/python3.12/site-packages (6.33.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/py312/lib/python3.12/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "CUDAを使用できません",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m torch.cuda.is_available(), \u001b[33m\"\u001b[39m\u001b[33mCUDAを使用できません\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ACT2FN\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcache_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Cache, DynamicCache\n",
      "\u001b[31mAssertionError\u001b[39m: CUDAを使用できません"
     ]
    }
   ],
   "source": [
    "%pip install -qU transformers==4.57.1\n",
    "%pip install sentencepiece protobuf\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "except ImportError:\n",
    "    from dotenv import load_dotenv\n",
    "    import os\n",
    "    load_dotenv()\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "assert HF_TOKEN\n",
    "\n",
    "import os\n",
    "import logging as logging_\n",
    "import transformers\n",
    "from transformers import PretrainedConfig\n",
    "from transformers.utils import logging\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "# トークナイザー\n",
    "\n",
    "import json\n",
    "import os\n",
    "import unicodedata\n",
    "from functools import lru_cache\n",
    "from typing import Optional\n",
    "\n",
    "import regex as re\n",
    "\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "from transformers.utils import logging\n",
    "\n",
    "# モデル\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDAを使用できません\"\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.cache_utils import Cache, DynamicCache\n",
    "from transformers.generation import GenerationMixin\n",
    "from transformers.integrations import use_kernel_forward_from_hub\n",
    "from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask\n",
    "from transformers.modeling_flash_attention_utils import FlashAttentionKwargs\n",
    "from transformers.modeling_layers import (\n",
    "    GenericForQuestionAnswering,\n",
    "    GenericForSequenceClassification,\n",
    "    GenericForTokenClassification,\n",
    "    GradientCheckpointingLayer,\n",
    ")\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n",
    "from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n",
    "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n",
    "from transformers.processing_utils import Unpack\n",
    "from transformers.utils import TransformersKwargs, auto_docstring, can_return_tuple\n",
    "from transformers.utils.deprecation import deprecate_kwarg\n",
    "from transformers.utils.generic import check_model_inputs\n",
    "from transformers.models.qwen2.configuration_qwen2 import Qwen2Config\n",
    "\n",
    "\n",
    "# デバイス設定\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "# ログ設定\n",
    "\n",
    "if os.path.exists('debug.log'):\n",
    "    os.remove('debug.log')\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging_.DEBUG:\n",
    "            level = '🟦'\n",
    "        case logging_.INFO:\n",
    "            level = '🟩'\n",
    "        case logging_.WARNING:\n",
    "            level = '🟨'\n",
    "        case logging_.ERROR:\n",
    "            level = '🟥'\n",
    "        case logging_.CRITICAL:\n",
    "            level = '🛑'\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logging.set_verbosity_debug()\n",
    "logger = logging.get_logger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging_.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging_.FileHandler('debug.log')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging_.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.info(f\"Transformers version: {transformers.__version__}\")\n",
    "logger.info(f\"Numpy version: {np.__version__}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
