{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c211c7ff",
   "metadata": {},
   "source": [
    "# Mixtral-8x7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3094ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- [è«–æ–‡](https://arxiv.org/abs/2401.04088)\n",
    "- [å®Ÿè£…](https://github.com/mistralai/mistral-inference)\n",
    "- [ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ](https://mistral.ai/news/mixtral-of-experts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d52dffc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## æ¦‚è¦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9a70e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Mixtral 8x7B** ã¯ã€ã‚¹ãƒ‘ãƒ¼ã‚¹æ··åˆã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆï¼ˆSparse Mixture of Experts: **SMoE**ï¼‰è¨€èªãƒ¢ãƒ‡ãƒ«\n",
    "\n",
    "Mistral 7Bã¨ç•°ãªã‚Šã€å„å±¤ãŒ8ã¤ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆ**ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ**ï¼‰ã§æ§‹æˆã•ã‚Œã‚‹\n",
    "\n",
    "ãƒˆãƒ¼ã‚¯ãƒ³æ¯ã« **ãƒ«ãƒ¼ã‚¿ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯** ãŒ2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠã—ã€ãã®å‡ºåŠ›ã‚’çµ„ã¿åˆã‚ã›ã¦å‡¦ç†ã™ã‚‹\n",
    "\n",
    "ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’å‹•çš„ã«é¸æŠã™ã‚‹ä»•çµ„ã¿ã«ã‚ˆã‚Šã€47Bï¼ˆ470å„„ï¼‰ã®ã†ã¡13Bï¼ˆ130å„„ï¼‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã—ã‹ä½¿ç”¨ã—ãªã„\n",
    "\n",
    "è¨ˆç®—ã‚³ã‚¹ãƒˆã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’æŠ‘ãˆã¦ã€Llama 2 70Bã‚„GPT-3.5ã«åŒ¹æ•µã™ã‚‹æ€§èƒ½ã‚’å®Ÿç¾\n",
    "\n",
    "SFTï¼ˆSupervised Fine-Tuningï¼‰ã¨DPOï¼ˆDirect Preference Optimizationï¼‰ã§è¨“ç·´ã—ãŸ **Mixtral 8x7B Instruct** ã‚‚å…¬é–‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8243ed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7e1b24",
   "metadata": {},
   "source": [
    "Mixtralã¯ã€Mistral 7Bã‹ã‚‰ä»¥ä¸‹ã®å¤‰æ›´ã‚’åŠ ãˆã¦ã„ã‚‹:\n",
    "\n",
    "- Sliding Window Attentionï¼ˆSWAï¼‰ã‹ã‚‰Fully Dense Attentionã«å¤‰æ›´\n",
    "- ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ··åˆã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆå±¤ï¼ˆMoEå±¤ï¼‰ã«å¤‰æ›´ \n",
    "\n",
    "![](image/table_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c8739",
   "metadata": {},
   "source": [
    "### ã‚¹ãƒ‘ãƒ¼ã‚¹æ··åˆã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆï¼ˆSparse Mixture of Expertsï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56350ed",
   "metadata": {},
   "source": [
    "æ··åˆã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®è©³ç´°ã¯åˆ¥ã®[è«–æ–‡](https://arxiv.org/pdf/2401.04088)ã‚’å‚ç…§\n",
    "\n",
    "å…¥åŠ› $x$ ã«å¯¾ã™ã‚‹MoEã®å‡ºåŠ›ã¯ã€ **ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‡ºåŠ›ã®é‡ã¿ä»˜ãå’Œ** ã§æ±ºã¾ã‚‹\n",
    "\n",
    "ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿ã¯ã€ **ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‡ºåŠ›** ã§æ±ºã¾ã‚‹\n",
    "\n",
    "ã‚¹ãƒ‘ãƒ¼ã‚¹æ··åˆã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®æ¦‚è¦å›³:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3996b6a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![](image/figure_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bfd050",
   "metadata": {},
   "source": [
    "$n$ å€‹ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ $\\{E_0, E_1, transformers., E_{n-1}\\}$ ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã®å‡ºåŠ›ã®é‡ã¿ä»˜ãå’Œ:\n",
    "\n",
    "$$\n",
    "\\sum_{i=0}^{n-1}G(x)_{i}\\cdot E_{i}(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667dfb07",
   "metadata": {},
   "source": [
    "- $G(x)_i$: å…¥åŠ› $x$ ã«å¯¾ã™ã‚‹ $i$ ç•ªç›®ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é‡ã¿ï¼ˆã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰\n",
    "- $E_i(x)$: å…¥åŠ› $x$ ã‚’ $i$ ç•ªç›®ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãŒå‡¦ç†ã—ãŸå‡ºåŠ›ï¼ˆã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951187ee",
   "metadata": {},
   "source": [
    "Mixtralã®ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€ä¸Šä½ $K$ å€‹ï¼ˆTop-Kï¼‰ã®ãƒ­ã‚¸ãƒƒãƒˆã«å¯¾ã—ã¦ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚’é©ç”¨ã—ãŸé–¢æ•°:\n",
    "\n",
    "$$\n",
    "G(x) := \\text{Softmax}(\\text{TopK}(x \\cdot W_g))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d03c1",
   "metadata": {},
   "source": [
    "- $W_g$: ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿è¡Œåˆ—\n",
    "- $x\\cdot W_g$: å„ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ã‚¹ã‚³ã‚¢ï¼ˆãƒ­ã‚¸ãƒƒãƒˆï¼‰\n",
    "- $\\text{TopK}$: ä¸Šä½$K$å€‹ã«å«ã¾ã‚Œãªã„ãƒ­ã‚¸ãƒƒãƒˆã‚’ãƒã‚¤ãƒŠã‚¹ç„¡é™å¤§ã«ã™ã‚‹é–¢æ•°\n",
    "- $\\text{Softmax}(\\cdot)$: ãƒã‚¤ãƒŠã‚¹ç„¡é™å¤§ã«ãªã£ãŸãƒ­ã‚¸ãƒƒãƒˆã‚’é™¤ã„ã¦åˆè¨ˆ $1.0$ ã®ç¢ºç‡åˆ†å¸ƒã«å¤‰æ›ã™ã‚‹é–¢æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104779c1",
   "metadata": {},
   "source": [
    "ä½¿ç”¨ã™ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆæ•° $K$ ã‚’å›ºå®šã—ã€ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç·æ•° $n$ ã‚’å¢—ã‚„ã™ã“ã¨ã§ã€åŠ¹ç‡çš„ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç·æ•°ã‚’å¢—åŠ ã§ãã‚‹:\n",
    "\n",
    "- ãƒˆãƒ¼ã‚¯ãƒ³æ¯ã«ä½¿ç”¨ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ **ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°** ï¼ˆactive parameter countï¼‰ã¨å‘¼ã¶\n",
    "- ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç·æ•°ã‚’ **ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°** ï¼ˆsparse parameter countï¼‰ã¨å‘¼ã¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630079c7",
   "metadata": {},
   "source": [
    "MoEå±¤ã¯ã€å˜ä¸€ã‚‚ã—ãã¯è¤‡æ•°ã®GPUã§åŠ¹ç‡çš„ã«å®Ÿè¡Œã§ãã‚‹:\n",
    "\n",
    "- å˜ä¸€GPUã§ã®åŠ¹ç‡åŒ–æ‰‹æ³•\n",
    "    - [Megablocks][1]: MoEã®FFNã®æ“ä½œã‚’å¤§ããªã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ä¹—ç®—ã¨ã—ã¦æ‰±ã„ã€å®Ÿè¡Œé€Ÿåº¦ã‚’å‘ä¸Šã•ã›ã‚‹\n",
    "- è¤‡æ•°GPUã§ã®åŠ¹ç‡åŒ–æ‰‹æ³•\n",
    "    - ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—åŒ–ï¼ˆModel Parallelism techniquesï¼‰: ãƒ¢ãƒ‡ãƒ«ã‚’å±¤ã”ã¨ã«åˆ†ã‘ã¦è¤‡æ•°ã®GPUã«å±•é–‹ã™ã‚‹\n",
    "    - [ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆä¸¦åˆ—åŒ–][2]ï¼ˆExpert Parallelism: EPï¼‰: ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†ã‘ã¦è¤‡æ•°ã®GPUã«å±•é–‹ã™ã‚‹\n",
    "\n",
    "[1]: https://proceedings.mlsys.org/paper_files/paper/2023/hash/5a54f79333768effe7e8927bcccffe40-Abstract-mlsys2023.html\n",
    "[2]: https://arxiv.org/abs/1701.06538\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d208d",
   "metadata": {},
   "source": [
    "Mixtralã§ã¯ã€ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’SwiGLUã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§å®Ÿè£…ã—ã€ä½¿ç”¨ã™ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆæ•°ã‚’ $K=2$ ã¨ã™ã‚‹:\n",
    "\n",
    "$$\n",
    "y = \\sum_{i=0}^{n-1} \\text{Softmax}(\\text{Top2}(x\\cdot W_g))_i \\cdot \\text{SwiGLU}_i(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc29f0bc",
   "metadata": {},
   "source": [
    "- $\\text{Softmax}(\\text{Top2}(x\\cdot W_g))_i$: $i$ ç•ªç›®ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã«å¯¾ã™ã‚‹é‡ã¿\n",
    "- $\\text{SwiGLU}_i(x)$: $i$ ç•ªç›®ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®å‡ºåŠ›"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4e45c",
   "metadata": {},
   "source": [
    "## ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946da742",
   "metadata": {},
   "source": [
    "Mixtralã¨Llamaã‚’ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§è©•ä¾¡ã—æ¯”è¼ƒ:\n",
    "\n",
    "- å¸¸è­˜æ¨è«–ï¼ˆ0-shotï¼‰\n",
    "    - Hellaswag: æ–‡è„ˆã‹ã‚‰è‡ªç„¶ã«ç¶šãçµæœ«ã‚’é¸ã¶\n",
    "    - Winogrande: ä»£åè©ãŒæŒ‡ã—ã¦ã„ã‚‹å˜èªã‚’é¸ã¶\n",
    "    - PIQAï¼ˆPhysical Interaction Question Answeringï¼‰: ç‰©ç†çš„ãªç†è§£ãŒå¿…è¦ãªé¸æŠè‚¢ã‚’é¸ã¶\n",
    "    - SIQAï¼ˆSocial Interaction QAï¼‰:äººã®æ„Ÿæƒ…ã®ç†è§£ãŒå¿…è¦ãªé¸æŠè‚¢ã‚’é¸ã¶\n",
    "    - OpenbookQA: ä¸€èˆ¬çš„ãªç§‘å­¦çš„äº‹å®Ÿï¼ˆOpen Bookï¼‰ã®ç†è§£ãŒå¿…è¦ãªé¸æŠè‚¢ã‚’é¸ã¶\n",
    "    - ARC-Easyï¼ˆAI2 Reasoning Challengeï¼‰: å°å­¦ç”Ÿãƒ¬ãƒ™ãƒ«ã®ç§‘å­¦ã®ç†è§£ãŒå¿…è¦ãªé¸æŠè‚¢ã‚’é¸ã¶\n",
    "    - ARC-Challenge: ä¸­å­¦ç”Ÿãƒ¬ãƒ™ãƒ«ã®ç§‘å­¦ã®ç†è§£ãŒå¿…è¦ãªé¸æŠè‚¢ã‚’é¸ã¶\n",
    "    - CommonsenseQA: ç¤¾ä¼šå¸¸è­˜ã®ç†è§£ãŒå¿…è¦ãªé¸æŠè‚¢ã‚’é¸ã¶\n",
    "- ä¸–ç•ŒçŸ¥è­˜ï¼ˆ5-shotï¼‰\n",
    "    - NaturalQuestions: ä¸ãˆã‚‰ã‚ŒãŸWikipediaã®ãƒšãƒ¼ã‚¸ã‹ã‚‰é•·ã„å›ç­”ã¨çŸ­ã„å›ç­”ã‚’æŠ½å‡ºã™ã‚‹\n",
    "    - TriviaQA: ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã‚„Wikipediaã®ãƒšãƒ¼ã‚¸ãŒä¸ãˆã‚‰ã‚Œã€ãã‚Œã‚‰ã‚’çµ±åˆã™ã‚‹å¿…è¦ã®ã‚ã‚‹å›ç­”ã‚’æŠ½å‡ºã™ã‚‹\n",
    "- èª­è§£ï¼ˆ0-shotï¼‰\n",
    "    - BoolQ: ä¸ãˆã‚‰ã‚ŒãŸæ–‡ç« ã«å¯¾ã—ã¦ã€ã¯ã„/ã„ã„ãˆã§å›ç­”ã™ã‚‹\n",
    "    - QuACï¼ˆQuestion Answering in Contextï¼‰: ä¸€äººã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé€£ç¶šã—ã¦è³ªå•ã—ãã‚Œã«å¯¾ã—ã¦å›ç­”ã—ç¶šã‘ã‚‹\n",
    "- æ•°å­¦\n",
    "    - GSM8Kï¼ˆ8-shotï¼‰: å°å­¦ç”Ÿãƒ¬ãƒ™ãƒ«ã®ç®—æ•°å•é¡Œ\n",
    "    - MATHï¼ˆ4-shotï¼‰: ç«¶æŠ€æ•°å­¦ãƒ¬ãƒ™ãƒ«ã®é›£ã—ã„æ•°å­¦å•é¡Œ\n",
    "- ã‚³ãƒ¼ãƒ‰\n",
    "    - Humanevalï¼ˆ0-shotï¼‰: äººãŒä½œæˆã—ãŸPythoné–¢æ•°ã‚’ãƒ’ãƒ³ãƒˆã‹ã‚‰å®Œæˆã•ã›ã‚‹\n",
    "    - MBPPï¼ˆMostly Basic Python Programmingï¼‰ï¼ˆ3-shotï¼‰: åˆå¿ƒè€…å‘ã‘ã®åŸºæœ¬çš„ãªPythonãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å•é¡Œã‚’è§£ã\n",
    "- ç·åˆ\n",
    "    - MMLUï¼ˆMassive Multitask Language Understandingï¼‰ï¼ˆ5-shotï¼‰: 57ã®ç•°ãªã‚‹åˆ†é‡ã®é¸æŠå•é¡Œ\n",
    "    - BBHï¼ˆBig-Bench Hardï¼‰ï¼ˆ3-shotï¼‰: ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ãŒè‹¦æ‰‹ã¨ã™ã‚‹23ã®æŒ‘æˆ¦çš„ãªã‚¿ã‚¹ã‚¯\n",
    "    - AGI Evalï¼ˆ3-5-shotï¼‰: ç±³å›½ã®å¤§å­¦å…¥å­¦è©¦é¨“ã€æ³•ç§‘å¤§å­¦é™¢è©¦é¨“ã€åŒ»å¸«å›½å®¶è©¦é¨“ãªã©ã®é¸æŠå•é¡Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b071e43",
   "metadata": {},
   "source": [
    "Mixtralã¯ã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒ5å€å¤šã„Llama2 70Bã‚’å¤šãã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ä¸Šå›ã£ãŸï¼ˆã‚³ãƒ¼ãƒ‰ã¨æ•°å­¦ãŒå¼·ã„ï¼‰:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ec94f",
   "metadata": {},
   "source": [
    "![](image/figure_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc4ba4",
   "metadata": {},
   "source": [
    "![](image/table_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54bdb7d",
   "metadata": {},
   "source": [
    "Mixtralã¯ã€ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå°‘ãªãæ€§èƒ½ãŒé«˜ã„:\n",
    "\n",
    "![](image/figure_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd406ac",
   "metadata": {},
   "source": [
    "Mixtralã¯ã€LlaMA 2 70Bã‚ˆã‚Šå„ªã‚Œã€GPT-3.5ï¼ˆGPT-3.5-Turboï¼‰ã«åŒ¹æ•µã™ã‚‹æ€§èƒ½ã‚’ç¤ºã—ãŸ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe974526",
   "metadata": {},
   "source": [
    "![](image/table_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b6938",
   "metadata": {},
   "source": [
    "Mixtralã¯ã€è‹±èªã®ä»–ã«ãƒ•ãƒ©ãƒ³ã‚¹èªãƒ»ãƒ‰ã‚¤ãƒ„èªãƒ»ã‚¹ãƒšã‚¤ãƒ³èªãƒ»ã‚¤ã‚¿ãƒªã‚¢èªã§Llama2 70 Bã‚’å¤§å¹…ã«ä¸Šå›ã‚‹æ€§èƒ½:\n",
    "\n",
    "![](image/table_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00793e7",
   "metadata": {},
   "source": [
    "é•·ã„æ–‡ç« ã‹ã‚‰æ¢ã—å‡ºã™ã‚¿ã‚¹ã‚¯ï¼ˆ[passkey retrieval][1]ï¼‰ã§ã¯ã€100%ã®æ¤œç´¢æ€§èƒ½ã‚’ç¤ºã—ã€å›°æƒ‘åº¦ï¼ˆperplexityï¼‰ã‚‚é•·ã•ã«å¿œã˜ã¦æ¸›å°‘:\n",
    "\n",
    "![](image/figure_4.png)\n",
    "\n",
    "[1]: https://arxiv.org/abs/2305.16300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e849f",
   "metadata": {},
   "source": [
    "Llama 2ã¨æ¯”è¼ƒã—ã¦ã€BBQï¼ˆBias Benchmark for QAï¼‰ã§ä½ã„ãƒã‚¤ã‚¢ã‚¹ã‚’ç¤ºã—ãŸ:\n",
    "\n",
    "![](image/figure_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11470c2d",
   "metadata": {},
   "source": [
    "æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯ã€MT-Benchã§ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚¦ã‚§ã‚¤ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¸­ã§æœ€ã‚‚é«˜ã„:\n",
    "\n",
    "![](image/figure_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbd7cd",
   "metadata": {},
   "source": [
    "## ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°åˆ†æ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af09458c",
   "metadata": {},
   "source": [
    "The Pileã®æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã„ã€ãƒˆãƒ”ãƒƒã‚¯æ¯ã«0å±¤ç›®ãƒ»15å±¤ç›®ãƒ»31å±¤ç›®ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆé¸æŠçŠ¶æ…‹ã‚’æ¸¬å®š\n",
    "\n",
    "ãƒˆãƒ”ãƒƒã‚¯ã«åŸºã¥ã„ãŸã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é¸æŠã«æ˜ç¢ºãªãƒ‘ã‚¿ãƒ¼ãƒ³ã¯è¦‹ã‚‰ã‚Œãªã‹ã£ãŸï¼ˆæ•°å­¦ã®ã¿ã‚ãšã‹ã«åå¿œï¼‰:\n",
    "\n",
    "![](image/figure_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8077637",
   "metadata": {},
   "source": [
    "![](image/table_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d1b54",
   "metadata": {},
   "source": [
    "ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®å‰²å½“ã§ã¯ã€`self`ãƒ»`Question`ãƒ»ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆãƒ»é€£ç¶šã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ãŒåŒã˜ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:\n",
    "\n",
    "![](image/figure_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8549895d",
   "metadata": {},
   "source": [
    "## å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d377de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU transformers==4.57.1\n",
    "%pip install -qU sentencepiece protobuf bitsandbytes accelerate\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "except ImportError:\n",
    "    from dotenv import load_dotenv\n",
    "    import os\n",
    "    load_dotenv()\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "assert HF_TOKEN\n",
    "\n",
    "import os\n",
    "import logging as logging_\n",
    "import transformers\n",
    "import bitsandbytes\n",
    "from transformers import PretrainedConfig\n",
    "from transformers.utils import logging\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n",
    "\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from typing import TYPE_CHECKING, Any, Optional\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from transformers.convert_slow_tokenizer import import_protobuf\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "from transformers.utils import logging\n",
    "from transformers.utils.import_utils import requires\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDAã‚’ä½¿ç”¨ã§ãã¾ã›ã‚“\"\n",
    "\n",
    "from transformers.utils.generic import check_model_inputs\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.cache_utils import Cache, DynamicCache\n",
    "from transformers.generation import GenerationMixin\n",
    "from transformers.integrations import use_kernel_forward_from_hub\n",
    "from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask\n",
    "from transformers.modeling_flash_attention_utils import FlashAttentionKwargs\n",
    "from transformers.modeling_layers import (\n",
    "    GenericForQuestionAnswering,\n",
    "    GenericForSequenceClassification,\n",
    "    GenericForTokenClassification,\n",
    "    GradientCheckpointingLayer,\n",
    ")\n",
    "from transformers.modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n",
    "from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n",
    "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n",
    "from transformers.processing_utils import Unpack\n",
    "from transformers.utils import TransformersKwargs, auto_docstring, can_return_tuple\n",
    "from transformers.utils.deprecation import deprecate_kwarg\n",
    "from transformers.utils.generic import OutputRecorder\n",
    "from transformers.models.mixtral.configuration_mixtral import MixtralConfig\n",
    "\n",
    "# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "# ãƒ­ã‚°è¨­å®š\n",
    "\n",
    "if os.path.exists('debug.log'):\n",
    "    os.remove('debug.log')\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging_.DEBUG:\n",
    "            level = 'ğŸŸ¦'\n",
    "        case logging_.INFO:\n",
    "            level = 'ğŸŸ©'\n",
    "        case logging_.WARNING:\n",
    "            level = 'ğŸŸ¨'\n",
    "        case logging_.ERROR:\n",
    "            level = 'ğŸŸ¥'\n",
    "        case logging_.CRITICAL:\n",
    "            level = 'ğŸ›‘'\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logging.set_verbosity_debug()\n",
    "logger = logging.get_logger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging_.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging_.FileHandler('debug.log')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging_.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.info(f\"Transformers version: {transformers.__version__}\")\n",
    "logger.info(f\"Numpy version: {np.__version__}\")\n",
    "logger.info(f\"BitsAndBytes version: {bitsandbytes.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aba00b",
   "metadata": {},
   "source": [
    "### LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eaeb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’æ¸ˆã¿èªå½™ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer.model\"}\n",
    "\n",
    "# å˜èªã®å…ˆé ­ã‚’ç¤ºã™ç‰¹æ®Šæ–‡å­—\n",
    "SPIECE_UNDERLINE = \"â–\"\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your \\\n",
    "answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure\\\n",
    " that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not \\\n",
    "correct. If you don't know the answer to a question, please don't share false information.\"\"\"  # fmt: skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ca666",
   "metadata": {},
   "outputs": [],
   "source": [
    "@requires(backends=(\"sentencepiece\",))\n",
    "class LlamaTokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"\n",
    "    Construct a Llama tokenizer. Based on byte-level Byte-Pair-Encoding. The default padding token is unset as there is\n",
    "    no padding token in the original model.\n",
    "\n",
    "    Args:\n",
    "        vocab_file (`str`):\n",
    "            Path to the vocabulary file.\n",
    "        unk_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<unk>\"`):\n",
    "            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
    "            token instead.\n",
    "        bos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<s>\"`):\n",
    "            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n",
    "        eos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"</s>\"`):\n",
    "            The end of sequence token.\n",
    "        pad_token (`str` or `tokenizers.AddedToken`, *optional*):\n",
    "            A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by\n",
    "            attention mechanisms or loss computation.\n",
    "        sp_model_kwargs (`dict[str, Any]`, `Optional`, *optional*):\n",
    "            Will be passed to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for\n",
    "            SentencePiece](https://github.com/google/sentencepiece/tree/master/python) can be used, among other things,\n",
    "            to set:\n",
    "\n",
    "            - `enable_sampling`: Enable subword regularization.\n",
    "            - `nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.\n",
    "\n",
    "              - `nbest_size = {0,1}`: No sampling is performed.\n",
    "              - `nbest_size > 1`: samples from the nbest_size results.\n",
    "              - `nbest_size < 0`: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)\n",
    "                using forward-filtering-and-backward-sampling algorithm.\n",
    "\n",
    "            - `alpha`: Smoothing parameter for unigram sampling, and dropout probability of merge operations for\n",
    "              BPE-dropout.\n",
    "\n",
    "        add_bos_token (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to add an `bos_token` at the start of sequences.\n",
    "        add_eos_token (`bool`, *optional*, defaults to `False`):\n",
    "            Whether or not to add an `eos_token` at the end of sequences.\n",
    "        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n",
    "            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n",
    "            extra spaces.\n",
    "        use_default_system_prompt (`bool`, *optional*, defaults to `False`):\n",
    "            Whether or not the default system prompt for Llama should be used.\n",
    "        spaces_between_special_tokens (`bool`, *optional*, defaults to `False`):\n",
    "            Whether or not to add spaces between special tokens.\n",
    "        legacy (`bool`, *optional*):\n",
    "            Whether or not the `legacy` behavior of the tokenizer should be used. Legacy is before the merge of #24622\n",
    "            and #25224 which includes fixes to properly handle tokens that appear after special tokens.\n",
    "            Make sure to also set `from_slow` to `True`.\n",
    "            A simple example:\n",
    "\n",
    "            - `legacy=True`:\n",
    "            ```python\n",
    "            >>> from transformers import LlamaTokenizerFast\n",
    "\n",
    "            >>> tokenizer = LlamaTokenizerFast.from_pretrained(\"huggyllama/llama-7b\", legacy=True, from_slow=True)\n",
    "            >>> tokenizer.encode(\"Hello <s>.\") # 869 is 'â–.'\n",
    "            [1, 15043, 29871, 1, 869]\n",
    "            ```\n",
    "            - `legacy=False`:\n",
    "            ```python\n",
    "            >>> from transformers import LlamaTokenizerFast\n",
    "\n",
    "            >>> tokenizer = LlamaTokenizerFast.from_pretrained(\"huggyllama/llama-7b\", legacy=False, from_slow=True)\n",
    "            >>> tokenizer.encode(\"Hello <s>.\")  # 29889 is '.'\n",
    "            [1, 15043, 29871, 1, 29889]\n",
    "            ```\n",
    "            Checkout the [pull request](https://github.com/huggingface/transformers/pull/24565) for more details.\n",
    "        add_prefix_space (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n",
    "            other word. Again, this should be set with `from_slow=True` to make sure it's taken into account.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    model_input_names = [\"input_ids\", \"attention_mask\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        unk_token=\"<unk>\",\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        pad_token=None,\n",
    "        sp_model_kwargs: Optional[dict[str, Any]] = None,\n",
    "        add_bos_token=True,\n",
    "        add_eos_token=False,\n",
    "        clean_up_tokenization_spaces=False,\n",
    "        use_default_system_prompt=False,\n",
    "        spaces_between_special_tokens=False,\n",
    "        legacy=None,\n",
    "        add_prefix_space=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        logger.info(f\"LlamaTokenizerã®åˆæœŸåŒ–é–‹å§‹ vocab_file={vocab_file} unk_token={unk_token} bos_token={bos_token} eos_token={eos_token} pad_token={pad_token} legacy={legacy} add_prefix_space={add_prefix_space}\")\n",
    "\n",
    "        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n",
    "        bos_token = AddedToken(bos_token, normalized=False, special=True) if isinstance(bos_token, str) else bos_token\n",
    "        eos_token = AddedToken(eos_token, normalized=False, special=True) if isinstance(eos_token, str) else eos_token\n",
    "        unk_token = AddedToken(unk_token, normalized=False, special=True) if isinstance(unk_token, str) else unk_token\n",
    "        pad_token = AddedToken(pad_token, normalized=False, special=True) if isinstance(pad_token, str) else pad_token\n",
    "\n",
    "        if legacy is None:\n",
    "            logger.warning_once(\n",
    "                f\"You are using the default legacy behaviour of the {self.__class__}. This is\"\n",
    "                \" expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you.\"\n",
    "                \" If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it\"\n",
    "                \" means, and thoroughly read the reason why this was added as explained in\"\n",
    "                \" https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file\"\n",
    "                \" you can ignore this message\"\n",
    "            )\n",
    "            legacy = True\n",
    "\n",
    "        self.legacy = legacy\n",
    "        self.vocab_file = vocab_file\n",
    "        self.add_bos_token = add_bos_token\n",
    "        self.add_eos_token = add_eos_token\n",
    "        self.use_default_system_prompt = use_default_system_prompt\n",
    "        self.sp_model = self.get_spm_processor(kwargs.pop(\"from_slow\", False))\n",
    "        self.add_prefix_space = add_prefix_space\n",
    "\n",
    "        super().__init__(\n",
    "            bos_token=bos_token,\n",
    "            eos_token=eos_token,\n",
    "            unk_token=unk_token,\n",
    "            pad_token=pad_token,\n",
    "            add_bos_token=add_bos_token,\n",
    "            add_eos_token=add_eos_token,\n",
    "            sp_model_kwargs=self.sp_model_kwargs,\n",
    "            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
    "            use_default_system_prompt=use_default_system_prompt,\n",
    "            spaces_between_special_tokens=spaces_between_special_tokens,\n",
    "            legacy=legacy,\n",
    "            add_prefix_space=add_prefix_space,\n",
    "            **kwargs,\n",
    "        )\n",
    "        logger.info(\"LlamaTokenizerã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    @property\n",
    "    def unk_token_length(self):\n",
    "        return len(self.sp_model.encode(str(self.unk_token)))\n",
    "\n",
    "    # Copied from transformers.models.t5.tokenization_t5.T5Tokenizer.get_spm_processor\n",
    "    def get_spm_processor(self, from_slow=False):\n",
    "        logger.info(f\"SentencePieceProcessorã®å–å¾— from_slow={from_slow} legacy={self.legacy}\")\n",
    "\n",
    "        tokenizer = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n",
    "        if self.legacy or from_slow:  # no dependency on protobuf\n",
    "            tokenizer.Load(self.vocab_file)\n",
    "            return tokenizer\n",
    "\n",
    "        with open(self.vocab_file, \"rb\") as f:\n",
    "            sp_model = f.read()\n",
    "            model_pb2 = import_protobuf(f\"The new behaviour of {self.__class__.__name__} (with `self.legacy = False`)\")\n",
    "            model = model_pb2.ModelProto.FromString(sp_model)\n",
    "            normalizer_spec = model_pb2.NormalizerSpec()\n",
    "            normalizer_spec.add_dummy_prefix = False\n",
    "            model.normalizer_spec.MergeFrom(normalizer_spec)\n",
    "            sp_model = model.SerializeToString()\n",
    "            tokenizer.LoadFromSerializedProto(sp_model)\n",
    "\n",
    "        logger.info(\"SentencePieceProcessorã®å–å¾—å®Œäº†\")\n",
    "        return tokenizer\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state[\"sp_model\"] = None\n",
    "        state[\"sp_model_proto\"] = self.sp_model.serialized_model_proto()\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        self.__dict__.update(d)\n",
    "        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n",
    "        self.sp_model.LoadFromSerializedProto(self.sp_model_proto)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"Returns vocab size\"\"\"\n",
    "        return self.sp_model.get_piece_size()\n",
    "\n",
    "    def get_vocab(self):\n",
    "        \"\"\"Returns vocab as a dict\"\"\"\n",
    "        logger.info(\"èªå½™ã®å–å¾—é–‹å§‹\")\n",
    "\n",
    "        vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)}\n",
    "        vocab.update(self.added_tokens_encoder)\n",
    "\n",
    "        logger.info(f\"èªå½™ã®å–å¾—å®Œäº† vocab_size={len(vocab)}\")\n",
    "        return vocab\n",
    "\n",
    "    # Copied from transformers.models.t5.tokenization_t5.T5Tokenizer.tokenize\n",
    "    def tokenize(self, text: \"TextInput\", **kwargs) -> list[str]:\n",
    "        \"\"\"\n",
    "        Converts a string to a list of tokens. If `self.legacy` is set to `False`, a prefix token is added unless the\n",
    "        first token is special.\n",
    "        \"\"\"\n",
    "        logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒ³åŒ–é–‹å§‹ text={text} legacy={self.legacy} add_prefix_space={self.add_prefix_space}\")\n",
    "\n",
    "        # True\n",
    "        if self.legacy or len(text) == 0:\n",
    "\n",
    "            # 'Hello' -> '_Hello'\n",
    "            res = super().tokenize(text, **kwargs)\n",
    "            logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒ³åŒ–å®Œäº†ï¼ˆlegacyï¼‰ tokens={res}\")\n",
    "            return res\n",
    "\n",
    "        text = text.replace(SPIECE_UNDERLINE, \" \")\n",
    "        if self.add_prefix_space:\n",
    "            text = SPIECE_UNDERLINE + text\n",
    "\n",
    "        tokens = super().tokenize(text, **kwargs)\n",
    "\n",
    "        if len(tokens) > 1 and tokens[0] == SPIECE_UNDERLINE and tokens[1] in self.all_special_tokens:\n",
    "            tokens = tokens[1:]\n",
    "        return tokens\n",
    "\n",
    "    # Copied from transformers.models.t5.tokenization_t5.T5Tokenizer._tokenize\n",
    "    def _tokenize(self, text, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns a tokenized string.\n",
    "\n",
    "        We de-activated the `add_dummy_prefix` option, thus the sentencepiece internals will always strip any\n",
    "        SPIECE_UNDERLINE. For example: `self.sp_model.encode(f\"{SPIECE_UNDERLINE}Hey\", out_type = str)` will give\n",
    "        `['H', 'e', 'y']` instead of `['â–He', 'y']`. Thus we always encode `f\"{unk_token}text\"` and strip the\n",
    "        `unk_token`. Here is an example with `unk_token = \"<unk>\"` and `unk_token_length = 4`.\n",
    "        `self.tokenizer.sp_model.encode(\"<unk> Hey\", out_type = str)[4:]`.\n",
    "        \"\"\"\n",
    "        logger.info(f\"_tokenizeã®é–‹å§‹ text={text} legacy={self.legacy}\")\n",
    "\n",
    "        # True\n",
    "        if self.legacy or not text.startswith((SPIECE_UNDERLINE, \" \")):\n",
    "            # 'Hello' -> '_Hello'\n",
    "            res = self.sp_model.encode(text, out_type=str)\n",
    "            logger.info(f\"_tokenizeã®å®Œäº†ï¼ˆlegacyï¼‰ tokens={res}\")\n",
    "            return res\n",
    "\n",
    "        # 1. Encode string + prefix ex: \"<unk> Hey\"\n",
    "        tokens = self.sp_model.encode(self.unk_token + text, out_type=str)\n",
    "        # 2. Remove self.unk_token from ['<','unk','>', 'â–Hey']\n",
    "        res = tokens[self.unk_token_length :] if len(tokens) >= self.unk_token_length else tokens\n",
    "        return res\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n",
    "        logger.info(f\"_convert_token_to_idã®é–‹å§‹ token={token}\")\n",
    "        # '_Hello' -> 22557\n",
    "        res = self.sp_model.piece_to_id(token)\n",
    "        logger.info(f\"_convert_token_to_idã®å®Œäº† token={token} id={res}\")\n",
    "        return res\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
    "        # logger.info(f\"_convert_id_to_tokenã®é–‹å§‹ id={index}\")\n",
    "        token = self.sp_model.IdToPiece(index)\n",
    "        res = token\n",
    "        # logger.info(f\"_convert_id_to_tokenã®å®Œäº† id={index} token={res}\")\n",
    "        return res\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n",
    "        logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®æ–‡å­—åˆ—å¤‰æ›é–‹å§‹ tokens={tokens} legacy={self.legacy} add_prefix_space={self.add_prefix_space}\")\n",
    "\n",
    "        # since we manually add the prefix space, we have to remove it when decoding\n",
    "        # ['_Hello', ',] -> ['Hello', ',']\n",
    "        if tokens[0].startswith(SPIECE_UNDERLINE) and self.add_prefix_space:\n",
    "            tokens[0] = tokens[0][1:]\n",
    "            logger.debug(f\"æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰æ¥é ­è¾ã‚¹ãƒšãƒ¼ã‚¹ã‚’å‰Šé™¤ {tokens=}\")\n",
    "\n",
    "        current_sub_tokens = []\n",
    "        out_string = \"\"\n",
    "        prev_is_special = False\n",
    "        for i, token in enumerate(tokens):\n",
    "            # make sure that special tokens are not decoded using sentencepiece model\n",
    "            if token in self.all_special_tokens:\n",
    "                if not prev_is_special and i != 0 and self.legacy:\n",
    "                    out_string += \" \"\n",
    "                out_string += self.sp_model.decode(current_sub_tokens) + token\n",
    "                logger.debug(f\"{current_sub_tokens=} ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—é›†ç´„ {out_string=}\")\n",
    "                prev_is_special = True\n",
    "                current_sub_tokens = []\n",
    "            else:\n",
    "                if prev_is_special and i == 1 and self.add_prefix_space and not token.startswith(SPIECE_UNDERLINE):\n",
    "                    out_string += \" \"\n",
    "                current_sub_tokens.append(token)\n",
    "                logger.debug(f\"ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã§ã¯ãªã„ã®ã§ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ  {current_sub_tokens=}\")\n",
    "                prev_is_special = False\n",
    "\n",
    "        out_string += self.sp_model.decode(current_sub_tokens)\n",
    "        logger.debug(f\"æœ€å¾Œã®ãƒãƒƒãƒ•ã‚¡ {current_sub_tokens=} ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—é›†ç´„ {out_string=}\")\n",
    "\n",
    "        logger.info(f\"ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®æ–‡å­—åˆ—å¤‰æ›å®Œäº† {out_string=}\")\n",
    "        return out_string\n",
    "\n",
    "    def save_vocabulary(self, save_directory, filename_prefix: Optional[str] = None) -> tuple[str]:\n",
    "        \"\"\"\n",
    "        Save the vocabulary and special tokens file to a directory.\n",
    "\n",
    "        Args:\n",
    "            save_directory (`str`):\n",
    "                The directory in which to save the vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            `Tuple(str)`: Paths to the files saved.\n",
    "        \"\"\"\n",
    "        logger.info(f\"èªå½™ã®ä¿å­˜é–‹å§‹ save_directory={save_directory} filename_prefix={filename_prefix}\")\n",
    "        if not os.path.isdir(save_directory):\n",
    "            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n",
    "            return\n",
    "        out_vocab_file = os.path.join(\n",
    "            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n",
    "        )\n",
    "\n",
    "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file) and os.path.isfile(self.vocab_file):\n",
    "            copyfile(self.vocab_file, out_vocab_file)\n",
    "        elif not os.path.isfile(self.vocab_file):\n",
    "            with open(out_vocab_file, \"wb\") as fi:\n",
    "                content_spiece_model = self.sp_model.serialized_model_proto()\n",
    "                fi.write(content_spiece_model)\n",
    "\n",
    "        logger.info(f\"èªå½™ã®ä¿å­˜å®Œäº† out_vocab_file={out_vocab_file}\")\n",
    "        return (out_vocab_file,)\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        logger.info(f\"ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ä»˜ãå…¥åŠ›ã®æ§‹ç¯‰é–‹å§‹ token_ids_0={token_ids_0} token_ids_1={token_ids_1}\")\n",
    "\n",
    "        # [1]\n",
    "        bos_token_id = [self.bos_token_id] if self.add_bos_token else []\n",
    "\n",
    "        # []\n",
    "        eos_token_id = [self.eos_token_id] if self.add_eos_token else []\n",
    "\n",
    "        # [1, 22557]\n",
    "        output = bos_token_id + token_ids_0 + eos_token_id\n",
    "\n",
    "        # False\n",
    "        if token_ids_1 is not None:\n",
    "            output = output + bos_token_id + token_ids_1 + eos_token_id\n",
    "\n",
    "        logger.info(f\"ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ä»˜ãå…¥åŠ›ã®æ§‹ç¯‰å®Œäº† output={output}\")\n",
    "        return output\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n",
    "    ) -> list[int]:\n",
    "        \"\"\"\n",
    "        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer `prepare_for_model` method.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (`list[int]`):\n",
    "                List of IDs.\n",
    "            token_ids_1 (`list[int]`, *optional*):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not the token list is already formatted with special tokens for the model.\n",
    "\n",
    "        Returns:\n",
    "            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "        logger.info(f\"ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚¹ã‚¯ã®å–å¾—é–‹å§‹ token_ids_0={token_ids_0} token_ids_1={token_ids_1} already_has_special_tokens={already_has_special_tokens}\")\n",
    "        if already_has_special_tokens:\n",
    "            res = super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n",
    "            )\n",
    "            logger.info(f\"ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚¹ã‚¯ã®å–å¾—å®Œäº†ï¼ˆã™ã§ã«ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®šæ¸ˆã¿ï¼‰ mask={res}\")\n",
    "            return res\n",
    "\n",
    "        bos_token_id = [1] if self.add_bos_token else []\n",
    "        eos_token_id = [1] if self.add_eos_token else []\n",
    "\n",
    "        if token_ids_1 is None:\n",
    "            res = bos_token_id + ([0] * len(token_ids_0)) + eos_token_id\n",
    "            logger.info(f\"ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚¹ã‚¯ã®å–å¾—å®Œäº†ï¼ˆtoken_ids_1ãŒç©ºï¼‰ mask={res}\")\n",
    "            return res\n",
    "        res = (\n",
    "            bos_token_id\n",
    "            + ([0] * len(token_ids_0))\n",
    "            + eos_token_id\n",
    "            + bos_token_id\n",
    "            + ([0] * len(token_ids_1))\n",
    "            + eos_token_id\n",
    "        )\n",
    "        logger.info(f\"ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚¹ã‚¯ã®å–å¾—å®Œäº† mask={res}\")\n",
    "        return res\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n",
    "    ) -> list[int]:\n",
    "        \"\"\"\n",
    "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task. An ALBERT\n",
    "        sequence pair mask has the following format:\n",
    "\n",
    "        ```\n",
    "        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence |\n",
    "        ```\n",
    "\n",
    "        if token_ids_1 is None, only returns the first portion of the mask (0s).\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (`list[int]`):\n",
    "                List of ids.\n",
    "            token_ids_1 (`list[int]`, *optional*):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "\n",
    "        Returns:\n",
    "            `list[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n",
    "        \"\"\"\n",
    "        logger.info(f\"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚¿ã‚¤ãƒ—IDã®ä½œæˆé–‹å§‹ token_ids_0={token_ids_0} token_ids_1={token_ids_1}\")\n",
    "\n",
    "        # [1]\n",
    "        bos_token_id = [self.bos_token_id] if self.add_bos_token else []\n",
    "\n",
    "        # []\n",
    "        eos_token_id = [self.eos_token_id] if self.add_eos_token else []\n",
    "\n",
    "        # [1, 22557] -> [0, 0]\n",
    "        output = [0] * len(bos_token_id + token_ids_0 + eos_token_id)\n",
    "\n",
    "        # False\n",
    "        if token_ids_1 is not None:\n",
    "            output += [1] * len(bos_token_id + token_ids_1 + eos_token_id)\n",
    "\n",
    "        logger.info(f\"ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚¿ã‚¤ãƒ—IDã®ä½œæˆå®Œäº† output={output}\")\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d9977c",
   "metadata": {},
   "source": [
    "### MixtralBlockSparseTop2MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2e8b9c",
   "metadata": {},
   "source": [
    "MixtralBlockSparseTop2MLPã¯ã€SMoEã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚¯ãƒ©ã‚¹\n",
    "\n",
    "å®Ÿä½“ã¯ã€SwiGLUã§å®Ÿè£…ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralBlockSparseTop2MLP(nn.Module):\n",
    "    def __init__(self, config: MixtralConfig):\n",
    "        logger.info(f\"MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–é–‹å§‹ {config.intermediate_size=}, {config.hidden_size=}, {config.hidden_act=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 14336\n",
    "        self.ffn_dim = config.intermediate_size\n",
    "\n",
    "        # 4096\n",
    "        self.hidden_dim = config.hidden_size\n",
    "\n",
    "        # 4096 -> 14336\n",
    "        self.w1 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n",
    "\n",
    "        # 14336 -> 4096\n",
    "        self.w2 = nn.Linear(self.ffn_dim, self.hidden_dim, bias=False)\n",
    "\n",
    "        # 14336 -> 4096\n",
    "        self.w3 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n",
    "\n",
    "        # SiLU\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "        logger.info(\"MixtralBlockSparseTop2MLPã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        logger.info(f\"MixtralBlockSparseTop2MLPã®é †ä¼æ’­é–‹å§‹ {hidden_states.shape=}\")\n",
    "\n",
    "        # (1, 4096) -> (1, 14336)\n",
    "        gate = self.act_fn(self.w1(hidden_states))\n",
    "        logger.debug(f\"ã‚²ãƒ¼ãƒˆã‚’è¨ˆç®— {gate.shape=}\")\n",
    "\n",
    "        # (1, 4096) -> (1, 14336)\n",
    "        up = self.w3(hidden_states)\n",
    "        logger.debug(f\"ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— {up.shape=}\")\n",
    "\n",
    "        # (1, 14336) -> (1, 4096)\n",
    "        current_hidden_states = self.w2(gate * up)\n",
    "        logger.debug(f\"ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— {current_hidden_states.shape=}\")\n",
    "\n",
    "        logger.info(f\"MixtralBlockSparseTop2MLPã®é †ä¼æ’­å®Œäº† {current_hidden_states.shape=}\")\n",
    "        return current_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27229a25",
   "metadata": {},
   "source": [
    "### MixtralSparseMoeBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b991ab09",
   "metadata": {},
   "source": [
    "MixtralSparseMoeBlockã¯ã€ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’çµ±åˆã™ã‚‹ã‚¯ãƒ©ã‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7124827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralSparseMoeBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This implementation is\n",
    "    strictly equivalent to standard MoE with full capacity (no\n",
    "    dropped tokens). It's faster since it formulates MoE operations\n",
    "    in terms of block-sparse operations to accommodate imbalanced\n",
    "    assignments of tokens to experts, whereas standard MoE either\n",
    "    (1) drop tokens at the cost of reduced performance or (2) set\n",
    "    capacity factor to number of experts and thus waste computation\n",
    "    and memory on padding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        logger.info(f\"MixtralSparseMoeBlockã®åˆæœŸåŒ–é–‹å§‹ {config.num_local_experts=}, {config.num_experts_per_tok=}, {config.hidden_size=}, {config.intermediate_size=}, {config.router_jitter_noise=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 4096\n",
    "        self.hidden_dim = config.hidden_size\n",
    "\n",
    "        # 14336\n",
    "        self.ffn_dim = config.intermediate_size\n",
    "\n",
    "        # 8\n",
    "        self.num_experts = config.num_local_experts\n",
    "\n",
    "        # 2\n",
    "        self.top_k = config.num_experts_per_tok\n",
    "\n",
    "        # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’åˆæœŸåŒ–\n",
    "        # 4096 -> 8\n",
    "        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n",
    "\n",
    "        # 8ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’åˆæœŸåŒ–\n",
    "        self.experts = nn.ModuleList([MixtralBlockSparseTop2MLP(config) for _ in range(self.num_experts)])\n",
    "\n",
    "        # Jitter parameters\n",
    "        # 0.0\n",
    "        self.jitter_noise = config.router_jitter_noise\n",
    "\n",
    "        logger.info(\"MixtralSparseMoeBlockã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"MixtralSparseMoeBlockã®é †ä¼æ’­é–‹å§‹ {hidden_states.shape=}\")\n",
    "\n",
    "        #########\n",
    "        # åˆæœŸåŒ– #\n",
    "        #########\n",
    "\n",
    "        # (1, 2, 4096)\n",
    "        batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "\n",
    "        # False\n",
    "        if self.training and self.jitter_noise > 0:\n",
    "            hidden_states *= torch.empty_like(hidden_states).uniform_(1.0 - self.jitter_noise, 1.0 + self.jitter_noise)\n",
    "\n",
    "        # (1 * 2, 4096)\n",
    "        hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "        logger.debug(f\"hidden_statesã‚’æ•´å½¢ {hidden_states.shape=}\")\n",
    "\n",
    "        ##############\n",
    "        # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚° #\n",
    "        ##############\n",
    "\n",
    "        # ã‚¹ã‚³ã‚¢ï¼ˆãƒ­ã‚¸ãƒƒãƒˆï¼‰ã‚’è¨ˆç®—\n",
    "        # (2, 4096) -> (2, 8)\n",
    "        router_logits = self.gate(hidden_states)\n",
    "        logger.debug(f\"ãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— {router_logits.shape=}\")\n",
    "\n",
    "        # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é‡ã¿ã‚’è¨ˆç®—ï¼ˆã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆã—ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚’é©ç”¨ï¼‰\n",
    "        # (2, 8)\n",
    "        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "        logger.debug(f\"ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®— {routing_weights.shape=}\")\n",
    "\n",
    "        # ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ\n",
    "        # (2, 2), (2, 2)\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "        logger.debug(f\"ä¸Šä½2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠ {routing_weights.shape=} {selected_experts.shape=}\")\n",
    "\n",
    "        # 2ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é‡ã¿ã‚’æ­£è¦åŒ–ã—ã€ãƒ€ã‚¦ãƒ³ã‚­ãƒ£ã‚¹ãƒˆ\n",
    "        # (2, 2)\n",
    "        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "        routing_weights = routing_weights.to(hidden_states.dtype)\n",
    "        logger.debug(f\"ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’æ­£è¦åŒ– {routing_weights.shape=}\")\n",
    "\n",
    "        ############\n",
    "        # å‡¦ç†ã®å®Ÿè¡Œ #\n",
    "        ############\n",
    "\n",
    "        # çµæœã‚’åˆæœŸåŒ–\n",
    "        # (2, 4096)\n",
    "        final_hidden_states = torch.zeros(\n",
    "            (batch_size * sequence_length, hidden_dim),\n",
    "            dtype=hidden_states.dtype,\n",
    "            device=hidden_states.device\n",
    "        )\n",
    "        logger.debug(f\"æœ€çµ‚çš„ãªhidden_statesã‚’åˆæœŸåŒ– {final_hidden_states.shape=}\")\n",
    "\n",
    "        # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãŒæ‹…å½“ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒã‚¹ã‚¯ã‚’ä½œæˆ\n",
    "        # One hot encode the selected experts to create an expert mask\n",
    "        # this will be used to easily index which expert is going to be sollicitated\n",
    "        # (8, 2, 2)\n",
    "        expert_mask = torch.nn.functional.one_hot(\n",
    "            selected_experts,\n",
    "            num_classes=self.num_experts\n",
    "        ).permute(2, 1, 0)\n",
    "        logger.debug(f\"ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒã‚¹ã‚¯ã‚’ä½œæˆ {expert_mask.shape=}\")\n",
    "\n",
    "        # (4, 1)\n",
    "        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n",
    "        logger.debug(f\"ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ’ãƒƒãƒˆã‚’è¨ˆç®— {expert_hit.shape=}\")\n",
    "\n",
    "        for expert_idx in expert_hit:\n",
    "            # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’å–å¾—\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "\n",
    "            # (1,), (1,)\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n",
    "            logger.debug(f\"ãƒã‚¹ã‚¯ã®å€¤ã‚’å–å¾— {expert_idx[0]=} {idx=} {top_x=}\")\n",
    "\n",
    "            # Index the correct hidden states and compute the expert hidden state for\n",
    "            # the current expert. We need to make sure to multiply the output hidden\n",
    "            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n",
    "            # (1, 4096)\n",
    "            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n",
    "            logger.debug(f\"ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ç¾åœ¨ã®hidden_statesã‚’å–å¾— {current_state.shape=}\")\n",
    "\n",
    "            # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ\n",
    "            # (1, 4096) -> (1, 4096)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "            logger.debug(f\"ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é †ä¼æ’­ã‚’å®Ÿè¡Œ {current_hidden_states.shape=}\")\n",
    "\n",
    "            # çµæœã«é‡ã¿ã‚’æ›ã‘ã¦åŠ ç®—\n",
    "            # (2, 4096)\n",
    "            # However `index_add_` only support torch tensors for indexing so we'll use\n",
    "            # the `top_x` tensor here.\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n",
    "            logger.debug(f\"æœ€çµ‚çš„ãªhidden_statesã«åŠ ç®— {final_hidden_states.shape=}\")\n",
    "\n",
    "        # å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™\n",
    "        # (2, 4096) -> (1, 2, 4096)\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "        logger.debug(f\"hidden_statesã®å½¢çŠ¶ã‚’å…ƒã«æˆ»ã™ {final_hidden_states.shape=}\")\n",
    "\n",
    "        logger.info(f\"MixtralSparseMoeBlockã®é †ä¼æ’­å®Œäº† {final_hidden_states.shape=} {router_logits.shape=}\")\n",
    "        return final_hidden_states, router_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d84a40",
   "metadata": {},
   "source": [
    "### MixtralRMSNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e970939",
   "metadata": {},
   "source": [
    "MixtralRMSNormã¯ã€äºŒä¹—å¹³å‡å¹³æ–¹æ ¹æ­£è¦åŒ–ï¼ˆRoot Mean Square Layer Normalizationï¼‰ã®ã‚¯ãƒ©ã‚¹\n",
    "\n",
    "ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã®æ‹¡å¤§ã‚„ç¸®å°ã‚’æŠ‘åˆ¶ã™ã‚‹ã“ã¨ã§ã€å­¦ç¿’ã‚’å®‰å®šåŒ–ã•ã›ã‚‹\n",
    "\n",
    "ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã®ç°¡æ˜“ç‰ˆã§ã€ä¸­å¿ƒåŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ãŸè»½é‡ç‰ˆ\n",
    "\n",
    "$$\n",
    "y_i = \\frac{x_i}{\\sqrt{\\frac{1}{n} \\sum_{j=1}^{n} x_j^2 + \\epsilon}} \\cdot g_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f080a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_kernel_forward_from_hub(\"RMSNorm\")\n",
    "class MixtralRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        MixtralRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        logger.info(f\"MixtralRMSNormã®åˆæœŸåŒ–é–‹å§‹ {hidden_size=}, {eps=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # (4096,)\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "\n",
    "        # 1e-05\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "        logger.info(\"MixtralRMSNormã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        logger.info(f\"MixtralRMSNormã®é †ä¼æ’­é–‹å§‹ {hidden_states.shape=}\")\n",
    "\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "\n",
    "        # (1, 2, 1)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        logger.debug(f\"åˆ†æ•£ã‚’è¨ˆç®— {variance.shape=}\")\n",
    "\n",
    "        # (1, 2, 4096)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        logger.debug(f\"æ­£è¦åŒ–ã‚’é©ç”¨ {hidden_states.shape=}\")\n",
    "\n",
    "        # (1, 2, 4096)\n",
    "        res = self.weight * hidden_states.to(input_dtype)\n",
    "        logger.debug(f\"ã‚²ã‚¤ãƒ³ã‚’é©ç”¨ {res.shape=}\")\n",
    "\n",
    "        logger.info(f\"MixtralRMSNormã®é †ä¼æ’­å®Œäº† {res.shape=}\")\n",
    "        return res\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c2f76",
   "metadata": {},
   "source": [
    "### MixtralRotaryEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a3c430",
   "metadata": {},
   "source": [
    "MixtralRotaryEmbeddingã¯ã€å›è»¢ä½ç½®åŸ‹ã‚è¾¼ã¿ï¼ˆRotary Position Embedding: RoPEï¼‰ã®é©ç”¨ã«å¿…è¦ãªã‚µã‚¤ãƒ³ã¨ã‚³ã‚µã‚¤ãƒ³ã®å€¤ã‚’è¨ˆç®—ã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "\n",
    "RoPEã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã«å¿œã˜ã¦ã‚¯ã‚¨ãƒªã¨ã‚­ãƒ¼ã‚’å¤‰èª¿ã•ã›ã‚‹ã“ã¨ã§ã€ä½ç½®æƒ…å ±ã‚’è¿½åŠ ã™ã‚‹æ‰‹æ³•\n",
    "\n",
    "ã‚¯ã‚¨ãƒªã®å¤‰èª¿ã®æµã‚Œ:\n",
    "\n",
    "1.  128å€‹ã®è¦ç´ ã‚’æŒã¤ã‚¯ã‚¨ãƒª$Q$ã‚’ã€å‰åŠ64å€‹$Q_1$ã¨å¾ŒåŠ64å€‹ã®$Q_2$ã«åˆ†ã‘ã‚‹\n",
    "2. $Q_1$ã¨$Q_2$ã‹ã‚‰ä¸€ã¤ãšã¤è¦ç´ ã‚’å–ã‚Šã€$(q1, q2)$ã®ãƒšã‚¢ã‚’ä½œã‚‹ï¼ˆ=2æ¬¡å…ƒå¹³é¢ã‚’æ§‹æˆï¼‰\n",
    "3. ãƒšã‚¢ã«å¯¾ã—è§’åº¦$\\theta$ã§å›è»¢ã‚’é©ç”¨ã™ã‚‹\n",
    "\n",
    "2æ¬¡å…ƒå›è»¢ã®å…¬å¼:\n",
    "\n",
    "$$\n",
    "(q_1 \\cos{\\theta} - q_2 \\sin{\\theta}, q_2 \\cos{\\theta} + q_1 \\sin{\\theta})\n",
    "$$\n",
    "\n",
    "å›è»¢ã®è§’åº¦:\n",
    "\n",
    "$$\n",
    "\\theta_{m, i} = m\\cdot b^{-\\frac{2i}{d}}\n",
    "$$\n",
    "\n",
    "- $m$: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ï¼ˆ$0, 1, 2,...$ï¼‰\n",
    "- $b^{-\\frac{2i}{d}}$: å‘¨æ³¢æ•°ã®é€†æ•°\n",
    "    - $b$: åŸºæ•°ï¼ˆ$10000$ï¼‰\n",
    "    - $i$: æ¬¡å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ$0, 1, 2, ..., d/2-1$ï¼‰\n",
    "    - $d$: ãƒ™ã‚¯ãƒˆãƒ«ã®ç·æ¬¡å…ƒæ•°\n",
    "\n",
    "ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ãŒé ã„ï¼ˆå›è»¢è§’åº¦ã®å·®ãŒå¤§ãã„ï¼‰ã»ã©å‘ããŒæƒã‚ãªããªã‚Šã€ã‚¯ã‚¨ãƒªã¨ã‚­ãƒ¼ã®è¡Œåˆ—ç©ã¯å°ã•ããªã‚‹\n",
    "\n",
    "2æ¬¡å…ƒå›è»¢ã®å®Ÿè£…ã¯ã€å…¬å¼ã‚’å±•é–‹ã—ã¦åŠ¹ç‡çš„ã«è¨ˆç®—ã‚’è¡Œã†ï¼ˆapply_rotate_pos_embé–¢æ•°ï¼‰:\n",
    "\n",
    "$$\n",
    "(q_1 \\cos{\\theta} - q_2 \\sin{\\theta}, q_2 \\cos{\\theta} + q_1 \\sin{\\theta})\n",
    "= [q_1, q_2] \\cdot \\cos{\\theta} + [-q_2, q_1] \\cdot \\sin{\\theta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(k_1 \\cos{\\theta} - k_2 \\sin{\\theta}, k_2 \\cos{\\theta} + k_1 \\sin{\\theta})\n",
    "= [k_1, k_2] \\cdot \\cos{\\theta} + [-k_2, k_1] \\cdot \\sin{\\theta}\n",
    "$$\n",
    "\n",
    "$x$ã‚’$[-x_2, x_1]$ã«å¤‰æ›ã™ã‚‹rotate_halé–¢æ•°ã‚’ä½¿ç”¨ã—ã€æ›´ã«å®Ÿè£…ã‚’ç°¡å˜ã«ã™ã‚‹:\n",
    "\n",
    "\n",
    "$$\n",
    "[q_1, q_2] \\cdot \\cos{\\theta} + [-q_2, q_1] \\cdot \\sin{\\theta}\n",
    "= Q \\cdot \\cos{\\theta} + \\text{rotate\\_half}(Q) \\cdot \\sin{\\theta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "[k_1, k_2] \\cdot \\cos{\\theta} + [-k_2, k_1] \\cdot \\sin{\\theta}\n",
    "= K \\cdot \\cos{\\theta} + \\text{rotate\\_half}(K) \\cdot \\sin{\\theta}\n",
    "$$GemmaRotaryEmbeddingã¯ã€å›è»¢ä½ç½®åŸ‹ã‚è¾¼ã¿ï¼ˆRotary Position Embedding: RoPEï¼‰ã®é©ç”¨ã«å¿…è¦ãªã‚µã‚¤ãƒ³ã¨ã‚³ã‚µã‚¤ãƒ³ã®å€¤ã‚’è¨ˆç®—ã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "\n",
    "RoPEã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã«å¿œã˜ã¦ã‚¯ã‚¨ãƒªã¨ã‚­ãƒ¼ã‚’å¤‰èª¿ã•ã›ã‚‹ã“ã¨ã§ã€ä½ç½®æƒ…å ±ã‚’è¿½åŠ ã™ã‚‹æ‰‹æ³•\n",
    "\n",
    "ã‚¯ã‚¨ãƒªã®å¤‰èª¿ã®æµã‚Œ:\n",
    "\n",
    "1.  128å€‹ã®è¦ç´ ã‚’æŒã¤ã‚¯ã‚¨ãƒª$Q$ã‚’ã€å‰åŠ64å€‹$Q_1$ã¨å¾ŒåŠ64å€‹ã®$Q_2$ã«åˆ†ã‘ã‚‹\n",
    "2. $Q_1$ã¨$Q_2$ã‹ã‚‰ä¸€ã¤ãšã¤è¦ç´ ã‚’å–ã‚Šã€$(q1, q2)$ã®ãƒšã‚¢ã‚’ä½œã‚‹ï¼ˆ=2æ¬¡å…ƒå¹³é¢ã‚’æ§‹æˆï¼‰\n",
    "3. ãƒšã‚¢ã«å¯¾ã—è§’åº¦$\\theta$ã§å›è»¢ã‚’é©ç”¨ã™ã‚‹\n",
    "\n",
    "2æ¬¡å…ƒå›è»¢ã®å…¬å¼:\n",
    "\n",
    "$$\n",
    "(q_1 \\cos{\\theta} - q_2 \\sin{\\theta}, q_2 \\cos{\\theta} + q_1 \\sin{\\theta})\n",
    "$$\n",
    "\n",
    "å›è»¢ã®è§’åº¦:\n",
    "\n",
    "$$\n",
    "\\theta_{m, i} = m\\cdot b^{-\\frac{2i}{d}}\n",
    "$$\n",
    "\n",
    "- $m$: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ï¼ˆ$0, 1, 2,...$ï¼‰\n",
    "- $b^{-\\frac{2i}{d}}$: å‘¨æ³¢æ•°ã®é€†æ•°\n",
    "    - $b$: åŸºæ•°ï¼ˆ$10000$ï¼‰\n",
    "    - $i$: æ¬¡å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ$0, 1, 2, ..., d/2-1$ï¼‰\n",
    "    - $d$: ãƒ™ã‚¯ãƒˆãƒ«ã®ç·æ¬¡å…ƒæ•°\n",
    "\n",
    "ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ãŒé ã„ï¼ˆå›è»¢è§’åº¦ã®å·®ãŒå¤§ãã„ï¼‰ã»ã©å‘ããŒæƒã‚ãªããªã‚Šã€ã‚¯ã‚¨ãƒªã¨ã‚­ãƒ¼ã®è¡Œåˆ—ç©ã¯å°ã•ããªã‚‹\n",
    "\n",
    "2æ¬¡å…ƒå›è»¢ã®å®Ÿè£…ã¯ã€å…¬å¼ã‚’å±•é–‹ã—ã¦åŠ¹ç‡çš„ã«è¨ˆç®—ã‚’è¡Œã†ï¼ˆapply_rotate_pos_embé–¢æ•°ï¼‰:\n",
    "\n",
    "$$\n",
    "(q_1 \\cos{\\theta} - q_2 \\sin{\\theta}, q_2 \\cos{\\theta} + q_1 \\sin{\\theta})\n",
    "= [q_1, q_2] \\cdot \\cos{\\theta} + [-q_2, q_1] \\cdot \\sin{\\theta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(k_1 \\cos{\\theta} - k_2 \\sin{\\theta}, k_2 \\cos{\\theta} + k_1 \\sin{\\theta})\n",
    "= [k_1, k_2] \\cdot \\cos{\\theta} + [-k_2, k_1] \\cdot \\sin{\\theta}\n",
    "$$\n",
    "\n",
    "$x$ã‚’$[-x_2, x_1]$ã«å¤‰æ›ã™ã‚‹rotate_halé–¢æ•°ã‚’ä½¿ç”¨ã—ã€æ›´ã«å®Ÿè£…ã‚’ç°¡å˜ã«ã™ã‚‹:\n",
    "\n",
    "\n",
    "$$\n",
    "[q_1, q_2] \\cdot \\cos{\\theta} + [-q_2, q_1] \\cdot \\sin{\\theta}\n",
    "= Q \\cdot \\cos{\\theta} + \\text{rotate\\_half}(Q) \\cdot \\sin{\\theta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "[k_1, k_2] \\cdot \\cos{\\theta} + [-k_2, k_1] \\cdot \\sin{\\theta}\n",
    "= K \\cdot \\cos{\\theta} + \\text{rotate\\_half}(K) \\cdot \\sin{\\theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    logger.info(f\"rotate_halfã®é–‹å§‹ {x.shape=}\")\n",
    "\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    logger.debug(f\"x1ã‚’å–å¾— {x1.shape=}\")\n",
    "\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    logger.debug(f\"x2ã‚’å–å¾— {x2.shape=}\")\n",
    "\n",
    "    res = torch.cat((-x2, x1), dim=-1)\n",
    "    logger.info(f\"rotate_halfã®å®Œäº† {res.shape=}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralRotaryEmbedding(nn.Module):\n",
    "    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n",
    "\n",
    "    def __init__(self, config: MixtralConfig, device=None):\n",
    "        logger.info(f\"MixtralRotaryEmbeddingã®åˆæœŸåŒ–é–‹å§‹ {config.max_position_embeddings=}\")\n",
    "        super().__init__()\n",
    "\n",
    "        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n",
    "            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n",
    "        else:\n",
    "            self.rope_type = \"default\"\n",
    "\n",
    "        # 32768\n",
    "        self.max_seq_len_cached = config.max_position_embeddings\n",
    "\n",
    "        # 32768\n",
    "        self.original_max_seq_len = config.max_position_embeddings\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # default\n",
    "        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n",
    "        logger.debug(f\"{self.rope_type=}\")\n",
    "\n",
    "        # (64,), 1.0\n",
    "        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n",
    "        logger.debug(f\"{inv_freq.shape=}, {self.attention_scaling=}\")\n",
    "\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "        logger.info(\"MixtralRotaryEmbeddingã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n",
    "    def forward(self, x, position_ids):\n",
    "        logger.info(f\"MixtralRotaryEmbeddingã®é †ä¼æ’­é–‹å§‹ {x.shape=} {position_ids.shape=}\")\n",
    "\n",
    "        # (1, 64, 1)\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
    "        logger.debug(f\"{inv_freq_expanded.shape=}\")\n",
    "\n",
    "        # (1, 1, 2)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        logger.debug(f\"{position_ids_expanded.shape=}\")\n",
    "\n",
    "        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n",
    "\n",
    "        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n",
    "            # (1, 64, 1) @ (1, 1, 2) -> (1, 64, 2) -> (1, 2, 64)\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            logger.debug(f\"{freqs.shape=}\")\n",
    "\n",
    "            # (1, 2, 128)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            logger.debug(f\"{emb.shape=}\")\n",
    "\n",
    "            # (1, 2, 128)\n",
    "            cos = emb.cos() * self.attention_scaling\n",
    "\n",
    "            # (1, 2, 128)\n",
    "            sin = emb.sin() * self.attention_scaling\n",
    "\n",
    "        logger.info(f\"MixtralRotaryEmbeddingã®é †ä¼æ’­å®Œäº† {cos.shape=} {sin.shape=}\")\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7729e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    logger.info(f\"apply_rotary_pos_embã®é–‹å§‹ {q.shape=} {k.shape=} {cos.shape=} {sin.shape=} {unsqueeze_dim=}\")\n",
    "\n",
    "    # (1, 2, 128) -> (1, 1, 2, 128)\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "\n",
    "    # (1, 2, 128) -> (1, 1, 2, 128)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "\n",
    "    # (1, 32, 2, 128) * (1, 1, 2, 128) + (1, 32, 2, 128) * (1, 1, 2, 128) -> (1, 32, 2, 128)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "\n",
    "    # (1, 8, 2, 128) * (1, 1, 2, 128) + (1, 8, 2, 128) * (1, 1, 2, 128) -> (1, 8, 2, 128)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "    logger.info(f\"apply_rotary_pos_embã®å®Œäº† {q_embed.shape=} {k_embed.shape=}\")\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c06577",
   "metadata": {},
   "source": [
    "### MixtralAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95727363",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: MixtralConfig, layer_idx: int):\n",
    "        logger.info(f\"MixtralAttentionã®åˆæœŸåŒ–é–‹å§‹ {config.hidden_size=}, {config.num_attention_heads=}, {config.num_key_value_heads=}, {getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=}, {config.attention_dropout=}, {layer_idx=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # 0, 1, 2, ..., 31\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        # 4096 // 32 = 128\n",
    "        self.head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n",
    "        logger.debug(f\"{self.head_dim=}\")\n",
    "\n",
    "        # 32 // 8 = 4\n",
    "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
    "        logger.debug(f\"{self.num_key_value_groups=}\")\n",
    "\n",
    "        # 0.088388347\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        logger.debug(f\"{self.scaling=}\")\n",
    "\n",
    "        # 0.0\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        logger.debug(f\"{self.attention_dropout=}\")\n",
    "\n",
    "        self.is_causal = True\n",
    "\n",
    "        # 4096 -> 32 * 128 = 4096\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n",
    "\n",
    "        # 4096 -> 8 * 128 = 1024\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n",
    "\n",
    "        # 4096 -> 8 * 128 = 1024\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n",
    "\n",
    "        # 32 * 128 = 4096 -> 4096\n",
    "        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n",
    "        logger.info(\"MixtralAttentionã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[FlashAttentionKwargs],\n",
    "    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        logger.info(f\"MixtralAttentionã®é †ä¼æ’­é–‹å§‹ {hidden_states.shape=} {attention_mask.shape if attention_mask is not None else None} {past_key_values is not None=} {cache_position.shape if cache_position is not None else None}\")\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2)\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, -1, 128)\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "        logger.debug(f\"{hidden_shape=}\")\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 8192) -> (1, 2, 32, 128) -> (1, 32, 2, 128)\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        logger.debug(f\"{query_states.shape=}\")\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 1024) -> (1, 2, 8, 128) -> (1, 8, 2, 128)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        logger.debug(f\"{key_states.shape=}\")\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 1024) -> (1, 2, 8, 128) -> (1, 8, 2, 128)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        logger.debug(f\"{value_states.shape=}\")\n",
    "\n",
    "        # (1, 2, 128), (1, 2, 128)\n",
    "        cos, sin = position_embeddings\n",
    "\n",
    "        # (1, 32, 2, 128), (1, 8, 2, 128)\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if past_key_values is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            # (1, 8, 2, 128), (1, 8, 2, 128)\n",
    "            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "            logger.debug(f\"éå»ã®ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã‚’æ›´æ–° {key_states.shape=} {value_states.shape=}\")\n",
    "\n",
    "        # SDPA\n",
    "        attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
    "        logger.debug(f\"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠ {attention_interface=}\")\n",
    "\n",
    "        logger.debug(f\"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—é–‹å§‹ {query_states.shape=} {key_states.shape=} {value_states.shape=} {attention_mask.shape if attention_mask is not None else None} {self.scaling=} sliding_window={getattr(self.config, 'sliding_window', None)} dropout={0.0 if not self.training else self.attention_dropout}\")\n",
    "\n",
    "        # (1, 32, 2, 128) -> (1, 32, 2, 128)\n",
    "        attn_output, attn_weights = attention_interface(\n",
    "            self,\n",
    "            query_states, # (1, 32, 2, 128)\n",
    "            key_states, # (1, 8, 2, 128)\n",
    "            value_states, # (1, 8, 2, 128)\n",
    "            attention_mask, # None\n",
    "            dropout=0.0 if not self.training else self.attention_dropout, # 0.0\n",
    "            scaling=self.scaling, # 0.088388347\n",
    "            sliding_window=getattr(self.config, \"sliding_window\", None), # None\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        logger.debug(f\"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—å®Œäº† {attn_output.shape=} {attn_weights.shape if attn_weights is not None else None}\")\n",
    "\n",
    "        # (1, 32, 2, 128) -> (1, 2, 4096)\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        logger.debug(f\"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å‡ºåŠ›ã‚’æ•´å½¢ {attn_output.shape=}\")\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 4096)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        logger.debug(f\"å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨ {attn_output.shape=}\")\n",
    "\n",
    "        logger.info(f\"MixtralAttentionã®é †ä¼æ’­å®Œäº† {attn_output.shape=}\")\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f94fb11",
   "metadata": {},
   "source": [
    "### MixtralDecoderLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892814ff",
   "metadata": {},
   "source": [
    "MixtralDecoderLayerã¯ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ã¨SMoEãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰ãªã‚‹Transformerã®ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã‚¯ãƒ©ã‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7edf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralDecoderLayer(GradientCheckpointingLayer):\n",
    "    def __init__(self, config: MixtralConfig, layer_idx: int):\n",
    "        logger.info(f\"MixtralDecoderLayerã®åˆæœŸåŒ–é–‹å§‹ {config.hidden_size=}, {config.num_attention_heads=}, {config.intermediate_size=}, {layer_idx=}\")\n",
    "        super().__init__()\n",
    "\n",
    "        # 4096\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        # ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’åˆæœŸåŒ–\n",
    "        self.self_attn = MixtralAttention(config, layer_idx)\n",
    "\n",
    "        # MoEãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆæœŸåŒ–\n",
    "        self.block_sparse_moe = MixtralSparseMoeBlock(config)\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å‰ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–\n",
    "        self.input_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å¾Œã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–\n",
    "        self.post_attention_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        logger.info(\"MixtralDecoderLayerã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> torch.FloatTensor:\n",
    "        logger.info(f\"MixtralDecoderLayerã®é †ä¼æ’­é–‹å§‹ {hidden_states.shape=} {attention_mask.shape if attention_mask is not None else None} {position_ids.shape if position_ids is not None else None} {past_key_values is not None=} {cache_position.shape if cache_position is not None else None}\")\n",
    "\n",
    "        ###################\n",
    "        # ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ #\n",
    "        ###################\n",
    "\n",
    "        # æ®‹å·®æ¥ç¶šç”¨ã«ã‚³ãƒ”ãƒ¼\n",
    "        # (1, 2, 4096)\n",
    "        residual = hidden_states\n",
    "\n",
    "        # ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã‚’é©ç”¨\n",
    "        # (1, 2, 4096) -> (1, 2, 4096)\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’é©ç”¨\n",
    "        hidden_states, _ = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            position_embeddings=position_embeddings,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            cache_position=cache_position,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # æ®‹å·®æ¥ç¶šã‚’é©ç”¨\n",
    "        # (1, 2, 4096) + (1, 2, 4096) -> (1, 2, 4096)\n",
    "        hidden_states = residual + hidden_states\n",
    "        logger.debug(f\"æ®‹å·®æ¥ç¶šã‚’é©ç”¨ {hidden_states.shape=}\")\n",
    "\n",
    "        ####################\n",
    "        # SMoEãƒ–ãƒ­ãƒƒã‚¯ã‚’é©ç”¨ #\n",
    "        ####################\n",
    "\n",
    "        # æ®‹å·®æ¥ç¶šç”¨ã«ã‚³ãƒ”ãƒ¼\n",
    "        # (1, 2, 4096)\n",
    "        residual = hidden_states\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 4096)\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 4096)\n",
    "        hidden_states, _ = self.block_sparse_moe(hidden_states)\n",
    "\n",
    "        # (1, 2, 4096) + (1, 2, 4096) -> (1, 2, 4096)\n",
    "        hidden_states = residual + hidden_states\n",
    "        logger.debug(f\"æ®‹å·®æ¥ç¶šã‚’é©ç”¨ {hidden_states.shape=}\")\n",
    "\n",
    "        logger.info(f\"MixtralDecoderLayerã®é †ä¼æ’­å®Œäº† {hidden_states.shape=}\")\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947bfdec",
   "metadata": {},
   "source": [
    "### MixtralPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c321bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralPreTrainedModel(PreTrainedModel):\n",
    "    config: MixtralConfig\n",
    "    base_model_prefix = \"model\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _no_split_modules = [\"MixtralDecoderLayer\"]\n",
    "    _skip_keys_device_placement = [\"past_key_values\"]\n",
    "    _supports_flash_attn = True\n",
    "    _supports_sdpa = True\n",
    "    _supports_flex_attn = True\n",
    "    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n",
    "    _supports_attention_backend = True\n",
    "    _can_record_outputs = {\n",
    "        \"router_logits\": OutputRecorder(MixtralSparseMoeBlock, index=1),\n",
    "        \"hidden_states\": MixtralDecoderLayer,\n",
    "        \"attentions\": MixtralAttention,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3509154b",
   "metadata": {},
   "source": [
    "### MixtralModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31654fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralModel(MixtralPreTrainedModel):\n",
    "    def __init__(self, config: MixtralConfig):\n",
    "        logger.info(f\"MixtralModelã®åˆæœŸåŒ–é–‹å§‹ {config.vocab_size=}, {config.hidden_size=}, {config.num_hidden_layers=}, {config.pad_token_id=}\")\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.padding_idx = config.pad_token_id\n",
    "\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "\n",
    "        # 32å±¤ã®ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’åˆæœŸåŒ–\n",
    "        self.layers = nn.ModuleList(\n",
    "            [MixtralDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "\n",
    "        self.norm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        self.rotary_emb = MixtralRotaryEmbedding(config=config)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "        logger.info(\"MixtralModelã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    @check_model_inputs\n",
    "    @auto_docstring\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> MoeModelOutputWithPast:\n",
    "        logger.info(f\"MixtralModelã®é †ä¼æ’­é–‹å§‹ {input_ids.shape=} {attention_mask.shape=} {position_ids.shape=} {past_key_values=} {inputs_embeds.shape if inputs_embeds is not None else None} {use_cache=} {cache_position=}\")\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "\n",
    "        if use_cache and past_key_values is None:\n",
    "            past_key_values = DynamicCache(config=self.config)\n",
    "            logger.debug(\"DynamicCacheã‚’åˆæœŸåŒ–\")\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            # (1, 2, 4096)\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "            logger.debug(f\"å…¥åŠ›ã®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾— {inputs_embeds.shape=}\")\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        # create_causal_mask\n",
    "        mask_function = create_causal_mask if self.config.sliding_window is None else create_sliding_window_causal_mask\n",
    "        logger.debug(f\"ãƒã‚¹ã‚¯é–¢æ•°ã‚’é¸æŠ {mask_function=}\")\n",
    "\n",
    "        # None\n",
    "        causal_mask = mask_function(\n",
    "            config=self.config,\n",
    "            input_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            cache_position=cache_position,\n",
    "            past_key_values=past_key_values,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "        logger.debug(f\"å› æœãƒã‚¹ã‚¯ã‚’ä½œæˆ {causal_mask=}\")\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        # (1, 2, 128), (1, 2, 128)\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "        logger.debug(f\"ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’ä½œæˆ {position_embeddings[0].shape=} {position_embeddings[1].shape=}\")\n",
    "\n",
    "        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n",
    "            hidden_states = decoder_layer(\n",
    "                hidden_states,\n",
    "                position_embeddings=position_embeddings,\n",
    "                attention_mask=causal_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 4096)\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        res = MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=past_key_values,\n",
    "        )\n",
    "        logger.info(f\"MixtralModelã®é †ä¼æ’­å®Œäº† {hidden_states.shape=}\")\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb9c6c",
   "metadata": {},
   "source": [
    "### MixtralForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralForCausalLM(MixtralPreTrainedModel, GenerationMixin):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n",
    "    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n",
    "\n",
    "    def __init__(self, config):\n",
    "        logger.info(\"MixtralForCausalLMã®åˆæœŸåŒ–é–‹å§‹\")\n",
    "        super().__init__(config)\n",
    "        self.model = MixtralModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.router_aux_loss_coef = config.router_aux_loss_coef\n",
    "        self.num_experts = config.num_local_experts\n",
    "        self.num_experts_per_tok = config.num_experts_per_tok\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "        logger.info(\"MixtralForCausalLMã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    @can_return_tuple\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_router_logits: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> MoeCausalLMOutputWithPast:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, MixtralForCausalLM\n",
    "\n",
    "        >>> model = MixtralForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "\n",
    "        logger.info(f\"MixtralForCausalLMã®é †ä¼æ’­é–‹å§‹ {input_ids.shape if input_ids is not None else None} {attention_mask.shape if attention_mask is not None else None} {position_ids.shape if position_ids is not None else None} {past_key_values is not None=} {inputs_embeds.shape if inputs_embeds is not None else None} {labels.shape if labels is not None else None} {use_cache=} {output_router_logits=} {cache_position.shape if cache_position is not None else None} {logits_to_keep=}\")\n",
    "\n",
    "        output_router_logits = (\n",
    "            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n",
    "        )\n",
    "        logger.debug(f\"{output_router_logits=}\")\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs: MoeModelOutputWithPast = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_router_logits=output_router_logits,\n",
    "            cache_position=cache_position,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
    "        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 32000)\n",
    "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n",
    "\n",
    "        aux_loss = None\n",
    "        if output_router_logits:\n",
    "            aux_loss = load_balancing_loss_func(\n",
    "                outputs.router_logits,\n",
    "                self.num_experts,\n",
    "                self.num_experts_per_tok,\n",
    "                attention_mask,\n",
    "            )\n",
    "            if labels is not None:\n",
    "                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n",
    "\n",
    "        res = MoeCausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            aux_loss=aux_loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            router_logits=outputs.router_logits,\n",
    "        )\n",
    "        logger.info(f\"MixtralForCausalLMã®é †ä¼æ’­å®Œäº† {logits.shape=}\")\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b9bd30",
   "metadata": {},
   "source": [
    "### æ¨è«–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67238978",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fffc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MixtralForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\", dtype=torch.bfloat16, load_in_4bit=True, device_map=\"auto\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaef015",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"Hello\"\n",
    "logger.info(f\"å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ {text=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df13cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**model_inputs, max_new_tokens=1)\n",
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c147da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "logger.info(f\"ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cf29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.WARN)\n",
    "text= \"Fukuoka is\"\n",
    "model_inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
