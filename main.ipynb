{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c211c7ff",
   "metadata": {},
   "source": [
    "# Mixtral-8x7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3094ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- [論文](https://arxiv.org/abs/2401.04088)\n",
    "- [実装](https://github.com/mistralai/mistral-inference)\n",
    "- [ウェブサイト](https://mistral.ai/news/mixtral-of-experts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d52dffc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 概要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9a70e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Mixtral 8x7B** は、スパース混合エキスパート（Sparse Mixture of Experts: **SMoE**）言語モデル\n",
    "\n",
    "Mistral 7Bと異なり、各層が8つのフィードフォワードネットワーク（**エキスパート**）で構成される\n",
    "\n",
    "トークン毎に **ルーターネットワーク** が2つのエキスパートを選択し、その出力を組み合わせて処理する\n",
    "\n",
    "エキスパートを動的に選択する仕組みにより、47B（470億）のうち13B（130億）パラメータしか使用しない\n",
    "\n",
    "計算コストとレイテンシを抑えて、Llama 2 70BやGPT-3.5に匹敵する性能を実現\n",
    "\n",
    "SFT（Supervised Fine-Tuning）とDPO（Direct Preference Optimization）で訓練した **Mixtral 8x7B Instruct** も公開"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8243ed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## アーキテクチャ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7e1b24",
   "metadata": {},
   "source": [
    "Mixtralは、Mistral 7Bから以下の変更を加えている:\n",
    "\n",
    "- Sliding Window Attention（SWA）からFully Dense Attentionに変更\n",
    "- フィードフォワードブロックを混合エキスパート層（MoE層）に変更 \n",
    "\n",
    "![](image/table_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c8739",
   "metadata": {},
   "source": [
    "### スパース混合エキスパート（Sparse Mixture of Experts）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56350ed",
   "metadata": {},
   "source": [
    "混合エキスパートの詳細は別の[論文](https://arxiv.org/pdf/2401.04088)を参照\n",
    "\n",
    "入力 $x$ に対するMoEの出力は、 **エキスパートネットワークの出力の重み付き和** で決まる\n",
    "\n",
    "エキスパートネットワークの重みは、 **ゲーティングネットワークの出力** で決まる\n",
    "\n",
    "スパース混合エキスパートの概要図:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3996b6a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![](image/figure_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bfd050",
   "metadata": {},
   "source": [
    "$n$ 個のエキスパートネットワーク $\\{E_0, E_1, transformers., E_{n-1}\\}$ が与えられたときの出力の重み付き和:\n",
    "\n",
    "$$\n",
    "\\sum_{i=0}^{n-1}G(x)_{i}\\cdot E_{i}(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667dfb07",
   "metadata": {},
   "source": [
    "- $G(x)_i$: 入力 $x$ に対する $i$ 番目のエキスパートの重み（ゲーティングネットワーク）\n",
    "- $E_i(x)$: 入力 $x$ を $i$ 番目のエキスパートが処理した出力（エキスパートネットワーク）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951187ee",
   "metadata": {},
   "source": [
    "Mixtralのゲーティングネットワークは、上位 $K$ 個（Top-K）のロジットに対してソフトマックスを適用した関数:\n",
    "\n",
    "$$\n",
    "G(x) := \\text{Softmax}(\\text{TopK}(x \\cdot W_g))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d03c1",
   "metadata": {},
   "source": [
    "- $W_g$: ゲーティングネットワークの重み行列\n",
    "- $x\\cdot W_g$: 各エキスパートのスコア（ロジット）\n",
    "- $\\text{TopK}$: 上位$K$個に含まれないロジットをマイナス無限大にする関数\n",
    "- $\\text{Softmax}(\\cdot)$: マイナス無限大になったロジットを除いて合計 $1.0$ の確率分布に変換する関数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104779c1",
   "metadata": {},
   "source": [
    "使用するエキスパート数 $K$ を固定し、エキスパートの総数 $n$ を増やすことで、効率的にパラメータ総数を増加できる:\n",
    "\n",
    "- トークン毎に使用するパラメータ数を **アクティブパラメータ数** （active parameter count）と呼ぶ\n",
    "- モデルのパラメータ総数を **スパースパラメータ数** （sparse parameter count）と呼ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630079c7",
   "metadata": {},
   "source": [
    "MoE層は、単一もしくは複数のGPUで効率的に実行できる:\n",
    "\n",
    "- 単一GPUでの効率化手法\n",
    "    - [Megablocks][1]: MoEのFFNの操作を大きなスパース行列乗算として扱い、実行速度を向上させる\n",
    "- 複数GPUでの効率化手法\n",
    "    - モデル並列化（Model Parallelism techniques）: モデルを層ごとに分けて複数のGPUに展開する\n",
    "    - [エキスパート並列化][2]（Expert Parallelism: EP）: エキスパートをグループに分けて複数のGPUに展開する\n",
    "\n",
    "[1]: https://proceedings.mlsys.org/paper_files/paper/2023/hash/5a54f79333768effe7e8927bcccffe40-Abstract-mlsys2023.html\n",
    "[2]: https://arxiv.org/abs/1701.06538\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d208d",
   "metadata": {},
   "source": [
    "Mixtralでは、エキスパートをSwiGLUアーキテクチャで実装し、使用するエキスパート数を $K=2$ とする:\n",
    "\n",
    "$$\n",
    "y = \\sum_{i=0}^{n-1} \\text{Softmax}(\\text{Top2}(x\\cdot W_g))_i \\cdot \\text{SwiGLU}_i(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc29f0bc",
   "metadata": {},
   "source": [
    "- $\\text{Softmax}(\\text{Top2}(x\\cdot W_g))_i$: $i$ 番目のエキスパートに対する重み\n",
    "- $\\text{SwiGLU}_i(x)$: $i$ 番目のエキスパートの出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4e45c",
   "metadata": {},
   "source": [
    "## ベンチマーク結果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946da742",
   "metadata": {},
   "source": [
    "MixtralとLlamaをベンチマークで評価し比較:\n",
    "\n",
    "- 常識推論（0-shot）\n",
    "    - Hellaswag: 文脈から自然に続く結末を選ぶ\n",
    "    - Winogrande: 代名詞が指している単語を選ぶ\n",
    "    - PIQA（Physical Interaction Question Answering）: 物理的な理解が必要な選択肢を選ぶ\n",
    "    - SIQA（Social Interaction QA）:人の感情の理解が必要な選択肢を選ぶ\n",
    "    - OpenbookQA: 一般的な科学的事実（Open Book）の理解が必要な選択肢を選ぶ\n",
    "    - ARC-Easy（AI2 Reasoning Challenge）: 小学生レベルの科学の理解が必要な選択肢を選ぶ\n",
    "    - ARC-Challenge: 中学生レベルの科学の理解が必要な選択肢を選ぶ\n",
    "    - CommonsenseQA: 社会常識の理解が必要な選択肢を選ぶ\n",
    "- 世界知識（5-shot）\n",
    "    - NaturalQuestions: 与えられたWikipediaのページから長い回答と短い回答を抽出する\n",
    "    - TriviaQA: ウェブページやWikipediaのページが与えられ、それらを統合する必要のある回答を抽出する\n",
    "- 読解（0-shot）\n",
    "    - BoolQ: 与えられた文章に対して、はい/いいえで回答する\n",
    "    - QuAC（Question Answering in Context）: 一人のユーザーが連続して質問しそれに対して回答し続ける\n",
    "- 数学\n",
    "    - GSM8K（8-shot）: 小学生レベルの算数問題\n",
    "    - MATH（4-shot）: 競技数学レベルの難しい数学問題\n",
    "- コード\n",
    "    - Humaneval（0-shot）: 人が作成したPython関数をヒントから完成させる\n",
    "    - MBPP（Mostly Basic Python Programming）（3-shot）: 初心者向けの基本的なPythonプログラミング問題を解く\n",
    "- 総合\n",
    "    - MMLU（Massive Multitask Language Understanding）（5-shot）: 57の異なる分野の選択問題\n",
    "    - BBH（Big-Bench Hard）（3-shot）: 現在のモデルが苦手とする23の挑戦的なタスク\n",
    "    - AGI Eval（3-5-shot）: 米国の大学入学試験、法科大学院試験、医師国家試験などの選択問題"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b071e43",
   "metadata": {},
   "source": [
    "Mixtralは、アクティブパラメータ数が5倍多いLlama2 70Bを多くのベンチマークで上回った（コードと数学が強い）:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ec94f",
   "metadata": {},
   "source": [
    "![](image/figure_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc4ba4",
   "metadata": {},
   "source": [
    "![](image/table_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54bdb7d",
   "metadata": {},
   "source": [
    "Mixtralは、アクティブパラメータ数が少なく性能が高い:\n",
    "\n",
    "![](image/figure_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd406ac",
   "metadata": {},
   "source": [
    "Mixtralは、LlaMA 2 70Bより優れ、GPT-3.5（GPT-3.5-Turbo）に匹敵する性能を示した:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe974526",
   "metadata": {},
   "source": [
    "![](image/table_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b6938",
   "metadata": {},
   "source": [
    "Mixtralは、英語の他にフランス語・ドイツ語・スペイン語・イタリア語でLlama2 70 Bを大幅に上回る性能:\n",
    "\n",
    "![](image/table_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00793e7",
   "metadata": {},
   "source": [
    "長い文章から探し出すタスク（[passkey retrieval][1]）では、100%の検索性能を示し、困惑度（perplexity）も長さに応じて減少:\n",
    "\n",
    "![](image/figure_4.png)\n",
    "\n",
    "[1]: https://arxiv.org/abs/2305.16300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e849f",
   "metadata": {},
   "source": [
    "Llama 2と比較して、BBQ（Bias Benchmark for QA）で低いバイアスを示した:\n",
    "\n",
    "![](image/figure_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11470c2d",
   "metadata": {},
   "source": [
    "指示チューニング済みモデルは、MT-Benchではオープンウェイトモデルの中で最も高い:\n",
    "\n",
    "![](image/figure_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbd7cd",
   "metadata": {},
   "source": [
    "## ルーティング分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af09458c",
   "metadata": {},
   "source": [
    "The Pileの検証データセットを使い、トピック毎に0層目・15層目・31層目のエキスパート選択状態を測定\n",
    "\n",
    "トピックに基づいたエキスパートの選択に明確なパターンは見られなかった（数学のみわずかに反応）:\n",
    "\n",
    "![](image/figure_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8077637",
   "metadata": {},
   "source": [
    "![](image/table_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d1b54",
   "metadata": {},
   "source": [
    "トークンごとのエキスパートの割当では、`self`・`Question`・インデント・連続したトークンが同じルーティング:\n",
    "\n",
    "![](image/figure_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8549895d",
   "metadata": {},
   "source": [
    "## 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d377de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU transformers==4.57.1\n",
    "%pip install -qU sentencepiece protobuf bitsandbytes accelerate\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "except ImportError:\n",
    "    from dotenv import load_dotenv\n",
    "    import os\n",
    "    load_dotenv()\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "assert HF_TOKEN\n",
    "\n",
    "import os\n",
    "import logging as logging_\n",
    "import transformers\n",
    "import bitsandbytes\n",
    "from transformers import PretrainedConfig\n",
    "from transformers.utils import logging\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# トークナイザー\n",
    "\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from typing import TYPE_CHECKING, Any, Optional\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from transformers.convert_slow_tokenizer import import_protobuf\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "from transformers.utils import logging\n",
    "from transformers.utils.import_utils import requires\n",
    "\n",
    "# モデル\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDAを使用できません\"\n",
    "\n",
    "from transformers.utils.generic import check_model_inputs\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.cache_utils import Cache, DynamicCache\n",
    "from transformers.generation import GenerationMixin\n",
    "from transformers.integrations import use_kernel_forward_from_hub\n",
    "from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask\n",
    "from transformers.modeling_flash_attention_utils import FlashAttentionKwargs\n",
    "from transformers.modeling_layers import (\n",
    "    GenericForQuestionAnswering,\n",
    "    GenericForSequenceClassification,\n",
    "    GenericForTokenClassification,\n",
    "    GradientCheckpointingLayer,\n",
    ")\n",
    "from transformers.modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n",
    "from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n",
    "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n",
    "from transformers.processing_utils import Unpack\n",
    "from transformers.utils import TransformersKwargs, auto_docstring, can_return_tuple\n",
    "from transformers.utils.deprecation import deprecate_kwarg\n",
    "from transformers.utils.generic import OutputRecorder\n",
    "from transformers.models.mixtral.configuration_mixtral import MixtralConfig\n",
    "\n",
    "# デバイス設定\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "# ログ設定\n",
    "\n",
    "if os.path.exists('debug.log'):\n",
    "    os.remove('debug.log')\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging_.DEBUG:\n",
    "            level = '🟦'\n",
    "        case logging_.INFO:\n",
    "            level = '🟩'\n",
    "        case logging_.WARNING:\n",
    "            level = '🟨'\n",
    "        case logging_.ERROR:\n",
    "            level = '🟥'\n",
    "        case logging_.CRITICAL:\n",
    "            level = '🛑'\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logging.set_verbosity_debug()\n",
    "logger = logging.get_logger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging_.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging_.FileHandler('debug.log')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging_.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.info(f\"Transformers version: {transformers.__version__}\")\n",
    "logger.info(f\"Numpy version: {np.__version__}\")\n",
    "logger.info(f\"BitsAndBytes version: {bitsandbytes.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aba00b",
   "metadata": {},
   "source": [
    "### LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eaeb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済み語彙ファイルのパス\n",
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer.model\"}\n",
    "\n",
    "# 単語の先頭を示す特殊文字\n",
    "SPIECE_UNDERLINE = \"▁\"\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your \\\n",
    "answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure\\\n",
    " that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not \\\n",
    "correct. If you don't know the answer to a question, please don't share false information.\"\"\"  # fmt: skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ca666",
   "metadata": {},
   "outputs": [],
   "source": [
    "@requires(backends=(\"sentencepiece\",))\n",
    "class LlamaTokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"\n",
    "    Construct a Llama tokenizer. Based on byte-level Byte-Pair-Encoding. The default padding token is unset as there is\n",
    "    no padding token in the original model.\n",
    "\n",
    "    Args:\n",
    "        vocab_file (`str`):\n",
    "            Path to the vocabulary file.\n",
    "        unk_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<unk>\"`):\n",
    "            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
    "            token instead.\n",
    "        bos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<s>\"`):\n",
    "            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n",
    "        eos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"</s>\"`):\n",
    "            The end of sequence token.\n",
    "        pad_token (`str` or `tokenizers.AddedToken`, *optional*):\n",
    "            A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by\n",
    "            attention mechanisms or loss computation.\n",
    "        sp_model_kwargs (`dict[str, Any]`, `Optional`, *optional*):\n",
    "            Will be passed to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for\n",
    "            SentencePiece](https://github.com/google/sentencepiece/tree/master/python) can be used, among other things,\n",
    "            to set:\n",
    "\n",
    "            - `enable_sampling`: Enable subword regularization.\n",
    "            - `nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.\n",
    "\n",
    "              - `nbest_size = {0,1}`: No sampling is performed.\n",
    "              - `nbest_size > 1`: samples from the nbest_size results.\n",
    "              - `nbest_size < 0`: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)\n",
    "                using forward-filtering-and-backward-sampling algorithm.\n",
    "\n",
    "            - `alpha`: Smoothing parameter for unigram sampling, and dropout probability of merge operations for\n",
    "              BPE-dropout.\n",
    "\n",
    "        add_bos_token (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to add an `bos_token` at the start of sequences.\n",
    "        add_eos_token (`bool`, *optional*, defaults to `False`):\n",
    "            Whether or not to add an `eos_token` at the end of sequences.\n",
    "        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n",
    "            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n",
    "            extra spaces.\n",
    "        use_default_system_prompt (`bool`, *optional*, defaults to `False`):\n",
    "            Whether or not the default system prompt for Llama should be used.\n",
    "        spaces_between_special_tokens (`bool`, *optional*, defaults to `False`):\n",
    "            Whether or not to add spaces between special tokens.\n",
    "        legacy (`bool`, *optional*):\n",
    "            Whether or not the `legacy` behavior of the tokenizer should be used. Legacy is before the merge of #24622\n",
    "            and #25224 which includes fixes to properly handle tokens that appear after special tokens.\n",
    "            Make sure to also set `from_slow` to `True`.\n",
    "            A simple example:\n",
    "\n",
    "            - `legacy=True`:\n",
    "            ```python\n",
    "            >>> from transformers import LlamaTokenizerFast\n",
    "\n",
    "            >>> tokenizer = LlamaTokenizerFast.from_pretrained(\"huggyllama/llama-7b\", legacy=True, from_slow=True)\n",
    "            >>> tokenizer.encode(\"Hello <s>.\") # 869 is '▁.'\n",
    "            [1, 15043, 29871, 1, 869]\n",
    "            ```\n",
    "            - `legacy=False`:\n",
    "            ```python\n",
    "            >>> from transformers import LlamaTokenizerFast\n",
    "\n",
    "            >>> tokenizer = LlamaTokenizerFast.from_pretrained(\"huggyllama/llama-7b\", legacy=False, from_slow=True)\n",
    "            >>> tokenizer.encode(\"Hello <s>.\")  # 29889 is '.'\n",
    "            [1, 15043, 29871, 1, 29889]\n",
    "            ```\n",
    "            Checkout the [pull request](https://github.com/huggingface/transformers/pull/24565) for more details.\n",
    "        add_prefix_space (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n",
    "            other word. Again, this should be set with `from_slow=True` to make sure it's taken into account.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    model_input_names = [\"input_ids\", \"attention_mask\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        unk_token=\"<unk>\",\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        pad_token=None,\n",
    "        sp_model_kwargs: Optional[dict[str, Any]] = None,\n",
    "        add_bos_token=True,\n",
    "        add_eos_token=False,\n",
    "        clean_up_tokenization_spaces=False,\n",
    "        use_default_system_prompt=False,\n",
    "        spaces_between_special_tokens=False,\n",
    "        legacy=None,\n",
    "        add_prefix_space=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        logger.info(f\"LlamaTokenizerの初期化開始 vocab_file={vocab_file} unk_token={unk_token} bos_token={bos_token} eos_token={eos_token} pad_token={pad_token} legacy={legacy} add_prefix_space={add_prefix_space}\")\n",
    "\n",
    "        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n",
    "        bos_token = AddedToken(bos_token, normalized=False, special=True) if isinstance(bos_token, str) else bos_token\n",
    "        eos_token = AddedToken(eos_token, normalized=False, special=True) if isinstance(eos_token, str) else eos_token\n",
    "        unk_token = AddedToken(unk_token, normalized=False, special=True) if isinstance(unk_token, str) else unk_token\n",
    "        pad_token = AddedToken(pad_token, normalized=False, special=True) if isinstance(pad_token, str) else pad_token\n",
    "\n",
    "        if legacy is None:\n",
    "            logger.warning_once(\n",
    "                f\"You are using the default legacy behaviour of the {self.__class__}. This is\"\n",
    "                \" expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you.\"\n",
    "                \" If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it\"\n",
    "                \" means, and thoroughly read the reason why this was added as explained in\"\n",
    "                \" https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file\"\n",
    "                \" you can ignore this message\"\n",
    "            )\n",
    "            legacy = True\n",
    "\n",
    "        self.legacy = legacy\n",
    "        self.vocab_file = vocab_file\n",
    "        self.add_bos_token = add_bos_token\n",
    "        self.add_eos_token = add_eos_token\n",
    "        self.use_default_system_prompt = use_default_system_prompt\n",
    "        self.sp_model = self.get_spm_processor(kwargs.pop(\"from_slow\", False))\n",
    "        self.add_prefix_space = add_prefix_space\n",
    "\n",
    "        super().__init__(\n",
    "            bos_token=bos_token,\n",
    "            eos_token=eos_token,\n",
    "            unk_token=unk_token,\n",
    "            pad_token=pad_token,\n",
    "            add_bos_token=add_bos_token,\n",
    "            add_eos_token=add_eos_token,\n",
    "            sp_model_kwargs=self.sp_model_kwargs,\n",
    "            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
    "            use_default_system_prompt=use_default_system_prompt,\n",
    "            spaces_between_special_tokens=spaces_between_special_tokens,\n",
    "            legacy=legacy,\n",
    "            add_prefix_space=add_prefix_space,\n",
    "            **kwargs,\n",
    "        )\n",
    "        logger.info(\"LlamaTokenizerの初期化完了\")\n",
    "\n",
    "    @property\n",
    "    def unk_token_length(self):\n",
    "        return len(self.sp_model.encode(str(self.unk_token)))\n",
    "\n",
    "    # Copied from transformers.models.t5.tokenization_t5.T5Tokenizer.get_spm_processor\n",
    "    def get_spm_processor(self, from_slow=False):\n",
    "        logger.info(f\"SentencePieceProcessorの取得 from_slow={from_slow} legacy={self.legacy}\")\n",
    "\n",
    "        tokenizer = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n",
    "        if self.legacy or from_slow:  # no dependency on protobuf\n",
    "            tokenizer.Load(self.vocab_file)\n",
    "            return tokenizer\n",
    "\n",
    "        with open(self.vocab_file, \"rb\") as f:\n",
    "            sp_model = f.read()\n",
    "            model_pb2 = import_protobuf(f\"The new behaviour of {self.__class__.__name__} (with `self.legacy = False`)\")\n",
    "            model = model_pb2.ModelProto.FromString(sp_model)\n",
    "            normalizer_spec = model_pb2.NormalizerSpec()\n",
    "            normalizer_spec.add_dummy_prefix = False\n",
    "            model.normalizer_spec.MergeFrom(normalizer_spec)\n",
    "            sp_model = model.SerializeToString()\n",
    "            tokenizer.LoadFromSerializedProto(sp_model)\n",
    "\n",
    "        logger.info(\"SentencePieceProcessorの取得完了\")\n",
    "        return tokenizer\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state[\"sp_model\"] = None\n",
    "        state[\"sp_model_proto\"] = self.sp_model.serialized_model_proto()\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        self.__dict__.update(d)\n",
    "        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n",
    "        self.sp_model.LoadFromSerializedProto(self.sp_model_proto)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"Returns vocab size\"\"\"\n",
    "        return self.sp_model.get_piece_size()\n",
    "\n",
    "    def get_vocab(self):\n",
    "        \"\"\"Returns vocab as a dict\"\"\"\n",
    "        logger.info(\"語彙の取得開始\")\n",
    "\n",
    "        vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)}\n",
    "        vocab.update(self.added_tokens_encoder)\n",
    "\n",
    "        logger.info(f\"語彙の取得完了 vocab_size={len(vocab)}\")\n",
    "        return vocab\n",
    "\n",
    "    # Copied from transformers.models.t5.tokenization_t5.T5Tokenizer.tokenize\n",
    "    def tokenize(self, text: \"TextInput\", **kwargs) -> list[str]:\n",
    "        \"\"\"\n",
    "        Converts a string to a list of tokens. If `self.legacy` is set to `False`, a prefix token is added unless the\n",
    "        first token is special.\n",
    "        \"\"\"\n",
    "        logger.info(f\"トークン化開始 text={text} legacy={self.legacy} add_prefix_space={self.add_prefix_space}\")\n",
    "\n",
    "        # True\n",
    "        if self.legacy or len(text) == 0:\n",
    "\n",
    "            # 'Hello' -> '_Hello'\n",
    "            res = super().tokenize(text, **kwargs)\n",
    "            logger.info(f\"トークン化完了（legacy） tokens={res}\")\n",
    "            return res\n",
    "\n",
    "        text = text.replace(SPIECE_UNDERLINE, \" \")\n",
    "        if self.add_prefix_space:\n",
    "            text = SPIECE_UNDERLINE + text\n",
    "\n",
    "        tokens = super().tokenize(text, **kwargs)\n",
    "\n",
    "        if len(tokens) > 1 and tokens[0] == SPIECE_UNDERLINE and tokens[1] in self.all_special_tokens:\n",
    "            tokens = tokens[1:]\n",
    "        return tokens\n",
    "\n",
    "    # Copied from transformers.models.t5.tokenization_t5.T5Tokenizer._tokenize\n",
    "    def _tokenize(self, text, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns a tokenized string.\n",
    "\n",
    "        We de-activated the `add_dummy_prefix` option, thus the sentencepiece internals will always strip any\n",
    "        SPIECE_UNDERLINE. For example: `self.sp_model.encode(f\"{SPIECE_UNDERLINE}Hey\", out_type = str)` will give\n",
    "        `['H', 'e', 'y']` instead of `['▁He', 'y']`. Thus we always encode `f\"{unk_token}text\"` and strip the\n",
    "        `unk_token`. Here is an example with `unk_token = \"<unk>\"` and `unk_token_length = 4`.\n",
    "        `self.tokenizer.sp_model.encode(\"<unk> Hey\", out_type = str)[4:]`.\n",
    "        \"\"\"\n",
    "        logger.info(f\"_tokenizeの開始 text={text} legacy={self.legacy}\")\n",
    "\n",
    "        # True\n",
    "        if self.legacy or not text.startswith((SPIECE_UNDERLINE, \" \")):\n",
    "            # 'Hello' -> '_Hello'\n",
    "            res = self.sp_model.encode(text, out_type=str)\n",
    "            logger.info(f\"_tokenizeの完了（legacy） tokens={res}\")\n",
    "            return res\n",
    "\n",
    "        # 1. Encode string + prefix ex: \"<unk> Hey\"\n",
    "        tokens = self.sp_model.encode(self.unk_token + text, out_type=str)\n",
    "        # 2. Remove self.unk_token from ['<','unk','>', '▁Hey']\n",
    "        res = tokens[self.unk_token_length :] if len(tokens) >= self.unk_token_length else tokens\n",
    "        return res\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n",
    "        logger.info(f\"_convert_token_to_idの開始 token={token}\")\n",
    "        # '_Hello' -> 22557\n",
    "        res = self.sp_model.piece_to_id(token)\n",
    "        logger.info(f\"_convert_token_to_idの完了 token={token} id={res}\")\n",
    "        return res\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
    "        # logger.info(f\"_convert_id_to_tokenの開始 id={index}\")\n",
    "        token = self.sp_model.IdToPiece(index)\n",
    "        res = token\n",
    "        # logger.info(f\"_convert_id_to_tokenの完了 id={index} token={res}\")\n",
    "        return res\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n",
    "        logger.info(f\"トークン列の文字列変換開始 tokens={tokens} legacy={self.legacy} add_prefix_space={self.add_prefix_space}\")\n",
    "\n",
    "        # since we manually add the prefix space, we have to remove it when decoding\n",
    "        # ['_Hello', ',] -> ['Hello', ',']\n",
    "        if tokens[0].startswith(SPIECE_UNDERLINE) and self.add_prefix_space:\n",
    "            tokens[0] = tokens[0][1:]\n",
    "            logger.debug(f\"最初のトークンから接頭辞スペースを削除 {tokens=}\")\n",
    "\n",
    "        current_sub_tokens = []\n",
    "        out_string = \"\"\n",
    "        prev_is_special = False\n",
    "        for i, token in enumerate(tokens):\n",
    "            # make sure that special tokens are not decoded using sentencepiece model\n",
    "            if token in self.all_special_tokens:\n",
    "                if not prev_is_special and i != 0 and self.legacy:\n",
    "                    out_string += \" \"\n",
    "                out_string += self.sp_model.decode(current_sub_tokens) + token\n",
    "                logger.debug(f\"{current_sub_tokens=} をデコードし集約 {out_string=}\")\n",
    "                prev_is_special = True\n",
    "                current_sub_tokens = []\n",
    "            else:\n",
    "                if prev_is_special and i == 1 and self.add_prefix_space and not token.startswith(SPIECE_UNDERLINE):\n",
    "                    out_string += \" \"\n",
    "                current_sub_tokens.append(token)\n",
    "                logger.debug(f\"特殊トークンではないのでバッファに追加 {current_sub_tokens=}\")\n",
    "                prev_is_special = False\n",
    "\n",
    "        out_string += self.sp_model.decode(current_sub_tokens)\n",
    "        logger.debug(f\"最後のバッファ {current_sub_tokens=} をデコードし集約 {out_string=}\")\n",
    "\n",
    "        logger.info(f\"トークン列の文字列変換完了 {out_string=}\")\n",
    "        return out_string\n",
    "\n",
    "    def save_vocabulary(self, save_directory, filename_prefix: Optional[str] = None) -> tuple[str]:\n",
    "        \"\"\"\n",
    "        Save the vocabulary and special tokens file to a directory.\n",
    "\n",
    "        Args:\n",
    "            save_directory (`str`):\n",
    "                The directory in which to save the vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            `Tuple(str)`: Paths to the files saved.\n",
    "        \"\"\"\n",
    "        logger.info(f\"語彙の保存開始 save_directory={save_directory} filename_prefix={filename_prefix}\")\n",
    "        if not os.path.isdir(save_directory):\n",
    "            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n",
    "            return\n",
    "        out_vocab_file = os.path.join(\n",
    "            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n",
    "        )\n",
    "\n",
    "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file) and os.path.isfile(self.vocab_file):\n",
    "            copyfile(self.vocab_file, out_vocab_file)\n",
    "        elif not os.path.isfile(self.vocab_file):\n",
    "            with open(out_vocab_file, \"wb\") as fi:\n",
    "                content_spiece_model = self.sp_model.serialized_model_proto()\n",
    "                fi.write(content_spiece_model)\n",
    "\n",
    "        logger.info(f\"語彙の保存完了 out_vocab_file={out_vocab_file}\")\n",
    "        return (out_vocab_file,)\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        logger.info(f\"特殊トークン付き入力の構築開始 token_ids_0={token_ids_0} token_ids_1={token_ids_1}\")\n",
    "\n",
    "        # [1]\n",
    "        bos_token_id = [self.bos_token_id] if self.add_bos_token else []\n",
    "\n",
    "        # []\n",
    "        eos_token_id = [self.eos_token_id] if self.add_eos_token else []\n",
    "\n",
    "        # [1, 22557]\n",
    "        output = bos_token_id + token_ids_0 + eos_token_id\n",
    "\n",
    "        # False\n",
    "        if token_ids_1 is not None:\n",
    "            output = output + bos_token_id + token_ids_1 + eos_token_id\n",
    "\n",
    "        logger.info(f\"特殊トークン付き入力の構築完了 output={output}\")\n",
    "        return output\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n",
    "    ) -> list[int]:\n",
    "        \"\"\"\n",
    "        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer `prepare_for_model` method.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (`list[int]`):\n",
    "                List of IDs.\n",
    "            token_ids_1 (`list[int]`, *optional*):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not the token list is already formatted with special tokens for the model.\n",
    "\n",
    "        Returns:\n",
    "            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "        logger.info(f\"特殊トークンマスクの取得開始 token_ids_0={token_ids_0} token_ids_1={token_ids_1} already_has_special_tokens={already_has_special_tokens}\")\n",
    "        if already_has_special_tokens:\n",
    "            res = super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n",
    "            )\n",
    "            logger.info(f\"特殊トークンマスクの取得完了（すでに特殊トークンを設定済み） mask={res}\")\n",
    "            return res\n",
    "\n",
    "        bos_token_id = [1] if self.add_bos_token else []\n",
    "        eos_token_id = [1] if self.add_eos_token else []\n",
    "\n",
    "        if token_ids_1 is None:\n",
    "            res = bos_token_id + ([0] * len(token_ids_0)) + eos_token_id\n",
    "            logger.info(f\"特殊トークンマスクの取得完了（token_ids_1が空） mask={res}\")\n",
    "            return res\n",
    "        res = (\n",
    "            bos_token_id\n",
    "            + ([0] * len(token_ids_0))\n",
    "            + eos_token_id\n",
    "            + bos_token_id\n",
    "            + ([0] * len(token_ids_1))\n",
    "            + eos_token_id\n",
    "        )\n",
    "        logger.info(f\"特殊トークンマスクの取得完了 mask={res}\")\n",
    "        return res\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n",
    "    ) -> list[int]:\n",
    "        \"\"\"\n",
    "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task. An ALBERT\n",
    "        sequence pair mask has the following format:\n",
    "\n",
    "        ```\n",
    "        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence |\n",
    "        ```\n",
    "\n",
    "        if token_ids_1 is None, only returns the first portion of the mask (0s).\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (`list[int]`):\n",
    "                List of ids.\n",
    "            token_ids_1 (`list[int]`, *optional*):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "\n",
    "        Returns:\n",
    "            `list[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n",
    "        \"\"\"\n",
    "        logger.info(f\"シーケンスからトークンタイプIDの作成開始 token_ids_0={token_ids_0} token_ids_1={token_ids_1}\")\n",
    "\n",
    "        # [1]\n",
    "        bos_token_id = [self.bos_token_id] if self.add_bos_token else []\n",
    "\n",
    "        # []\n",
    "        eos_token_id = [self.eos_token_id] if self.add_eos_token else []\n",
    "\n",
    "        # [1, 22557] -> [0, 0]\n",
    "        output = [0] * len(bos_token_id + token_ids_0 + eos_token_id)\n",
    "\n",
    "        # False\n",
    "        if token_ids_1 is not None:\n",
    "            output += [1] * len(bos_token_id + token_ids_1 + eos_token_id)\n",
    "\n",
    "        logger.info(f\"シーケンスからトークンタイプIDの作成完了 output={output}\")\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d9977c",
   "metadata": {},
   "source": [
    "### MixtralBlockSparseTop2MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2e8b9c",
   "metadata": {},
   "source": [
    "MixtralBlockSparseTop2MLPは、SMoEのエキスパートクラス\n",
    "\n",
    "実体は、SwiGLUで実装されたフィードフォワードネットワーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralBlockSparseTop2MLP(nn.Module):\n",
    "    def __init__(self, config: MixtralConfig):\n",
    "        logger.info(f\"MixtralBlockSparseTop2MLPの初期化開始 {config.intermediate_size=}, {config.hidden_size=}, {config.hidden_act=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 14336\n",
    "        self.ffn_dim = config.intermediate_size\n",
    "\n",
    "        # 4096\n",
    "        self.hidden_dim = config.hidden_size\n",
    "\n",
    "        # 4096 -> 14336\n",
    "        self.w1 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n",
    "\n",
    "        # 14336 -> 4096\n",
    "        self.w2 = nn.Linear(self.ffn_dim, self.hidden_dim, bias=False)\n",
    "\n",
    "        # 14336 -> 4096\n",
    "        self.w3 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n",
    "\n",
    "        # SiLU\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "        logger.info(\"MixtralBlockSparseTop2MLPの初期化完了\")\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        logger.info(f\"MixtralBlockSparseTop2MLPの順伝播開始 {hidden_states.shape=}\")\n",
    "\n",
    "        # (1, 4096) -> (1, 14336)\n",
    "        gate = self.act_fn(self.w1(hidden_states))\n",
    "        logger.debug(f\"ゲートを計算 {gate.shape=}\")\n",
    "\n",
    "        # (1, 4096) -> (1, 14336)\n",
    "        up = self.w3(hidden_states)\n",
    "        logger.debug(f\"アッププロジェクションを計算 {up.shape=}\")\n",
    "\n",
    "        # (1, 14336) -> (1, 4096)\n",
    "        current_hidden_states = self.w2(gate * up)\n",
    "        logger.debug(f\"ダウンプロジェクションを計算 {current_hidden_states.shape=}\")\n",
    "\n",
    "        logger.info(f\"MixtralBlockSparseTop2MLPの順伝播完了 {current_hidden_states.shape=}\")\n",
    "        return current_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27229a25",
   "metadata": {},
   "source": [
    "### MixtralSparseMoeBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b991ab09",
   "metadata": {},
   "source": [
    "MixtralSparseMoeBlockは、ルーティングネットワークとエキスパートネットワークを統合するクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7124827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralSparseMoeBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This implementation is\n",
    "    strictly equivalent to standard MoE with full capacity (no\n",
    "    dropped tokens). It's faster since it formulates MoE operations\n",
    "    in terms of block-sparse operations to accommodate imbalanced\n",
    "    assignments of tokens to experts, whereas standard MoE either\n",
    "    (1) drop tokens at the cost of reduced performance or (2) set\n",
    "    capacity factor to number of experts and thus waste computation\n",
    "    and memory on padding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        logger.info(f\"MixtralSparseMoeBlockの初期化開始 {config.num_local_experts=}, {config.num_experts_per_tok=}, {config.hidden_size=}, {config.intermediate_size=}, {config.router_jitter_noise=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 4096\n",
    "        self.hidden_dim = config.hidden_size\n",
    "\n",
    "        # 14336\n",
    "        self.ffn_dim = config.intermediate_size\n",
    "\n",
    "        # 8\n",
    "        self.num_experts = config.num_local_experts\n",
    "\n",
    "        # 2\n",
    "        self.top_k = config.num_experts_per_tok\n",
    "\n",
    "        # ルーティングネットワークを初期化\n",
    "        # 4096 -> 8\n",
    "        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n",
    "\n",
    "        # 8つのエキスパートネットワークを初期化\n",
    "        self.experts = nn.ModuleList([MixtralBlockSparseTop2MLP(config) for _ in range(self.num_experts)])\n",
    "\n",
    "        # Jitter parameters\n",
    "        # 0.0\n",
    "        self.jitter_noise = config.router_jitter_noise\n",
    "\n",
    "        logger.info(\"MixtralSparseMoeBlockの初期化完了\")\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"MixtralSparseMoeBlockの順伝播開始 {hidden_states.shape=}\")\n",
    "\n",
    "        #########\n",
    "        # 初期化 #\n",
    "        #########\n",
    "\n",
    "        # (1, 2, 4096)\n",
    "        batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "\n",
    "        # False\n",
    "        if self.training and self.jitter_noise > 0:\n",
    "            hidden_states *= torch.empty_like(hidden_states).uniform_(1.0 - self.jitter_noise, 1.0 + self.jitter_noise)\n",
    "\n",
    "        # (1 * 2, 4096)\n",
    "        hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "        logger.debug(f\"hidden_statesを整形 {hidden_states.shape=}\")\n",
    "\n",
    "        ##############\n",
    "        # ルーティング #\n",
    "        ##############\n",
    "\n",
    "        # スコア（ロジット）を計算\n",
    "        # (2, 4096) -> (2, 8)\n",
    "        router_logits = self.gate(hidden_states)\n",
    "        logger.debug(f\"ルーターのロジットを計算 {router_logits.shape=}\")\n",
    "\n",
    "        # エキスパートの重みを計算（アップキャストし、ソフトマックスを適用）\n",
    "        # (2, 8)\n",
    "        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "        logger.debug(f\"ルーティングの重みを計算 {routing_weights.shape=}\")\n",
    "\n",
    "        # 上位2つのエキスパートを選択\n",
    "        # (2, 2), (2, 2)\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "        logger.debug(f\"上位2つのエキスパートを選択 {routing_weights.shape=} {selected_experts.shape=}\")\n",
    "\n",
    "        # 2つのエキスパートの重みを正規化し、ダウンキャスト\n",
    "        # (2, 2)\n",
    "        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "        routing_weights = routing_weights.to(hidden_states.dtype)\n",
    "        logger.debug(f\"ルーティングの重みを正規化 {routing_weights.shape=}\")\n",
    "\n",
    "        ############\n",
    "        # 処理の実行 #\n",
    "        ############\n",
    "\n",
    "        # 結果を初期化\n",
    "        # (2, 4096)\n",
    "        final_hidden_states = torch.zeros(\n",
    "            (batch_size * sequence_length, hidden_dim),\n",
    "            dtype=hidden_states.dtype,\n",
    "            device=hidden_states.device\n",
    "        )\n",
    "        logger.debug(f\"最終的なhidden_statesを初期化 {final_hidden_states.shape=}\")\n",
    "\n",
    "        # エキスパートが担当するトークンのマスクを作成\n",
    "        # One hot encode the selected experts to create an expert mask\n",
    "        # this will be used to easily index which expert is going to be sollicitated\n",
    "        # (8, 2, 2)\n",
    "        expert_mask = torch.nn.functional.one_hot(\n",
    "            selected_experts,\n",
    "            num_classes=self.num_experts\n",
    "        ).permute(2, 1, 0)\n",
    "        logger.debug(f\"エキスパートマスクを作成 {expert_mask.shape=}\")\n",
    "\n",
    "        # (4, 1)\n",
    "        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n",
    "        logger.debug(f\"エキスパートヒットを計算 {expert_hit.shape=}\")\n",
    "\n",
    "        for expert_idx in expert_hit:\n",
    "            # エキスパートを取得\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "\n",
    "            # (1,), (1,)\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n",
    "            logger.debug(f\"マスクの値を取得 {expert_idx[0]=} {idx=} {top_x=}\")\n",
    "\n",
    "            # Index the correct hidden states and compute the expert hidden state for\n",
    "            # the current expert. We need to make sure to multiply the output hidden\n",
    "            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n",
    "            # (1, 4096)\n",
    "            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n",
    "            logger.debug(f\"エキスパートの現在のhidden_statesを取得 {current_state.shape=}\")\n",
    "\n",
    "            # エキスパートの順伝播を実行\n",
    "            # (1, 4096) -> (1, 4096)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "            logger.debug(f\"エキスパートの順伝播を実行 {current_hidden_states.shape=}\")\n",
    "\n",
    "            # 結果に重みを掛けて加算\n",
    "            # (2, 4096)\n",
    "            # However `index_add_` only support torch tensors for indexing so we'll use\n",
    "            # the `top_x` tensor here.\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n",
    "            logger.debug(f\"最終的なhidden_statesに加算 {final_hidden_states.shape=}\")\n",
    "\n",
    "        # 形状を元に戻す\n",
    "        # (2, 4096) -> (1, 2, 4096)\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "        logger.debug(f\"hidden_statesの形状を元に戻す {final_hidden_states.shape=}\")\n",
    "\n",
    "        logger.info(f\"MixtralSparseMoeBlockの順伝播完了 {final_hidden_states.shape=} {router_logits.shape=}\")\n",
    "        return final_hidden_states, router_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d84a40",
   "metadata": {},
   "source": [
    "### MixtralRMSNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e970939",
   "metadata": {},
   "source": [
    "MixtralRMSNormは、二乗平均平方根正規化（Root Mean Square Layer Normalization）のクラス\n",
    "\n",
    "データの分布の拡大や縮小を抑制することで、学習を安定化させる\n",
    "\n",
    "レイヤー正規化の簡易版で、中心化をスキップした軽量版\n",
    "\n",
    "$$\n",
    "y_i = \\frac{x_i}{\\sqrt{\\frac{1}{n} \\sum_{j=1}^{n} x_j^2 + \\epsilon}} \\cdot g_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f080a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_kernel_forward_from_hub(\"RMSNorm\")\n",
    "class MixtralRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        MixtralRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        logger.info(f\"MixtralRMSNormの初期化開始 {hidden_size=}, {eps=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # (4096,)\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "\n",
    "        # 1e-05\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "        logger.info(\"MixtralRMSNormの初期化完了\")\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        logger.info(f\"MixtralRMSNormの順伝播開始 {hidden_states.shape=}\")\n",
    "\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "\n",
    "        # (1, 2, 1)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        logger.debug(f\"分散を計算 {variance.shape=}\")\n",
    "\n",
    "        # (1, 2, 4096)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        logger.debug(f\"正規化を適用 {hidden_states.shape=}\")\n",
    "\n",
    "        # (1, 2, 4096)\n",
    "        res = self.weight * hidden_states.to(input_dtype)\n",
    "        logger.debug(f\"ゲインを適用 {res.shape=}\")\n",
    "\n",
    "        logger.info(f\"MixtralRMSNormの順伝播完了 {res.shape=}\")\n",
    "        return res\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c2f76",
   "metadata": {},
   "source": [
    "### MixtralRotaryEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a3c430",
   "metadata": {},
   "source": [
    "MixtralRotaryEmbeddingは、回転位置埋め込み（Rotary Position Embedding: RoPE）の適用に必要なサインとコサインの値を計算するクラス\n",
    "\n",
    "RoPEは、トークンの位置に応じてクエリとキーを変調させることで、位置情報を追加する手法\n",
    "\n",
    "クエリの変調の流れ:\n",
    "\n",
    "1.  128個の要素を持つクエリ$Q$を、前半64個$Q_1$と後半64個の$Q_2$に分ける\n",
    "2. $Q_1$と$Q_2$から一つずつ要素を取り、$(q1, q2)$のペアを作る（=2次元平面を構成）\n",
    "3. ペアに対し角度$\\theta$で回転を適用する\n",
    "\n",
    "2次元回転の公式:\n",
    "\n",
    "$$\n",
    "(q_1 \\cos{\\theta} - q_2 \\sin{\\theta}, q_2 \\cos{\\theta} + q_1 \\sin{\\theta})\n",
    "$$\n",
    "\n",
    "回転の角度:\n",
    "\n",
    "$$\n",
    "\\theta_{m, i} = m\\cdot b^{-\\frac{2i}{d}}\n",
    "$$\n",
    "\n",
    "- $m$: シーケンス内でのトークンの位置（$0, 1, 2,...$）\n",
    "- $b^{-\\frac{2i}{d}}$: 周波数の逆数\n",
    "    - $b$: 基数（$10000$）\n",
    "    - $i$: 次元インデックス（$0, 1, 2, ..., d/2-1$）\n",
    "    - $d$: ベクトルの総次元数\n",
    "\n",
    "トークンの位置が遠い（回転角度の差が大きい）ほど向きが揃わなくなり、クエリとキーの行列積は小さくなる\n",
    "\n",
    "2次元回転の実装は、公式を展開して効率的に計算を行う（apply_rotate_pos_emb関数）:\n",
    "\n",
    "$$\n",
    "(q_1 \\cos{\\theta} - q_2 \\sin{\\theta}, q_2 \\cos{\\theta} + q_1 \\sin{\\theta})\n",
    "= [q_1, q_2] \\cdot \\cos{\\theta} + [-q_2, q_1] \\cdot \\sin{\\theta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(k_1 \\cos{\\theta} - k_2 \\sin{\\theta}, k_2 \\cos{\\theta} + k_1 \\sin{\\theta})\n",
    "= [k_1, k_2] \\cdot \\cos{\\theta} + [-k_2, k_1] \\cdot \\sin{\\theta}\n",
    "$$\n",
    "\n",
    "$x$を$[-x_2, x_1]$に変換するrotate_hal関数を使用し、更に実装を簡単にする:\n",
    "\n",
    "\n",
    "$$\n",
    "[q_1, q_2] \\cdot \\cos{\\theta} + [-q_2, q_1] \\cdot \\sin{\\theta}\n",
    "= Q \\cdot \\cos{\\theta} + \\text{rotate\\_half}(Q) \\cdot \\sin{\\theta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "[k_1, k_2] \\cdot \\cos{\\theta} + [-k_2, k_1] \\cdot \\sin{\\theta}\n",
    "= K \\cdot \\cos{\\theta} + \\text{rotate\\_half}(K) \\cdot \\sin{\\theta}\n",
    "$$GemmaRotaryEmbeddingは、回転位置埋め込み（Rotary Position Embedding: RoPE）の適用に必要なサインとコサインの値を計算するクラス\n",
    "\n",
    "RoPEは、トークンの位置に応じてクエリとキーを変調させることで、位置情報を追加する手法\n",
    "\n",
    "クエリの変調の流れ:\n",
    "\n",
    "1.  128個の要素を持つクエリ$Q$を、前半64個$Q_1$と後半64個の$Q_2$に分ける\n",
    "2. $Q_1$と$Q_2$から一つずつ要素を取り、$(q1, q2)$のペアを作る（=2次元平面を構成）\n",
    "3. ペアに対し角度$\\theta$で回転を適用する\n",
    "\n",
    "2次元回転の公式:\n",
    "\n",
    "$$\n",
    "(q_1 \\cos{\\theta} - q_2 \\sin{\\theta}, q_2 \\cos{\\theta} + q_1 \\sin{\\theta})\n",
    "$$\n",
    "\n",
    "回転の角度:\n",
    "\n",
    "$$\n",
    "\\theta_{m, i} = m\\cdot b^{-\\frac{2i}{d}}\n",
    "$$\n",
    "\n",
    "- $m$: シーケンス内でのトークンの位置（$0, 1, 2,...$）\n",
    "- $b^{-\\frac{2i}{d}}$: 周波数の逆数\n",
    "    - $b$: 基数（$10000$）\n",
    "    - $i$: 次元インデックス（$0, 1, 2, ..., d/2-1$）\n",
    "    - $d$: ベクトルの総次元数\n",
    "\n",
    "トークンの位置が遠い（回転角度の差が大きい）ほど向きが揃わなくなり、クエリとキーの行列積は小さくなる\n",
    "\n",
    "2次元回転の実装は、公式を展開して効率的に計算を行う（apply_rotate_pos_emb関数）:\n",
    "\n",
    "$$\n",
    "(q_1 \\cos{\\theta} - q_2 \\sin{\\theta}, q_2 \\cos{\\theta} + q_1 \\sin{\\theta})\n",
    "= [q_1, q_2] \\cdot \\cos{\\theta} + [-q_2, q_1] \\cdot \\sin{\\theta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(k_1 \\cos{\\theta} - k_2 \\sin{\\theta}, k_2 \\cos{\\theta} + k_1 \\sin{\\theta})\n",
    "= [k_1, k_2] \\cdot \\cos{\\theta} + [-k_2, k_1] \\cdot \\sin{\\theta}\n",
    "$$\n",
    "\n",
    "$x$を$[-x_2, x_1]$に変換するrotate_hal関数を使用し、更に実装を簡単にする:\n",
    "\n",
    "\n",
    "$$\n",
    "[q_1, q_2] \\cdot \\cos{\\theta} + [-q_2, q_1] \\cdot \\sin{\\theta}\n",
    "= Q \\cdot \\cos{\\theta} + \\text{rotate\\_half}(Q) \\cdot \\sin{\\theta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "[k_1, k_2] \\cdot \\cos{\\theta} + [-k_2, k_1] \\cdot \\sin{\\theta}\n",
    "= K \\cdot \\cos{\\theta} + \\text{rotate\\_half}(K) \\cdot \\sin{\\theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    logger.info(f\"rotate_halfの開始 {x.shape=}\")\n",
    "\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    logger.debug(f\"x1を取得 {x1.shape=}\")\n",
    "\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    logger.debug(f\"x2を取得 {x2.shape=}\")\n",
    "\n",
    "    res = torch.cat((-x2, x1), dim=-1)\n",
    "    logger.info(f\"rotate_halfの完了 {res.shape=}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralRotaryEmbedding(nn.Module):\n",
    "    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n",
    "\n",
    "    def __init__(self, config: MixtralConfig, device=None):\n",
    "        logger.info(f\"MixtralRotaryEmbeddingの初期化開始 {config.max_position_embeddings=}\")\n",
    "        super().__init__()\n",
    "\n",
    "        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n",
    "            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n",
    "        else:\n",
    "            self.rope_type = \"default\"\n",
    "\n",
    "        # 32768\n",
    "        self.max_seq_len_cached = config.max_position_embeddings\n",
    "\n",
    "        # 32768\n",
    "        self.original_max_seq_len = config.max_position_embeddings\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # default\n",
    "        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n",
    "        logger.debug(f\"{self.rope_type=}\")\n",
    "\n",
    "        # (64,), 1.0\n",
    "        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n",
    "        logger.debug(f\"{inv_freq.shape=}, {self.attention_scaling=}\")\n",
    "\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "        logger.info(\"MixtralRotaryEmbeddingの初期化完了\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n",
    "    def forward(self, x, position_ids):\n",
    "        logger.info(f\"MixtralRotaryEmbeddingの順伝播開始 {x.shape=} {position_ids.shape=}\")\n",
    "\n",
    "        # (1, 64, 1)\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
    "        logger.debug(f\"{inv_freq_expanded.shape=}\")\n",
    "\n",
    "        # (1, 1, 2)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        logger.debug(f\"{position_ids_expanded.shape=}\")\n",
    "\n",
    "        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n",
    "\n",
    "        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n",
    "            # (1, 64, 1) @ (1, 1, 2) -> (1, 64, 2) -> (1, 2, 64)\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            logger.debug(f\"{freqs.shape=}\")\n",
    "\n",
    "            # (1, 2, 128)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            logger.debug(f\"{emb.shape=}\")\n",
    "\n",
    "            # (1, 2, 128)\n",
    "            cos = emb.cos() * self.attention_scaling\n",
    "\n",
    "            # (1, 2, 128)\n",
    "            sin = emb.sin() * self.attention_scaling\n",
    "\n",
    "        logger.info(f\"MixtralRotaryEmbeddingの順伝播完了 {cos.shape=} {sin.shape=}\")\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7729e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    logger.info(f\"apply_rotary_pos_embの開始 {q.shape=} {k.shape=} {cos.shape=} {sin.shape=} {unsqueeze_dim=}\")\n",
    "\n",
    "    # (1, 2, 128) -> (1, 1, 2, 128)\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "\n",
    "    # (1, 2, 128) -> (1, 1, 2, 128)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "\n",
    "    # (1, 32, 2, 128) * (1, 1, 2, 128) + (1, 32, 2, 128) * (1, 1, 2, 128) -> (1, 32, 2, 128)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "\n",
    "    # (1, 8, 2, 128) * (1, 1, 2, 128) + (1, 8, 2, 128) * (1, 1, 2, 128) -> (1, 8, 2, 128)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "    logger.info(f\"apply_rotary_pos_embの完了 {q_embed.shape=} {k_embed.shape=}\")\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c06577",
   "metadata": {},
   "source": [
    "### MixtralAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95727363",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: MixtralConfig, layer_idx: int):\n",
    "        logger.info(f\"MixtralAttentionの初期化開始 {config.hidden_size=}, {config.num_attention_heads=}, {config.num_key_value_heads=}, {getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=}, {config.attention_dropout=}, {layer_idx=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # 0, 1, 2, ..., 31\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        # 4096 // 32 = 128\n",
    "        self.head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n",
    "        logger.debug(f\"{self.head_dim=}\")\n",
    "\n",
    "        # 32 // 8 = 4\n",
    "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
    "        logger.debug(f\"{self.num_key_value_groups=}\")\n",
    "\n",
    "        # 0.088388347\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        logger.debug(f\"{self.scaling=}\")\n",
    "\n",
    "        # 0.0\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        logger.debug(f\"{self.attention_dropout=}\")\n",
    "\n",
    "        self.is_causal = True\n",
    "\n",
    "        # 4096 -> 32 * 128 = 4096\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n",
    "\n",
    "        # 4096 -> 8 * 128 = 1024\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n",
    "\n",
    "        # 4096 -> 8 * 128 = 1024\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n",
    "\n",
    "        # 32 * 128 = 4096 -> 4096\n",
    "        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n",
    "        logger.info(\"MixtralAttentionの初期化完了\")\n",
    "\n",
    "    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[FlashAttentionKwargs],\n",
    "    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        logger.info(f\"MixtralAttentionの順伝播開始 {hidden_states.shape=} {attention_mask.shape if attention_mask is not None else None} {past_key_values is not None=} {cache_position.shape if cache_position is not None else None}\")\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2)\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, -1, 128)\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "        logger.debug(f\"{hidden_shape=}\")\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 8192) -> (1, 2, 32, 128) -> (1, 32, 2, 128)\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        logger.debug(f\"{query_states.shape=}\")\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 1024) -> (1, 2, 8, 128) -> (1, 8, 2, 128)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        logger.debug(f\"{key_states.shape=}\")\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 1024) -> (1, 2, 8, 128) -> (1, 8, 2, 128)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        logger.debug(f\"{value_states.shape=}\")\n",
    "\n",
    "        # (1, 2, 128), (1, 2, 128)\n",
    "        cos, sin = position_embeddings\n",
    "\n",
    "        # (1, 32, 2, 128), (1, 8, 2, 128)\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if past_key_values is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            # (1, 8, 2, 128), (1, 8, 2, 128)\n",
    "            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "            logger.debug(f\"過去のキー・バリューを更新 {key_states.shape=} {value_states.shape=}\")\n",
    "\n",
    "        # SDPA\n",
    "        attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
    "        logger.debug(f\"アテンションの実装を選択 {attention_interface=}\")\n",
    "\n",
    "        logger.debug(f\"アテンションを計算開始 {query_states.shape=} {key_states.shape=} {value_states.shape=} {attention_mask.shape if attention_mask is not None else None} {self.scaling=} sliding_window={getattr(self.config, 'sliding_window', None)} dropout={0.0 if not self.training else self.attention_dropout}\")\n",
    "\n",
    "        # (1, 32, 2, 128) -> (1, 32, 2, 128)\n",
    "        attn_output, attn_weights = attention_interface(\n",
    "            self,\n",
    "            query_states, # (1, 32, 2, 128)\n",
    "            key_states, # (1, 8, 2, 128)\n",
    "            value_states, # (1, 8, 2, 128)\n",
    "            attention_mask, # None\n",
    "            dropout=0.0 if not self.training else self.attention_dropout, # 0.0\n",
    "            scaling=self.scaling, # 0.088388347\n",
    "            sliding_window=getattr(self.config, \"sliding_window\", None), # None\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        logger.debug(f\"アテンションを計算完了 {attn_output.shape=} {attn_weights.shape if attn_weights is not None else None}\")\n",
    "\n",
    "        # (1, 32, 2, 128) -> (1, 2, 4096)\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        logger.debug(f\"アテンション出力を整形 {attn_output.shape=}\")\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 4096)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        logger.debug(f\"出力のプロジェクションを適用 {attn_output.shape=}\")\n",
    "\n",
    "        logger.info(f\"MixtralAttentionの順伝播完了 {attn_output.shape=}\")\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f94fb11",
   "metadata": {},
   "source": [
    "### MixtralDecoderLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892814ff",
   "metadata": {},
   "source": [
    "MixtralDecoderLayerは、アテンションブロックとSMoEブロックからなるTransformerのデコーダークラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7edf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralDecoderLayer(GradientCheckpointingLayer):\n",
    "    def __init__(self, config: MixtralConfig, layer_idx: int):\n",
    "        logger.info(f\"MixtralDecoderLayerの初期化開始 {config.hidden_size=}, {config.num_attention_heads=}, {config.intermediate_size=}, {layer_idx=}\")\n",
    "        super().__init__()\n",
    "\n",
    "        # 4096\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        # セルフアテンションを初期化\n",
    "        self.self_attn = MixtralAttention(config, layer_idx)\n",
    "\n",
    "        # MoEブロックを初期化\n",
    "        self.block_sparse_moe = MixtralSparseMoeBlock(config)\n",
    "\n",
    "        # アテンションの前のレイヤー正規化\n",
    "        self.input_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        # アテンションの後のレイヤー正規化\n",
    "        self.post_attention_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        logger.info(\"MixtralDecoderLayerの初期化完了\")\n",
    "\n",
    "    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> torch.FloatTensor:\n",
    "        logger.info(f\"MixtralDecoderLayerの順伝播開始 {hidden_states.shape=} {attention_mask.shape if attention_mask is not None else None} {position_ids.shape if position_ids is not None else None} {past_key_values is not None=} {cache_position.shape if cache_position is not None else None}\")\n",
    "\n",
    "        ###################\n",
    "        # セルフアテンション #\n",
    "        ###################\n",
    "\n",
    "        # 残差接続用にコピー\n",
    "        # (1, 2, 4096)\n",
    "        residual = hidden_states\n",
    "\n",
    "        # レイヤー正規化を適用\n",
    "        # (1, 2, 4096) -> (1, 2, 4096)\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # セルフアテンションを適用\n",
    "        hidden_states, _ = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            position_embeddings=position_embeddings,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            cache_position=cache_position,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # 残差接続を適用\n",
    "        # (1, 2, 4096) + (1, 2, 4096) -> (1, 2, 4096)\n",
    "        hidden_states = residual + hidden_states\n",
    "        logger.debug(f\"残差接続を適用 {hidden_states.shape=}\")\n",
    "\n",
    "        ####################\n",
    "        # SMoEブロックを適用 #\n",
    "        ####################\n",
    "\n",
    "        # 残差接続用にコピー\n",
    "        # (1, 2, 4096)\n",
    "        residual = hidden_states\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 4096)\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 4096)\n",
    "        hidden_states, _ = self.block_sparse_moe(hidden_states)\n",
    "\n",
    "        # (1, 2, 4096) + (1, 2, 4096) -> (1, 2, 4096)\n",
    "        hidden_states = residual + hidden_states\n",
    "        logger.debug(f\"残差接続を適用 {hidden_states.shape=}\")\n",
    "\n",
    "        logger.info(f\"MixtralDecoderLayerの順伝播完了 {hidden_states.shape=}\")\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947bfdec",
   "metadata": {},
   "source": [
    "### MixtralPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c321bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralPreTrainedModel(PreTrainedModel):\n",
    "    config: MixtralConfig\n",
    "    base_model_prefix = \"model\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _no_split_modules = [\"MixtralDecoderLayer\"]\n",
    "    _skip_keys_device_placement = [\"past_key_values\"]\n",
    "    _supports_flash_attn = True\n",
    "    _supports_sdpa = True\n",
    "    _supports_flex_attn = True\n",
    "    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n",
    "    _supports_attention_backend = True\n",
    "    _can_record_outputs = {\n",
    "        \"router_logits\": OutputRecorder(MixtralSparseMoeBlock, index=1),\n",
    "        \"hidden_states\": MixtralDecoderLayer,\n",
    "        \"attentions\": MixtralAttention,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3509154b",
   "metadata": {},
   "source": [
    "### MixtralModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31654fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralModel(MixtralPreTrainedModel):\n",
    "    def __init__(self, config: MixtralConfig):\n",
    "        logger.info(f\"MixtralModelの初期化開始 {config.vocab_size=}, {config.hidden_size=}, {config.num_hidden_layers=}, {config.pad_token_id=}\")\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.padding_idx = config.pad_token_id\n",
    "\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "\n",
    "        # 32層のデコーダーレイヤーを初期化\n",
    "        self.layers = nn.ModuleList(\n",
    "            [MixtralDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "\n",
    "        self.norm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        self.rotary_emb = MixtralRotaryEmbedding(config=config)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "        logger.info(\"MixtralModelの初期化完了\")\n",
    "\n",
    "    @check_model_inputs\n",
    "    @auto_docstring\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> MoeModelOutputWithPast:\n",
    "        logger.info(f\"MixtralModelの順伝播開始 {input_ids.shape=} {attention_mask.shape=} {position_ids.shape=} {past_key_values=} {inputs_embeds.shape if inputs_embeds is not None else None} {use_cache=} {cache_position=}\")\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "\n",
    "        if use_cache and past_key_values is None:\n",
    "            past_key_values = DynamicCache(config=self.config)\n",
    "            logger.debug(\"DynamicCacheを初期化\")\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            # (1, 2, 4096)\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "            logger.debug(f\"入力の埋め込みを取得 {inputs_embeds.shape=}\")\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        # create_causal_mask\n",
    "        mask_function = create_causal_mask if self.config.sliding_window is None else create_sliding_window_causal_mask\n",
    "        logger.debug(f\"マスク関数を選択 {mask_function=}\")\n",
    "\n",
    "        # None\n",
    "        causal_mask = mask_function(\n",
    "            config=self.config,\n",
    "            input_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            cache_position=cache_position,\n",
    "            past_key_values=past_key_values,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "        logger.debug(f\"因果マスクを作成 {causal_mask=}\")\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        # (1, 2, 128), (1, 2, 128)\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "        logger.debug(f\"位置埋め込みを作成 {position_embeddings[0].shape=} {position_embeddings[1].shape=}\")\n",
    "\n",
    "        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n",
    "            hidden_states = decoder_layer(\n",
    "                hidden_states,\n",
    "                position_embeddings=position_embeddings,\n",
    "                attention_mask=causal_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 4096)\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        res = MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=past_key_values,\n",
    "        )\n",
    "        logger.info(f\"MixtralModelの順伝播完了 {hidden_states.shape=}\")\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb9c6c",
   "metadata": {},
   "source": [
    "### MixtralForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralForCausalLM(MixtralPreTrainedModel, GenerationMixin):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n",
    "    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n",
    "\n",
    "    def __init__(self, config):\n",
    "        logger.info(\"MixtralForCausalLMの初期化開始\")\n",
    "        super().__init__(config)\n",
    "        self.model = MixtralModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.router_aux_loss_coef = config.router_aux_loss_coef\n",
    "        self.num_experts = config.num_local_experts\n",
    "        self.num_experts_per_tok = config.num_experts_per_tok\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "        logger.info(\"MixtralForCausalLMの初期化完了\")\n",
    "\n",
    "    @can_return_tuple\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_router_logits: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> MoeCausalLMOutputWithPast:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, MixtralForCausalLM\n",
    "\n",
    "        >>> model = MixtralForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "\n",
    "        logger.info(f\"MixtralForCausalLMの順伝播開始 {input_ids.shape if input_ids is not None else None} {attention_mask.shape if attention_mask is not None else None} {position_ids.shape if position_ids is not None else None} {past_key_values is not None=} {inputs_embeds.shape if inputs_embeds is not None else None} {labels.shape if labels is not None else None} {use_cache=} {output_router_logits=} {cache_position.shape if cache_position is not None else None} {logits_to_keep=}\")\n",
    "\n",
    "        output_router_logits = (\n",
    "            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n",
    "        )\n",
    "        logger.debug(f\"{output_router_logits=}\")\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs: MoeModelOutputWithPast = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_router_logits=output_router_logits,\n",
    "            cache_position=cache_position,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
    "        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 32000)\n",
    "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n",
    "\n",
    "        aux_loss = None\n",
    "        if output_router_logits:\n",
    "            aux_loss = load_balancing_loss_func(\n",
    "                outputs.router_logits,\n",
    "                self.num_experts,\n",
    "                self.num_experts_per_tok,\n",
    "                attention_mask,\n",
    "            )\n",
    "            if labels is not None:\n",
    "                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n",
    "\n",
    "        res = MoeCausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            aux_loss=aux_loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            router_logits=outputs.router_logits,\n",
    "        )\n",
    "        logger.info(f\"MixtralForCausalLMの順伝播完了 {logits.shape=}\")\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b9bd30",
   "metadata": {},
   "source": [
    "### 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67238978",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fffc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MixtralForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\", dtype=torch.bfloat16, load_in_4bit=True, device_map=\"auto\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaef015",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"Hello\"\n",
    "logger.info(f\"入力プロンプト {text=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df13cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**model_inputs, max_new_tokens=1)\n",
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c147da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "logger.info(f\"生成されたテキスト {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cf29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.WARN)\n",
    "text= \"Fukuoka is\"\n",
    "model_inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
