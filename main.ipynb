{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c211c7ff",
   "metadata": {},
   "source": [
    "# Mixtral-8x7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3094ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- [Ë´ñÊñá](https://arxiv.org/abs/2401.04088)\n",
    "- [ÂÆüË£Ö](https://github.com/mistralai/mistral-inference)\n",
    "- [„Ç¶„Çß„Éñ„Çµ„Ç§„Éà](https://mistral.ai/news/mixtral-of-experts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d52dffc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Ê¶ÇË¶Å"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9a70e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Mixtral 8x7B** „ÅØ„ÄÅ„Çπ„Éë„Éº„ÇπÊ∑∑Âêà„Ç®„Ç≠„Çπ„Éë„Éº„ÉàÔºàSparse Mixture of Experts: **SMoE**ÔºâË®ÄË™û„É¢„Éá„É´\n",
    "\n",
    "Mistral 7B„Å®Áï∞„Å™„Çä„ÄÅÂêÑÂ±§„Åå8„Å§„ÅÆ„Éï„Ç£„Éº„Éâ„Éï„Ç©„ÉØ„Éº„Éâ„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÔºà**„Ç®„Ç≠„Çπ„Éë„Éº„Éà**Ôºâ„ÅßÊßãÊàê„Åï„Çå„Çã\n",
    "\n",
    "„Éà„Éº„ÇØ„É≥ÊØé„Å´ **„É´„Éº„Çø„Éº„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ** „Åå2„Å§„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÇíÈÅ∏Êäû„Åó„ÄÅ„Åù„ÅÆÂá∫Âäõ„ÇíÁµÑ„ÅøÂêà„Çè„Åõ„Å¶Âá¶ÁêÜ„Åô„Çã\n",
    "\n",
    "„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÇíÂãïÁöÑ„Å´ÈÅ∏Êäû„Åô„Çã‰ªïÁµÑ„Åø„Å´„Çà„Çä„ÄÅ47BÔºà470ÂÑÑÔºâ„ÅÆ„ÅÜ„Å°13BÔºà130ÂÑÑÔºâ„Éë„É©„É°„Éº„Çø„Åó„Åã‰ΩøÁî®„Åó„Å™„ÅÑ\n",
    "\n",
    "Ë®àÁÆó„Ç≥„Çπ„Éà„Å®„É¨„Ç§„ÉÜ„É≥„Ç∑„ÇíÊäë„Åà„Å¶„ÄÅLlama 2 70B„ÇÑGPT-3.5„Å´ÂåπÊïµ„Åô„ÇãÊÄßËÉΩ„ÇíÂÆüÁèæ\n",
    "\n",
    "SFTÔºàSupervised Fine-TuningÔºâ„Å®DPOÔºàDirect Preference OptimizationÔºâ„ÅßË®ìÁ∑¥„Åó„Åü **Mixtral 8x7B Instruct** „ÇÇÂÖ¨Èñã"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8243ed",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## „Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7e1b24",
   "metadata": {},
   "source": [
    "Mixtral„ÅØ„ÄÅMistral 7B„Åã„Çâ‰ª•‰∏ã„ÅÆÂ§âÊõ¥„ÇíÂä†„Åà„Å¶„ÅÑ„Çã:\n",
    "\n",
    "- Sliding Window AttentionÔºàSWAÔºâ„Åã„ÇâFully Dense Attention„Å´Â§âÊõ¥\n",
    "- „Éï„Ç£„Éº„Éâ„Éï„Ç©„ÉØ„Éº„Éâ„Éñ„É≠„ÉÉ„ÇØ„ÇíÊ∑∑Âêà„Ç®„Ç≠„Çπ„Éë„Éº„ÉàÂ±§ÔºàMoEÂ±§Ôºâ„Å´Â§âÊõ¥ \n",
    "\n",
    "![](image/table_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c8739",
   "metadata": {},
   "source": [
    "### „Çπ„Éë„Éº„ÇπÊ∑∑Âêà„Ç®„Ç≠„Çπ„Éë„Éº„ÉàÔºàSparse Mixture of ExpertsÔºâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56350ed",
   "metadata": {},
   "source": [
    "Ê∑∑Âêà„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆË©≥Á¥∞„ÅØÂà•„ÅÆ[Ë´ñÊñá](https://arxiv.org/pdf/2401.04088)„ÇíÂèÇÁÖß\n",
    "\n",
    "ÂÖ•Âäõ $x$ „Å´ÂØæ„Åô„ÇãMoE„ÅÆÂá∫Âäõ„ÅØ„ÄÅ **„Ç®„Ç≠„Çπ„Éë„Éº„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÂá∫Âäõ„ÅÆÈáç„Åø‰ªò„ÅçÂíå** „ÅßÊ±∫„Åæ„Çã\n",
    "\n",
    "„Ç®„Ç≠„Çπ„Éë„Éº„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÈáç„Åø„ÅØ„ÄÅ **„Ç≤„Éº„ÉÜ„Ç£„É≥„Ç∞„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÂá∫Âäõ** „ÅßÊ±∫„Åæ„Çã\n",
    "\n",
    "„Çπ„Éë„Éº„ÇπÊ∑∑Âêà„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆÊ¶ÇË¶ÅÂõ≥:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3996b6a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "![](image/figure_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bfd050",
   "metadata": {},
   "source": [
    "$n$ ÂÄã„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ $\\{E_0, E_1, transformers., E_{n-1}\\}$ „Åå‰∏é„Åà„Çâ„Çå„Åü„Å®„Åç„ÅÆÂá∫Âäõ„ÅÆÈáç„Åø‰ªò„ÅçÂíå:\n",
    "\n",
    "$$\n",
    "\\sum_{i=0}^{n-1}G(x)_{i}\\cdot E_{i}(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667dfb07",
   "metadata": {},
   "source": [
    "- $G(x)_i$: ÂÖ•Âäõ $x$ „Å´ÂØæ„Åô„Çã $i$ Áï™ÁõÆ„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆÈáç„ÅøÔºà„Ç≤„Éº„ÉÜ„Ç£„É≥„Ç∞„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÔºâ\n",
    "- $E_i(x)$: ÂÖ•Âäõ $x$ „Çí $i$ Áï™ÁõÆ„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅåÂá¶ÁêÜ„Åó„ÅüÂá∫ÂäõÔºà„Ç®„Ç≠„Çπ„Éë„Éº„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÔºâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951187ee",
   "metadata": {},
   "source": [
    "Mixtral„ÅÆ„Ç≤„Éº„ÉÜ„Ç£„É≥„Ç∞„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅØ„ÄÅ‰∏ä‰Ωç $K$ ÂÄãÔºàTop-KÔºâ„ÅÆ„É≠„Ç∏„ÉÉ„Éà„Å´ÂØæ„Åó„Å¶„ÇΩ„Éï„Éà„Éû„ÉÉ„ÇØ„Çπ„ÇíÈÅ©Áî®„Åó„ÅüÈñ¢Êï∞:\n",
    "\n",
    "$$\n",
    "G(x) := \\text{Softmax}(\\text{TopK}(x \\cdot W_g))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610d03c1",
   "metadata": {},
   "source": [
    "- $W_g$: „Ç≤„Éº„ÉÜ„Ç£„É≥„Ç∞„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÈáç„ÅøË°åÂàó\n",
    "- $x\\cdot W_g$: ÂêÑ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆ„Çπ„Ç≥„Ç¢Ôºà„É≠„Ç∏„ÉÉ„ÉàÔºâ\n",
    "- $\\text{TopK}$: ‰∏ä‰Ωç$K$ÂÄã„Å´Âê´„Åæ„Çå„Å™„ÅÑ„É≠„Ç∏„ÉÉ„Éà„Çí„Éû„Ç§„Éä„ÇπÁÑ°ÈôêÂ§ß„Å´„Åô„ÇãÈñ¢Êï∞\n",
    "- $\\text{Softmax}(\\cdot)$: „Éû„Ç§„Éä„ÇπÁÑ°ÈôêÂ§ß„Å´„Å™„Å£„Åü„É≠„Ç∏„ÉÉ„Éà„ÇíÈô§„ÅÑ„Å¶ÂêàË®à $1.0$ „ÅÆÁ¢∫ÁéáÂàÜÂ∏É„Å´Â§âÊèõ„Åô„ÇãÈñ¢Êï∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104779c1",
   "metadata": {},
   "source": [
    "‰ΩøÁî®„Åô„Çã„Ç®„Ç≠„Çπ„Éë„Éº„ÉàÊï∞ $K$ „ÇíÂõ∫ÂÆö„Åó„ÄÅ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆÁ∑èÊï∞ $n$ „ÇíÂ¢ó„ÇÑ„Åô„Åì„Å®„Åß„ÄÅÂäπÁéáÁöÑ„Å´„Éë„É©„É°„Éº„ÇøÁ∑èÊï∞„ÇíÂ¢óÂä†„Åß„Åç„Çã:\n",
    "\n",
    "- „Éà„Éº„ÇØ„É≥ÊØé„Å´‰ΩøÁî®„Åô„Çã„Éë„É©„É°„Éº„ÇøÊï∞„Çí **„Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Éë„É©„É°„Éº„ÇøÊï∞** Ôºàactive parameter countÔºâ„Å®Âëº„Å∂\n",
    "- „É¢„Éá„É´„ÅÆ„Éë„É©„É°„Éº„ÇøÁ∑èÊï∞„Çí **„Çπ„Éë„Éº„Çπ„Éë„É©„É°„Éº„ÇøÊï∞** Ôºàsparse parameter countÔºâ„Å®Âëº„Å∂"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630079c7",
   "metadata": {},
   "source": [
    "MoEÂ±§„ÅØ„ÄÅÂçò‰∏Ä„ÇÇ„Åó„Åè„ÅØË§áÊï∞„ÅÆGPU„ÅßÂäπÁéáÁöÑ„Å´ÂÆüË°å„Åß„Åç„Çã:\n",
    "\n",
    "- Âçò‰∏ÄGPU„Åß„ÅÆÂäπÁéáÂåñÊâãÊ≥ï\n",
    "    - [Megablocks][1]: MoE„ÅÆFFN„ÅÆÊìç‰Ωú„ÇíÂ§ß„Åç„Å™„Çπ„Éë„Éº„ÇπË°åÂàó‰πóÁÆó„Å®„Åó„Å¶Êâ±„ÅÑ„ÄÅÂÆüË°åÈÄüÂ∫¶„ÇíÂêë‰∏ä„Åï„Åõ„Çã\n",
    "- Ë§áÊï∞GPU„Åß„ÅÆÂäπÁéáÂåñÊâãÊ≥ï\n",
    "    - „É¢„Éá„É´‰∏¶ÂàóÂåñÔºàModel Parallelism techniquesÔºâ: „É¢„Éá„É´„ÇíÂ±§„Åî„Å®„Å´ÂàÜ„Åë„Å¶Ë§áÊï∞„ÅÆGPU„Å´Â±ïÈñã„Åô„Çã\n",
    "    - [„Ç®„Ç≠„Çπ„Éë„Éº„Éà‰∏¶ÂàóÂåñ][2]ÔºàExpert Parallelism: EPÔºâ: „Ç®„Ç≠„Çπ„Éë„Éº„Éà„Çí„Ç∞„É´„Éº„Éó„Å´ÂàÜ„Åë„Å¶Ë§áÊï∞„ÅÆGPU„Å´Â±ïÈñã„Åô„Çã\n",
    "\n",
    "[1]: https://proceedings.mlsys.org/paper_files/paper/2023/hash/5a54f79333768effe7e8927bcccffe40-Abstract-mlsys2023.html\n",
    "[2]: https://arxiv.org/abs/1701.06538\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d208d",
   "metadata": {},
   "source": [
    "Mixtral„Åß„ÅØ„ÄÅ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÇíSwiGLU„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅßÂÆüË£Ö„Åó„ÄÅ‰ΩøÁî®„Åô„Çã„Ç®„Ç≠„Çπ„Éë„Éº„ÉàÊï∞„Çí $K=2$ „Å®„Åô„Çã:\n",
    "\n",
    "$$\n",
    "y = \\sum_{i=0}^{n-1} \\text{Softmax}(\\text{Top2}(x\\cdot W_g))_i \\cdot \\text{SwiGLU}_i(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc29f0bc",
   "metadata": {},
   "source": [
    "- $\\text{Softmax}(\\text{Top2}(x\\cdot W_g))_i$: $i$ Áï™ÁõÆ„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„Å´ÂØæ„Åô„ÇãÈáç„Åø\n",
    "- $\\text{SwiGLU}_i(x)$: $i$ Áï™ÁõÆ„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆÂá∫Âäõ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e4e45c",
   "metadata": {},
   "source": [
    "## „Éô„É≥„ÉÅ„Éû„Éº„ÇØÁµêÊûú"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946da742",
   "metadata": {},
   "source": [
    "Mixtral„Å®Llama„Çí„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÅßË©ï‰æ°„ÅóÊØîËºÉ:\n",
    "\n",
    "- Â∏∏Ë≠òÊé®Ë´ñÔºà0-shotÔºâ\n",
    "    - Hellaswag: ÊñáËÑà„Åã„ÇâËá™ÁÑ∂„Å´Á∂ö„ÅèÁµêÊú´„ÇíÈÅ∏„Å∂\n",
    "    - Winogrande: ‰ª£ÂêçË©û„ÅåÊåá„Åó„Å¶„ÅÑ„ÇãÂçòË™û„ÇíÈÅ∏„Å∂\n",
    "    - PIQAÔºàPhysical Interaction Question AnsweringÔºâ: Áâ©ÁêÜÁöÑ„Å™ÁêÜËß£„ÅåÂøÖË¶Å„Å™ÈÅ∏ÊäûËÇ¢„ÇíÈÅ∏„Å∂\n",
    "    - SIQAÔºàSocial Interaction QAÔºâ:‰∫∫„ÅÆÊÑüÊÉÖ„ÅÆÁêÜËß£„ÅåÂøÖË¶Å„Å™ÈÅ∏ÊäûËÇ¢„ÇíÈÅ∏„Å∂\n",
    "    - OpenbookQA: ‰∏ÄËà¨ÁöÑ„Å™ÁßëÂ≠¶ÁöÑ‰∫ãÂÆüÔºàOpen BookÔºâ„ÅÆÁêÜËß£„ÅåÂøÖË¶Å„Å™ÈÅ∏ÊäûËÇ¢„ÇíÈÅ∏„Å∂\n",
    "    - ARC-EasyÔºàAI2 Reasoning ChallengeÔºâ: Â∞èÂ≠¶Áîü„É¨„Éô„É´„ÅÆÁßëÂ≠¶„ÅÆÁêÜËß£„ÅåÂøÖË¶Å„Å™ÈÅ∏ÊäûËÇ¢„ÇíÈÅ∏„Å∂\n",
    "    - ARC-Challenge: ‰∏≠Â≠¶Áîü„É¨„Éô„É´„ÅÆÁßëÂ≠¶„ÅÆÁêÜËß£„ÅåÂøÖË¶Å„Å™ÈÅ∏ÊäûËÇ¢„ÇíÈÅ∏„Å∂\n",
    "    - CommonsenseQA: Á§æ‰ºöÂ∏∏Ë≠ò„ÅÆÁêÜËß£„ÅåÂøÖË¶Å„Å™ÈÅ∏ÊäûËÇ¢„ÇíÈÅ∏„Å∂\n",
    "- ‰∏ñÁïåÁü•Ë≠òÔºà5-shotÔºâ\n",
    "    - NaturalQuestions: ‰∏é„Åà„Çâ„Çå„ÅüWikipedia„ÅÆ„Éö„Éº„Ç∏„Åã„ÇâÈï∑„ÅÑÂõûÁ≠î„Å®Áü≠„ÅÑÂõûÁ≠î„ÇíÊäΩÂá∫„Åô„Çã\n",
    "    - TriviaQA: „Ç¶„Çß„Éñ„Éö„Éº„Ç∏„ÇÑWikipedia„ÅÆ„Éö„Éº„Ç∏„Åå‰∏é„Åà„Çâ„Çå„ÄÅ„Åù„Çå„Çâ„ÇíÁµ±Âêà„Åô„ÇãÂøÖË¶Å„ÅÆ„ÅÇ„ÇãÂõûÁ≠î„ÇíÊäΩÂá∫„Åô„Çã\n",
    "- Ë™≠Ëß£Ôºà0-shotÔºâ\n",
    "    - BoolQ: ‰∏é„Åà„Çâ„Çå„ÅüÊñáÁ´†„Å´ÂØæ„Åó„Å¶„ÄÅ„ÅØ„ÅÑ/„ÅÑ„ÅÑ„Åà„ÅßÂõûÁ≠î„Åô„Çã\n",
    "    - QuACÔºàQuestion Answering in ContextÔºâ: ‰∏Ä‰∫∫„ÅÆ„É¶„Éº„Ç∂„Éº„ÅåÈÄ£Á∂ö„Åó„Å¶Ë≥™Âïè„Åó„Åù„Çå„Å´ÂØæ„Åó„Å¶ÂõûÁ≠î„ÅóÁ∂ö„Åë„Çã\n",
    "- Êï∞Â≠¶\n",
    "    - GSM8KÔºà8-shotÔºâ: Â∞èÂ≠¶Áîü„É¨„Éô„É´„ÅÆÁÆóÊï∞ÂïèÈ°å\n",
    "    - MATHÔºà4-shotÔºâ: Á´∂ÊäÄÊï∞Â≠¶„É¨„Éô„É´„ÅÆÈõ£„Åó„ÅÑÊï∞Â≠¶ÂïèÈ°å\n",
    "- „Ç≥„Éº„Éâ\n",
    "    - HumanevalÔºà0-shotÔºâ: ‰∫∫„Åå‰ΩúÊàê„Åó„ÅüPythonÈñ¢Êï∞„Çí„Éí„É≥„Éà„Åã„ÇâÂÆåÊàê„Åï„Åõ„Çã\n",
    "    - MBPPÔºàMostly Basic Python ProgrammingÔºâÔºà3-shotÔºâ: ÂàùÂøÉËÄÖÂêë„Åë„ÅÆÂü∫Êú¨ÁöÑ„Å™Python„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞ÂïèÈ°å„ÇíËß£„Åè\n",
    "- Á∑èÂêà\n",
    "    - MMLUÔºàMassive Multitask Language UnderstandingÔºâÔºà5-shotÔºâ: 57„ÅÆÁï∞„Å™„ÇãÂàÜÈáé„ÅÆÈÅ∏ÊäûÂïèÈ°å\n",
    "    - BBHÔºàBig-Bench HardÔºâÔºà3-shotÔºâ: ÁèæÂú®„ÅÆ„É¢„Éá„É´„ÅåËã¶Êâã„Å®„Åô„Çã23„ÅÆÊåëÊà¶ÁöÑ„Å™„Çø„Çπ„ÇØ\n",
    "    - AGI EvalÔºà3-5-shotÔºâ: Á±≥ÂõΩ„ÅÆÂ§ßÂ≠¶ÂÖ•Â≠¶Ë©¶È®ì„ÄÅÊ≥ïÁßëÂ§ßÂ≠¶Èô¢Ë©¶È®ì„ÄÅÂåªÂ∏´ÂõΩÂÆ∂Ë©¶È®ì„Å™„Å©„ÅÆÈÅ∏ÊäûÂïèÈ°å"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b071e43",
   "metadata": {},
   "source": [
    "Mixtral„ÅØ„ÄÅ„Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Éë„É©„É°„Éº„ÇøÊï∞„Åå5ÂÄçÂ§ö„ÅÑLlama2 70B„ÇíÂ§ö„Åè„ÅÆ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Åß‰∏äÂõû„Å£„ÅüÔºà„Ç≥„Éº„Éâ„Å®Êï∞Â≠¶„ÅåÂº∑„ÅÑÔºâ:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77ec94f",
   "metadata": {},
   "source": [
    "![](image/figure_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc4ba4",
   "metadata": {},
   "source": [
    "![](image/table_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54bdb7d",
   "metadata": {},
   "source": [
    "Mixtral„ÅØ„ÄÅ„Ç¢„ÇØ„ÉÜ„Ç£„Éñ„Éë„É©„É°„Éº„ÇøÊï∞„ÅåÂ∞ë„Å™„ÅèÊÄßËÉΩ„ÅåÈ´ò„ÅÑ:\n",
    "\n",
    "![](image/figure_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd406ac",
   "metadata": {},
   "source": [
    "Mixtral„ÅØ„ÄÅLlaMA 2 70B„Çà„ÇäÂÑ™„Çå„ÄÅGPT-3.5ÔºàGPT-3.5-TurboÔºâ„Å´ÂåπÊïµ„Åô„ÇãÊÄßËÉΩ„ÇíÁ§∫„Åó„Åü:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe974526",
   "metadata": {},
   "source": [
    "![](image/table_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b6938",
   "metadata": {},
   "source": [
    "Mixtral„ÅØ„ÄÅËã±Ë™û„ÅÆ‰ªñ„Å´„Éï„É©„É≥„ÇπË™û„Éª„Éâ„Ç§„ÉÑË™û„Éª„Çπ„Éö„Ç§„É≥Ë™û„Éª„Ç§„Çø„É™„Ç¢Ë™û„ÅßLlama2 70 B„ÇíÂ§ßÂπÖ„Å´‰∏äÂõû„ÇãÊÄßËÉΩ:\n",
    "\n",
    "![](image/table_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00793e7",
   "metadata": {},
   "source": [
    "Èï∑„ÅÑÊñáÁ´†„Åã„ÇâÊé¢„ÅóÂá∫„Åô„Çø„Çπ„ÇØÔºà[passkey retrieval][1]Ôºâ„Åß„ÅØ„ÄÅ100%„ÅÆÊ§úÁ¥¢ÊÄßËÉΩ„ÇíÁ§∫„Åó„ÄÅÂõ∞ÊÉëÂ∫¶ÔºàperplexityÔºâ„ÇÇÈï∑„Åï„Å´Âøú„Åò„Å¶Ê∏õÂ∞ë:\n",
    "\n",
    "![](image/figure_4.png)\n",
    "\n",
    "[1]: https://arxiv.org/abs/2305.16300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e849f",
   "metadata": {},
   "source": [
    "Llama 2„Å®ÊØîËºÉ„Åó„Å¶„ÄÅBBQÔºàBias Benchmark for QAÔºâ„Åß‰Ωé„ÅÑ„Éê„Ç§„Ç¢„Çπ„ÇíÁ§∫„Åó„Åü:\n",
    "\n",
    "![](image/figure_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11470c2d",
   "metadata": {},
   "source": [
    "ÊåáÁ§∫„ÉÅ„É•„Éº„Éã„É≥„Ç∞Ê∏à„Åø„É¢„Éá„É´„ÅØ„ÄÅMT-Bench„Åß„ÅØ„Ç™„Éº„Éó„É≥„Ç¶„Çß„Ç§„Éà„É¢„Éá„É´„ÅÆ‰∏≠„ÅßÊúÄ„ÇÇÈ´ò„ÅÑ:\n",
    "\n",
    "![](image/figure_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbd7cd",
   "metadata": {},
   "source": [
    "## „É´„Éº„ÉÜ„Ç£„É≥„Ç∞ÂàÜÊûê"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af09458c",
   "metadata": {},
   "source": [
    "The Pile„ÅÆÊ§úË®º„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí‰Ωø„ÅÑ„ÄÅ„Éà„Éî„ÉÉ„ÇØÊØé„Å´0Â±§ÁõÆ„Éª15Â±§ÁõÆ„Éª31Â±§ÁõÆ„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„ÉàÈÅ∏ÊäûÁä∂ÊÖã„ÇíÊ∏¨ÂÆö\n",
    "\n",
    "„Éà„Éî„ÉÉ„ÇØ„Å´Âü∫„Å•„ÅÑ„Åü„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆÈÅ∏Êäû„Å´ÊòéÁ¢∫„Å™„Éë„Çø„Éº„É≥„ÅØË¶ã„Çâ„Çå„Å™„Åã„Å£„ÅüÔºàÊï∞Â≠¶„ÅÆ„Åø„Çè„Åö„Åã„Å´ÂèçÂøúÔºâ:\n",
    "\n",
    "![](image/figure_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8077637",
   "metadata": {},
   "source": [
    "![](image/table_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d1b54",
   "metadata": {},
   "source": [
    "„Éà„Éº„ÇØ„É≥„Åî„Å®„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆÂâ≤ÂΩì„Åß„ÅØ„ÄÅ`self`„Éª`Question`„Éª„Ç§„É≥„Éá„É≥„Éà„ÉªÈÄ£Á∂ö„Åó„Åü„Éà„Éº„ÇØ„É≥„ÅåÂêå„Åò„É´„Éº„ÉÜ„Ç£„É≥„Ç∞:\n",
    "\n",
    "![](image/figure_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8549895d",
   "metadata": {},
   "source": [
    "## ÂÆüË£Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d377de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU transformers==4.57.1\n",
    "%pip install -qU sentencepiece protobuf bitsandbytes accelerate\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
    "except ImportError:\n",
    "    from dotenv import load_dotenv\n",
    "    import os\n",
    "    load_dotenv()\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "assert HF_TOKEN\n",
    "\n",
    "import os\n",
    "import logging as logging_\n",
    "import transformers\n",
    "import bitsandbytes\n",
    "from transformers import PretrainedConfig\n",
    "from transformers.utils import logging\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# „Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº\n",
    "\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from typing import TYPE_CHECKING, Any, Optional\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from transformers.convert_slow_tokenizer import import_protobuf\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "from transformers.utils import logging\n",
    "from transformers.utils.import_utils import requires\n",
    "\n",
    "# „É¢„Éá„É´\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA„Çí‰ΩøÁî®„Åß„Åç„Åæ„Åõ„Çì\"\n",
    "\n",
    "from transformers.utils.generic import check_model_inputs\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.cache_utils import Cache, DynamicCache\n",
    "from transformers.generation import GenerationMixin\n",
    "from transformers.integrations import use_kernel_forward_from_hub\n",
    "from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask\n",
    "from transformers.modeling_flash_attention_utils import FlashAttentionKwargs\n",
    "from transformers.modeling_layers import (\n",
    "    GenericForQuestionAnswering,\n",
    "    GenericForSequenceClassification,\n",
    "    GenericForTokenClassification,\n",
    "    GradientCheckpointingLayer,\n",
    ")\n",
    "from transformers.modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n",
    "from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n",
    "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n",
    "from transformers.processing_utils import Unpack\n",
    "from transformers.utils import TransformersKwargs, auto_docstring, can_return_tuple\n",
    "from transformers.utils.deprecation import deprecate_kwarg\n",
    "from transformers.utils.generic import OutputRecorder\n",
    "from transformers.models.mixtral.configuration_mixtral import MixtralConfig\n",
    "\n",
    "# „Éá„Éê„Ç§„ÇπË®≠ÂÆö\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "# „É≠„Ç∞Ë®≠ÂÆö\n",
    "\n",
    "if os.path.exists('debug.log'):\n",
    "    os.remove('debug.log')\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging_.DEBUG:\n",
    "            level = 'üü¶'\n",
    "        case logging_.INFO:\n",
    "            level = 'üü©'\n",
    "        case logging_.WARNING:\n",
    "            level = 'üü®'\n",
    "        case logging_.ERROR:\n",
    "            level = 'üü•'\n",
    "        case logging_.CRITICAL:\n",
    "            level = 'üõë'\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logging.set_verbosity_debug()\n",
    "logger = logging.get_logger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging_.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging_.FileHandler('debug.log')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging_.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.info(f\"Transformers version: {transformers.__version__}\")\n",
    "logger.info(f\"Numpy version: {np.__version__}\")\n",
    "logger.info(f\"BitsAndBytes version: {bitsandbytes.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aba00b",
   "metadata": {},
   "source": [
    "### LlamaTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c79e7",
   "metadata": {},
   "source": [
    "LlamaTokenizer„ÅØ„ÄÅSentencePiece„É¢„Ç∏„É•„Éº„É´„ÅÆ„Éê„Ç§„Éà„É¨„Éô„É´BPEÔºàByte Pair EncodingÔºâ„Çí‰ΩøÁî®„Åó„Åü„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„ÇØ„É©„Çπ:\n",
    "\n",
    "1. `tokenizer(\"Hello\")`„ÇíÂÆüË°å„Åô„Çã„Å®„ÄÅ`tokenize`„É°„ÇΩ„ÉÉ„Éâ„ÅåÂëº„Å≥Âá∫„Åï„Çå„Çã\n",
    "1. `_tokenize`„É°„ÇΩ„ÉÉ„Éâ„Åß„ÄÅÂÖ•Âäõ„ÇíSentencePiece„Åß„Éà„Éº„ÇØ„É≥ÂåñÔºà`Hello` -> `_Hello`Ôºâ\n",
    "1. `_convert_token_to_id`„É°„ÇΩ„ÉÉ„Éâ„Åß„ÄÅ„Éà„Éº„ÇØ„É≥„ÇíID„Å´Â§âÊèõÔºà`_Hello` -> `22557`Ôºâ\n",
    "1. `build_inputs_with_special_tokens`„É°„ÇΩ„ÉÉ„Éâ„Åß„ÄÅÁâπÊÆä„Éà„Éº„ÇØ„É≥„ÇíËøΩÂä†Ôºà`[22557]` -> `[1, 22557]`Ôºâ\n",
    "1. `create_token_type_ids_from_sequences`„É°„ÇΩ„ÉÉ„Éâ„Åß„ÄÅ„Éà„Éº„ÇØ„É≥„Çø„Ç§„ÉóID„Çí‰ΩúÊàêÔºà`[0, 0]`Ôºâ\n",
    "1. ÁµêÊûú„ÇíÊï¥ÂΩ¢„Åó„Å¶Ëøî„Åô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eaeb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Â≠¶ÁøíÊ∏à„ÅøË™ûÂΩô„Éï„Ç°„Ç§„É´„ÅÆ„Éë„Çπ\n",
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer.model\"}\n",
    "\n",
    "# ÂçòË™û„ÅÆÂÖàÈ†≠„ÇíÁ§∫„ÅôÁâπÊÆäÊñáÂ≠ó\n",
    "SPIECE_UNDERLINE = \"‚ñÅ\"\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your \\\n",
    "answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure\\\n",
    " that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not \\\n",
    "correct. If you don't know the answer to a question, please don't share false information.\"\"\"  # fmt: skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ca666",
   "metadata": {},
   "outputs": [],
   "source": [
    "@requires(backends=(\"sentencepiece\",))\n",
    "class LlamaTokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"\n",
    "    Construct a Llama tokenizer. Based on byte-level Byte-Pair-Encoding. The default padding token is unset as there is\n",
    "    no padding token in the original model.\n",
    "\n",
    "    Args:\n",
    "        vocab_file (`str`):\n",
    "            Path to the vocabulary file.\n",
    "        unk_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<unk>\"`):\n",
    "            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
    "            token instead.\n",
    "        bos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"<s>\"`):\n",
    "            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n",
    "        eos_token (`str` or `tokenizers.AddedToken`, *optional*, defaults to `\"</s>\"`):\n",
    "            The end of sequence token.\n",
    "        pad_token (`str` or `tokenizers.AddedToken`, *optional*):\n",
    "            A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by\n",
    "            attention mechanisms or loss computation.\n",
    "        sp_model_kwargs (`dict[str, Any]`, `Optional`, *optional*):\n",
    "            Will be passed to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for\n",
    "            SentencePiece](https://github.com/google/sentencepiece/tree/master/python) can be used, among other things,\n",
    "            to set:\n",
    "\n",
    "            - `enable_sampling`: Enable subword regularization.\n",
    "            - `nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.\n",
    "\n",
    "              - `nbest_size = {0,1}`: No sampling is performed.\n",
    "              - `nbest_size > 1`: samples from the nbest_size results.\n",
    "              - `nbest_size < 0`: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)\n",
    "                using forward-filtering-and-backward-sampling algorithm.\n",
    "\n",
    "            - `alpha`: Smoothing parameter for unigram sampling, and dropout probability of merge operations for\n",
    "              BPE-dropout.\n",
    "\n",
    "        add_bos_token (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to add an `bos_token` at the start of sequences.\n",
    "        add_eos_token (`bool`, *optional*, defaults to `False`):\n",
    "            Whether or not to add an `eos_token` at the end of sequences.\n",
    "        clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n",
    "            Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like\n",
    "            extra spaces.\n",
    "        use_default_system_prompt (`bool`, *optional*, defaults to `False`):\n",
    "            Whether or not the default system prompt for Llama should be used.\n",
    "        spaces_between_special_tokens (`bool`, *optional*, defaults to `False`):\n",
    "            Whether or not to add spaces between special tokens.\n",
    "        legacy (`bool`, *optional*):\n",
    "            Whether or not the `legacy` behavior of the tokenizer should be used. Legacy is before the merge of #24622\n",
    "            and #25224 which includes fixes to properly handle tokens that appear after special tokens.\n",
    "            Make sure to also set `from_slow` to `True`.\n",
    "            A simple example:\n",
    "\n",
    "            - `legacy=True`:\n",
    "            ```python\n",
    "            >>> from transformers import LlamaTokenizerFast\n",
    "\n",
    "            >>> tokenizer = LlamaTokenizerFast.from_pretrained(\"huggyllama/llama-7b\", legacy=True, from_slow=True)\n",
    "            >>> tokenizer.encode(\"Hello <s>.\") # 869 is '‚ñÅ.'\n",
    "            [1, 15043, 29871, 1, 869]\n",
    "            ```\n",
    "            - `legacy=False`:\n",
    "            ```python\n",
    "            >>> from transformers import LlamaTokenizerFast\n",
    "\n",
    "            >>> tokenizer = LlamaTokenizerFast.from_pretrained(\"huggyllama/llama-7b\", legacy=False, from_slow=True)\n",
    "            >>> tokenizer.encode(\"Hello <s>.\")  # 29889 is '.'\n",
    "            [1, 15043, 29871, 1, 29889]\n",
    "            ```\n",
    "            Checkout the [pull request](https://github.com/huggingface/transformers/pull/24565) for more details.\n",
    "        add_prefix_space (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n",
    "            other word. Again, this should be set with `from_slow=True` to make sure it's taken into account.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    model_input_names = [\"input_ids\", \"attention_mask\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        unk_token=\"<unk>\",\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        pad_token=None,\n",
    "        sp_model_kwargs: Optional[dict[str, Any]] = None,\n",
    "        add_bos_token=True,\n",
    "        add_eos_token=False,\n",
    "        clean_up_tokenization_spaces=False,\n",
    "        use_default_system_prompt=False,\n",
    "        spaces_between_special_tokens=False,\n",
    "        legacy=None,\n",
    "        add_prefix_space=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        logger.info(f\"LlamaTokenizer„ÅÆÂàùÊúüÂåñÈñãÂßã vocab_file={vocab_file} unk_token={unk_token} bos_token={bos_token} eos_token={eos_token} pad_token={pad_token} legacy={legacy} add_prefix_space={add_prefix_space}\")\n",
    "\n",
    "        self.sp_model_kwargs = {} if sp_model_kwargs is None else sp_model_kwargs\n",
    "        bos_token = AddedToken(bos_token, normalized=False, special=True) if isinstance(bos_token, str) else bos_token\n",
    "        eos_token = AddedToken(eos_token, normalized=False, special=True) if isinstance(eos_token, str) else eos_token\n",
    "        unk_token = AddedToken(unk_token, normalized=False, special=True) if isinstance(unk_token, str) else unk_token\n",
    "        pad_token = AddedToken(pad_token, normalized=False, special=True) if isinstance(pad_token, str) else pad_token\n",
    "\n",
    "        if legacy is None:\n",
    "            logger.warning_once(\n",
    "                f\"You are using the default legacy behaviour of the {self.__class__}. This is\"\n",
    "                \" expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you.\"\n",
    "                \" If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it\"\n",
    "                \" means, and thoroughly read the reason why this was added as explained in\"\n",
    "                \" https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file\"\n",
    "                \" you can ignore this message\"\n",
    "            )\n",
    "            legacy = True\n",
    "\n",
    "        self.legacy = legacy\n",
    "        self.vocab_file = vocab_file\n",
    "        self.add_bos_token = add_bos_token\n",
    "        self.add_eos_token = add_eos_token\n",
    "        self.use_default_system_prompt = use_default_system_prompt\n",
    "        self.sp_model = self.get_spm_processor(kwargs.pop(\"from_slow\", False))\n",
    "        self.add_prefix_space = add_prefix_space\n",
    "\n",
    "        super().__init__(\n",
    "            bos_token=bos_token,\n",
    "            eos_token=eos_token,\n",
    "            unk_token=unk_token,\n",
    "            pad_token=pad_token,\n",
    "            add_bos_token=add_bos_token,\n",
    "            add_eos_token=add_eos_token,\n",
    "            sp_model_kwargs=self.sp_model_kwargs,\n",
    "            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
    "            use_default_system_prompt=use_default_system_prompt,\n",
    "            spaces_between_special_tokens=spaces_between_special_tokens,\n",
    "            legacy=legacy,\n",
    "            add_prefix_space=add_prefix_space,\n",
    "            **kwargs,\n",
    "        )\n",
    "        logger.info(\"LlamaTokenizer„ÅÆÂàùÊúüÂåñÂÆå‰∫Ü\")\n",
    "\n",
    "    @property\n",
    "    def unk_token_length(self):\n",
    "        return len(self.sp_model.encode(str(self.unk_token)))\n",
    "\n",
    "    # Copied from transformers.models.t5.tokenization_t5.T5Tokenizer.get_spm_processor\n",
    "    def get_spm_processor(self, from_slow=False):\n",
    "        logger.info(f\"SentencePieceProcessor„ÅÆÂèñÂæó from_slow={from_slow} legacy={self.legacy}\")\n",
    "\n",
    "        tokenizer = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n",
    "        if self.legacy or from_slow:  # no dependency on protobuf\n",
    "            tokenizer.Load(self.vocab_file)\n",
    "            return tokenizer\n",
    "\n",
    "        with open(self.vocab_file, \"rb\") as f:\n",
    "            sp_model = f.read()\n",
    "            model_pb2 = import_protobuf(f\"The new behaviour of {self.__class__.__name__} (with `self.legacy = False`)\")\n",
    "            model = model_pb2.ModelProto.FromString(sp_model)\n",
    "            normalizer_spec = model_pb2.NormalizerSpec()\n",
    "            normalizer_spec.add_dummy_prefix = False\n",
    "            model.normalizer_spec.MergeFrom(normalizer_spec)\n",
    "            sp_model = model.SerializeToString()\n",
    "            tokenizer.LoadFromSerializedProto(sp_model)\n",
    "\n",
    "        logger.info(\"SentencePieceProcessor„ÅÆÂèñÂæóÂÆå‰∫Ü\")\n",
    "        return tokenizer\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        state[\"sp_model\"] = None\n",
    "        state[\"sp_model_proto\"] = self.sp_model.serialized_model_proto()\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        self.__dict__.update(d)\n",
    "        self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n",
    "        self.sp_model.LoadFromSerializedProto(self.sp_model_proto)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        \"\"\"Returns vocab size\"\"\"\n",
    "        return self.sp_model.get_piece_size()\n",
    "\n",
    "    def get_vocab(self):\n",
    "        \"\"\"Returns vocab as a dict\"\"\"\n",
    "        logger.info(\"Ë™ûÂΩô„ÅÆÂèñÂæóÈñãÂßã\")\n",
    "\n",
    "        vocab = {self.convert_ids_to_tokens(i): i for i in range(self.vocab_size)}\n",
    "        vocab.update(self.added_tokens_encoder)\n",
    "\n",
    "        logger.info(f\"Ë™ûÂΩô„ÅÆÂèñÂæóÂÆå‰∫Ü vocab_size={len(vocab)}\")\n",
    "        return vocab\n",
    "\n",
    "    # Copied from transformers.models.t5.tokenization_t5.T5Tokenizer.tokenize\n",
    "    def tokenize(self, text: \"TextInput\", **kwargs) -> list[str]:\n",
    "        \"\"\"\n",
    "        Converts a string to a list of tokens. If `self.legacy` is set to `False`, a prefix token is added unless the\n",
    "        first token is special.\n",
    "        \"\"\"\n",
    "        logger.info(f\"„Éà„Éº„ÇØ„É≥ÂåñÈñãÂßã text={text} legacy={self.legacy} add_prefix_space={self.add_prefix_space}\")\n",
    "\n",
    "        # True\n",
    "        if self.legacy or len(text) == 0:\n",
    "\n",
    "            # 'Hello' -> '_Hello'\n",
    "            res = super().tokenize(text, **kwargs)\n",
    "            logger.info(f\"„Éà„Éº„ÇØ„É≥ÂåñÂÆå‰∫ÜÔºàlegacyÔºâ tokens={res}\")\n",
    "            return res\n",
    "\n",
    "        text = text.replace(SPIECE_UNDERLINE, \" \")\n",
    "        if self.add_prefix_space:\n",
    "            text = SPIECE_UNDERLINE + text\n",
    "\n",
    "        tokens = super().tokenize(text, **kwargs)\n",
    "\n",
    "        if len(tokens) > 1 and tokens[0] == SPIECE_UNDERLINE and tokens[1] in self.all_special_tokens:\n",
    "            tokens = tokens[1:]\n",
    "        return tokens\n",
    "\n",
    "    # Copied from transformers.models.t5.tokenization_t5.T5Tokenizer._tokenize\n",
    "    def _tokenize(self, text, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns a tokenized string.\n",
    "\n",
    "        We de-activated the `add_dummy_prefix` option, thus the sentencepiece internals will always strip any\n",
    "        SPIECE_UNDERLINE. For example: `self.sp_model.encode(f\"{SPIECE_UNDERLINE}Hey\", out_type = str)` will give\n",
    "        `['H', 'e', 'y']` instead of `['‚ñÅHe', 'y']`. Thus we always encode `f\"{unk_token}text\"` and strip the\n",
    "        `unk_token`. Here is an example with `unk_token = \"<unk>\"` and `unk_token_length = 4`.\n",
    "        `self.tokenizer.sp_model.encode(\"<unk> Hey\", out_type = str)[4:]`.\n",
    "        \"\"\"\n",
    "        logger.info(f\"_tokenize„ÅÆÈñãÂßã text={text} legacy={self.legacy}\")\n",
    "\n",
    "        # True\n",
    "        if self.legacy or not text.startswith((SPIECE_UNDERLINE, \" \")):\n",
    "            # 'Hello' -> '_Hello'\n",
    "            res = self.sp_model.encode(text, out_type=str)\n",
    "            logger.info(f\"_tokenize„ÅÆÂÆå‰∫ÜÔºàlegacyÔºâ tokens={res}\")\n",
    "            return res\n",
    "\n",
    "        # 1. Encode string + prefix ex: \"<unk> Hey\"\n",
    "        tokens = self.sp_model.encode(self.unk_token + text, out_type=str)\n",
    "        # 2. Remove self.unk_token from ['<','unk','>', '‚ñÅHey']\n",
    "        res = tokens[self.unk_token_length :] if len(tokens) >= self.unk_token_length else tokens\n",
    "        return res\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n",
    "        logger.info(f\"_convert_token_to_id„ÅÆÈñãÂßã token={token}\")\n",
    "        # '_Hello' -> 22557\n",
    "        res = self.sp_model.piece_to_id(token)\n",
    "        logger.info(f\"_convert_token_to_id„ÅÆÂÆå‰∫Ü token={token} id={res}\")\n",
    "        return res\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
    "        # logger.info(f\"_convert_id_to_token„ÅÆÈñãÂßã id={index}\")\n",
    "        token = self.sp_model.IdToPiece(index)\n",
    "        res = token\n",
    "        # logger.info(f\"_convert_id_to_token„ÅÆÂÆå‰∫Ü id={index} token={res}\")\n",
    "        return res\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n",
    "        logger.info(f\"„Éà„Éº„ÇØ„É≥Âàó„ÅÆÊñáÂ≠óÂàóÂ§âÊèõÈñãÂßã tokens={tokens} legacy={self.legacy} add_prefix_space={self.add_prefix_space}\")\n",
    "\n",
    "        # since we manually add the prefix space, we have to remove it when decoding\n",
    "        # ['_Hello', ',] -> ['Hello', ',']\n",
    "        if tokens[0].startswith(SPIECE_UNDERLINE) and self.add_prefix_space:\n",
    "            tokens[0] = tokens[0][1:]\n",
    "            logger.debug(f\"ÊúÄÂàù„ÅÆ„Éà„Éº„ÇØ„É≥„Åã„ÇâÊé•È†≠Ëæû„Çπ„Éö„Éº„Çπ„ÇíÂâäÈô§ {tokens=}\")\n",
    "\n",
    "        current_sub_tokens = []\n",
    "        out_string = \"\"\n",
    "        prev_is_special = False\n",
    "        for i, token in enumerate(tokens):\n",
    "            # make sure that special tokens are not decoded using sentencepiece model\n",
    "            if token in self.all_special_tokens:\n",
    "                if not prev_is_special and i != 0 and self.legacy:\n",
    "                    out_string += \" \"\n",
    "                out_string += self.sp_model.decode(current_sub_tokens) + token\n",
    "                logger.debug(f\"{current_sub_tokens=} „Çí„Éá„Ç≥„Éº„Éâ„ÅóÈõÜÁ¥Ñ {out_string=}\")\n",
    "                prev_is_special = True\n",
    "                current_sub_tokens = []\n",
    "            else:\n",
    "                if prev_is_special and i == 1 and self.add_prefix_space and not token.startswith(SPIECE_UNDERLINE):\n",
    "                    out_string += \" \"\n",
    "                current_sub_tokens.append(token)\n",
    "                logger.debug(f\"ÁâπÊÆä„Éà„Éº„ÇØ„É≥„Åß„ÅØ„Å™„ÅÑ„ÅÆ„Åß„Éê„ÉÉ„Éï„Ç°„Å´ËøΩÂä† {current_sub_tokens=}\")\n",
    "                prev_is_special = False\n",
    "\n",
    "        out_string += self.sp_model.decode(current_sub_tokens)\n",
    "        logger.debug(f\"ÊúÄÂæå„ÅÆ„Éê„ÉÉ„Éï„Ç° {current_sub_tokens=} „Çí„Éá„Ç≥„Éº„Éâ„ÅóÈõÜÁ¥Ñ {out_string=}\")\n",
    "\n",
    "        logger.info(f\"„Éà„Éº„ÇØ„É≥Âàó„ÅÆÊñáÂ≠óÂàóÂ§âÊèõÂÆå‰∫Ü {out_string=}\")\n",
    "        return out_string\n",
    "\n",
    "    def save_vocabulary(self, save_directory, filename_prefix: Optional[str] = None) -> tuple[str]:\n",
    "        \"\"\"\n",
    "        Save the vocabulary and special tokens file to a directory.\n",
    "\n",
    "        Args:\n",
    "            save_directory (`str`):\n",
    "                The directory in which to save the vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            `Tuple(str)`: Paths to the files saved.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Ë™ûÂΩô„ÅÆ‰øùÂ≠òÈñãÂßã save_directory={save_directory} filename_prefix={filename_prefix}\")\n",
    "        if not os.path.isdir(save_directory):\n",
    "            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n",
    "            return\n",
    "        out_vocab_file = os.path.join(\n",
    "            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n",
    "        )\n",
    "\n",
    "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file) and os.path.isfile(self.vocab_file):\n",
    "            copyfile(self.vocab_file, out_vocab_file)\n",
    "        elif not os.path.isfile(self.vocab_file):\n",
    "            with open(out_vocab_file, \"wb\") as fi:\n",
    "                content_spiece_model = self.sp_model.serialized_model_proto()\n",
    "                fi.write(content_spiece_model)\n",
    "\n",
    "        logger.info(f\"Ë™ûÂΩô„ÅÆ‰øùÂ≠òÂÆå‰∫Ü out_vocab_file={out_vocab_file}\")\n",
    "        return (out_vocab_file,)\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        logger.info(f\"ÁâπÊÆä„Éà„Éº„ÇØ„É≥‰ªò„ÅçÂÖ•Âäõ„ÅÆÊßãÁØâÈñãÂßã token_ids_0={token_ids_0} token_ids_1={token_ids_1}\")\n",
    "\n",
    "        # [1]\n",
    "        bos_token_id = [self.bos_token_id] if self.add_bos_token else []\n",
    "\n",
    "        # []\n",
    "        eos_token_id = [self.eos_token_id] if self.add_eos_token else []\n",
    "\n",
    "        # [1, 22557]\n",
    "        output = bos_token_id + token_ids_0 + eos_token_id\n",
    "\n",
    "        # False\n",
    "        if token_ids_1 is not None:\n",
    "            output = output + bos_token_id + token_ids_1 + eos_token_id\n",
    "\n",
    "        logger.info(f\"ÁâπÊÆä„Éà„Éº„ÇØ„É≥‰ªò„ÅçÂÖ•Âäõ„ÅÆÊßãÁØâÂÆå‰∫Ü output={output}\")\n",
    "        return output\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None, already_has_special_tokens: bool = False\n",
    "    ) -> list[int]:\n",
    "        \"\"\"\n",
    "        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer `prepare_for_model` method.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (`list[int]`):\n",
    "                List of IDs.\n",
    "            token_ids_1 (`list[int]`, *optional*):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n",
    "                Whether or not the token list is already formatted with special tokens for the model.\n",
    "\n",
    "        Returns:\n",
    "            `list[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "        logger.info(f\"ÁâπÊÆä„Éà„Éº„ÇØ„É≥„Éû„Çπ„ÇØ„ÅÆÂèñÂæóÈñãÂßã token_ids_0={token_ids_0} token_ids_1={token_ids_1} already_has_special_tokens={already_has_special_tokens}\")\n",
    "        if already_has_special_tokens:\n",
    "            res = super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0, token_ids_1=token_ids_1, already_has_special_tokens=True\n",
    "            )\n",
    "            logger.info(f\"ÁâπÊÆä„Éà„Éº„ÇØ„É≥„Éû„Çπ„ÇØ„ÅÆÂèñÂæóÂÆå‰∫ÜÔºà„Åô„Åß„Å´ÁâπÊÆä„Éà„Éº„ÇØ„É≥„ÇíË®≠ÂÆöÊ∏à„ÅøÔºâ mask={res}\")\n",
    "            return res\n",
    "\n",
    "        bos_token_id = [1] if self.add_bos_token else []\n",
    "        eos_token_id = [1] if self.add_eos_token else []\n",
    "\n",
    "        if token_ids_1 is None:\n",
    "            res = bos_token_id + ([0] * len(token_ids_0)) + eos_token_id\n",
    "            logger.info(f\"ÁâπÊÆä„Éà„Éº„ÇØ„É≥„Éû„Çπ„ÇØ„ÅÆÂèñÂæóÂÆå‰∫ÜÔºàtoken_ids_1„ÅåÁ©∫Ôºâ mask={res}\")\n",
    "            return res\n",
    "        res = (\n",
    "            bos_token_id\n",
    "            + ([0] * len(token_ids_0))\n",
    "            + eos_token_id\n",
    "            + bos_token_id\n",
    "            + ([0] * len(token_ids_1))\n",
    "            + eos_token_id\n",
    "        )\n",
    "        logger.info(f\"ÁâπÊÆä„Éà„Éº„ÇØ„É≥„Éû„Çπ„ÇØ„ÅÆÂèñÂæóÂÆå‰∫Ü mask={res}\")\n",
    "        return res\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: list[int], token_ids_1: Optional[list[int]] = None\n",
    "    ) -> list[int]:\n",
    "        \"\"\"\n",
    "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task. An ALBERT\n",
    "        sequence pair mask has the following format:\n",
    "\n",
    "        ```\n",
    "        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence |\n",
    "        ```\n",
    "\n",
    "        if token_ids_1 is None, only returns the first portion of the mask (0s).\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (`list[int]`):\n",
    "                List of ids.\n",
    "            token_ids_1 (`list[int]`, *optional*):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "\n",
    "        Returns:\n",
    "            `list[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).\n",
    "        \"\"\"\n",
    "        logger.info(f\"„Ç∑„Éº„Ç±„É≥„Çπ„Åã„Çâ„Éà„Éº„ÇØ„É≥„Çø„Ç§„ÉóID„ÅÆ‰ΩúÊàêÈñãÂßã token_ids_0={token_ids_0} token_ids_1={token_ids_1}\")\n",
    "\n",
    "        # [1]\n",
    "        bos_token_id = [self.bos_token_id] if self.add_bos_token else []\n",
    "\n",
    "        # []\n",
    "        eos_token_id = [self.eos_token_id] if self.add_eos_token else []\n",
    "\n",
    "        # [1, 22557] -> [0, 0]\n",
    "        output = [0] * len(bos_token_id + token_ids_0 + eos_token_id)\n",
    "\n",
    "        # False\n",
    "        if token_ids_1 is not None:\n",
    "            output += [1] * len(bos_token_id + token_ids_1 + eos_token_id)\n",
    "\n",
    "        logger.info(f\"„Ç∑„Éº„Ç±„É≥„Çπ„Åã„Çâ„Éà„Éº„ÇØ„É≥„Çø„Ç§„ÉóID„ÅÆ‰ΩúÊàêÂÆå‰∫Ü output={output}\")\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d9977c",
   "metadata": {},
   "source": [
    "### MixtralBlockSparseTop2MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2e8b9c",
   "metadata": {},
   "source": [
    "MixtralBlockSparseTop2MLP„ÅØ„ÄÅSMoE„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÇØ„É©„Çπ\n",
    "\n",
    "ÂÆü‰Ωì„ÅØ„ÄÅSwiGLU„ÅßÂÆüË£Ö„Åï„Çå„Åü„Éï„Ç£„Éº„Éâ„Éï„Ç©„ÉØ„Éº„Éâ„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralBlockSparseTop2MLP(nn.Module):\n",
    "    def __init__(self, config: MixtralConfig):\n",
    "        logger.info(f\"MixtralBlockSparseTop2MLP„ÅÆÂàùÊúüÂåñÈñãÂßã {config.intermediate_size=}, {config.hidden_size=}, {config.hidden_act=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 14336\n",
    "        self.ffn_dim = config.intermediate_size\n",
    "\n",
    "        # 4096\n",
    "        self.hidden_dim = config.hidden_size\n",
    "\n",
    "        # 4096 -> 14336\n",
    "        self.w1 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n",
    "\n",
    "        # 14336 -> 4096\n",
    "        self.w2 = nn.Linear(self.ffn_dim, self.hidden_dim, bias=False)\n",
    "\n",
    "        # 14336 -> 4096\n",
    "        self.w3 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n",
    "\n",
    "        # SiLU\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "        logger.info(\"MixtralBlockSparseTop2MLP„ÅÆÂàùÊúüÂåñÂÆå‰∫Ü\")\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        logger.info(f\"MixtralBlockSparseTop2MLP„ÅÆÈ†Ü‰ºùÊí≠ÈñãÂßã {hidden_states.shape=}\")\n",
    "\n",
    "        # (1, 4096) -> (1, 14336)\n",
    "        gate = self.act_fn(self.w1(hidden_states))\n",
    "        logger.debug(f\"„Ç≤„Éº„Éà„ÇíË®àÁÆó {gate.shape=}\")\n",
    "\n",
    "        # (1, 4096) -> (1, 14336)\n",
    "        up = self.w3(hidden_states)\n",
    "        logger.debug(f\"„Ç¢„ÉÉ„Éó„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„ÇíË®àÁÆó {up.shape=}\")\n",
    "\n",
    "        # (1, 14336) -> (1, 4096)\n",
    "        current_hidden_states = self.w2(gate * up)\n",
    "        logger.debug(f\"„ÉÄ„Ç¶„É≥„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„ÇíË®àÁÆó {current_hidden_states.shape=}\")\n",
    "\n",
    "        logger.info(f\"MixtralBlockSparseTop2MLP„ÅÆÈ†Ü‰ºùÊí≠ÂÆå‰∫Ü {current_hidden_states.shape=}\")\n",
    "        return current_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27229a25",
   "metadata": {},
   "source": [
    "### MixtralSparseMoeBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b991ab09",
   "metadata": {},
   "source": [
    "MixtralSparseMoeBlock„ÅØ„ÄÅ„É´„Éº„ÉÜ„Ç£„É≥„Ç∞„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Å®„Ç®„Ç≠„Çπ„Éë„Éº„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÇíÁµ±Âêà„Åô„Çã„ÇØ„É©„Çπ\n",
    "\n",
    "ÂøÖË¶ÅÊúÄÂ∞èÈôê„ÅÆÈ†Ü‰ºùÊí≠Êï∞„ÅßÂá¶ÁêÜ„ÅåÂÆüË°å„Åß„Åç„Çã„Çà„ÅÜ„Å´ÊúÄÈÅ©Âåñ„Åï„Çå„Å¶„ÅÑ„Çã:\n",
    "\n",
    "1. ÂÖ•Âäõ„ÇíÁ∑öÂΩ¢Â±§„Å´ÈÄö„Åó„ÄÅ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„Åî„Å®„ÅÆ„É≠„Ç∏„ÉÉ„Éà„ÇíË®àÁÆó„Åó„ÄÅ„ÇΩ„Éï„Éà„Éû„ÉÉ„ÇØ„Çπ„ÇíÈÅ©Áî®\n",
    "2. ‰∏ä‰Ωç2„Å§„ÅÆÂÄ§„Å®„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÇíÊäΩÂá∫„Åó„ÄÅ„Åù„ÅÆÂêàË®à„Åå1.0„Å´„Å™„Çã„Çà„ÅÜ„Å´Èáç„Åø„ÇíÊ≠£Ë¶èÂåñ\n",
    "3. „Ç®„Ç≠„Çπ„Éë„Éº„Éà„Åî„Å®„Å´ÊãÖÂΩì„Åô„Çã„Éà„Éº„ÇØ„É≥„ÇíÈ†Ü‰ºùÊí≠„Åó„ÄÅÈáç„Åø„Çí„Åã„Åë„Å¶„ÄÅ„Ç¢„Ç≠„É•„Éº„É†„É¨„Éº„Çø„Éº„Å´Âä†ÁÆó\n",
    "4. „Ç¢„Ç≠„É•„Éº„É†„É¨„Éº„Çø„Éº„ÇíËøî„Åô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7124827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralSparseMoeBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This implementation is\n",
    "    strictly equivalent to standard MoE with full capacity (no\n",
    "    dropped tokens). It's faster since it formulates MoE operations\n",
    "    in terms of block-sparse operations to accommodate imbalanced\n",
    "    assignments of tokens to experts, whereas standard MoE either\n",
    "    (1) drop tokens at the cost of reduced performance or (2) set\n",
    "    capacity factor to number of experts and thus waste computation\n",
    "    and memory on padding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        logger.info(f\"MixtralSparseMoeBlock„ÅÆÂàùÊúüÂåñÈñãÂßã {config.num_local_experts=}, {config.num_experts_per_tok=}, {config.hidden_size=}, {config.intermediate_size=}, {config.router_jitter_noise=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 4096\n",
    "        self.hidden_dim = config.hidden_size\n",
    "\n",
    "        # 14336\n",
    "        self.ffn_dim = config.intermediate_size\n",
    "\n",
    "        # 8\n",
    "        self.num_experts = config.num_local_experts\n",
    "\n",
    "        # 2\n",
    "        self.top_k = config.num_experts_per_tok\n",
    "\n",
    "        # „É´„Éº„ÉÜ„Ç£„É≥„Ç∞„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÇíÂàùÊúüÂåñ\n",
    "        # 4096 -> 8\n",
    "        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n",
    "\n",
    "        # 8„Å§„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÇíÂàùÊúüÂåñ\n",
    "        self.experts = nn.ModuleList([MixtralBlockSparseTop2MLP(config) for _ in range(self.num_experts)])\n",
    "\n",
    "        # Jitter parameters\n",
    "        # 0.0\n",
    "        self.jitter_noise = config.router_jitter_noise\n",
    "\n",
    "        logger.info(\"MixtralSparseMoeBlock„ÅÆÂàùÊúüÂåñÂÆå‰∫Ü\")\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        logger.info(f\"MixtralSparseMoeBlock„ÅÆÈ†Ü‰ºùÊí≠ÈñãÂßã {hidden_states.shape=}\")\n",
    "\n",
    "        #########\n",
    "        # ÂàùÊúüÂåñ #\n",
    "        #########\n",
    "\n",
    "        # („Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫, „Ç∑„Éº„Ç±„É≥„ÇπÈï∑, Èö†„ÇåÂ±§Ê¨°ÂÖÉ)\n",
    "        # (1, 2, 4096)\n",
    "        batch_size, sequence_length, hidden_dim = hidden_states.shape\n",
    "\n",
    "        # False\n",
    "        if self.training and self.jitter_noise > 0:\n",
    "            hidden_states *= torch.empty_like(hidden_states).uniform_(1.0 - self.jitter_noise, 1.0 + self.jitter_noise)\n",
    "\n",
    "        # (Á∑è„Éà„Éº„ÇØ„É≥Êï∞, Èö†„ÇåÂ±§Ê¨°ÂÖÉ)„Å´Êï¥ÂΩ¢\n",
    "        # (1 * 2, 4096)\n",
    "        hidden_states = hidden_states.view(-1, hidden_dim)\n",
    "        logger.debug(f\"hidden_states„ÇíÊï¥ÂΩ¢ {hidden_states.shape=}\")\n",
    "\n",
    "        ##############\n",
    "        # „É´„Éº„ÉÜ„Ç£„É≥„Ç∞ #\n",
    "        ##############\n",
    "\n",
    "        # „Çπ„Ç≥„Ç¢Ôºà„É≠„Ç∏„ÉÉ„ÉàÔºâ„ÇíË®àÁÆó\n",
    "        # (2, 4096) -> (2, 8)\n",
    "        router_logits = self.gate(hidden_states)\n",
    "        logger.debug(f\"„É´„Éº„Çø„Éº„ÅÆ„É≠„Ç∏„ÉÉ„Éà„ÇíË®àÁÆó {router_logits.shape=}\")\n",
    "\n",
    "        # „Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆÈáç„Åø„ÇíË®àÁÆóÔºà„Ç¢„ÉÉ„Éó„Ç≠„É£„Çπ„Éà„Åó„ÄÅ„ÇΩ„Éï„Éà„Éû„ÉÉ„ÇØ„Çπ„ÇíÈÅ©Áî®Ôºâ\n",
    "        # (2, 8) -> (2, 8)\n",
    "        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "        logger.debug(f\"„É´„Éº„ÉÜ„Ç£„É≥„Ç∞„ÅÆÈáç„Åø„ÇíË®àÁÆó {routing_weights.shape=}\")\n",
    "\n",
    "        # ‰∏ä‰Ωç2„Å§„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÇíÈÅ∏Êäû\n",
    "        # routing_weights: 2„Å§„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆÈáç„Åø („Ç∑„Éº„Ç±„É≥„ÇπÈï∑, „Ç®„Ç≠„Çπ„Éë„Éº„ÉàÊï∞)\n",
    "        # selected_experts: 0„Åã„Çâ7„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ („Ç∑„Éº„Ç±„É≥„ÇπÈï∑, „Ç®„Ç≠„Çπ„Éë„Éº„ÉàÊï∞)\n",
    "        # (2, 8) -> (2, 2), (2, 2)\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "        logger.debug(f\"‰∏ä‰Ωç2„Å§„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÇíÈÅ∏Êäû {routing_weights.shape=} {selected_experts.shape=}\")\n",
    "\n",
    "        # 2„Å§„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆÈáç„Åø„ÅåÂêàË®à1.0„Å´„Å™„Çã„Çà„ÅÜ„Å´Ê≠£Ë¶èÂåñ„Åó„ÄÅ„ÉÄ„Ç¶„É≥„Ç≠„É£„Çπ„Éà („Ç∑„Éº„Ç±„É≥„ÇπÈï∑, „Ç®„Ç≠„Çπ„Éë„Éº„ÉàÊï∞)\n",
    "        # (2, 2)\n",
    "        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n",
    "        routing_weights = routing_weights.to(hidden_states.dtype)\n",
    "        logger.debug(f\"„É´„Éº„ÉÜ„Ç£„É≥„Ç∞„ÅÆÈáç„Åø„ÇíÊ≠£Ë¶èÂåñ {routing_weights.shape=}\")\n",
    "\n",
    "        ############\n",
    "        # Âá¶ÁêÜ„ÅÆÂÆüË°å #\n",
    "        ############\n",
    "\n",
    "        # „Ç¢„Ç≠„É•„Éº„É†„É¨„Éº„Çø„Éº„ÇíÂàùÊúüÂåñ\n",
    "        # (2, 4096)\n",
    "        final_hidden_states = torch.zeros(\n",
    "            (batch_size * sequence_length, hidden_dim),\n",
    "            dtype=hidden_states.dtype,\n",
    "            device=hidden_states.device\n",
    "        )\n",
    "        logger.debug(f\"ÊúÄÁµÇÁöÑ„Å™hidden_states„ÇíÂàùÊúüÂåñ {final_hidden_states.shape=}\")\n",
    "\n",
    "        # „Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅåÊãÖÂΩì„Åô„Çã„Éà„Éº„ÇØ„É≥„ÅÆ„ÉØ„É≥„Éõ„ÉÉ„Éà„Ç®„É≥„Ç≥„Éº„Éá„Ç£„É≥„Ç∞„Éû„Çπ„ÇØ„Çí‰ΩúÊàê („Ç®„Ç≠„Çπ„Éë„Éº„ÉàÊï∞, ÈÅ∏ÊäûËÇ¢, Á∑è„Éà„Éº„ÇØ„É≥Êï∞)\n",
    "        # ‰æã: „Ç®„Ç≠„Çπ„Éë„Éº„Éà0„ÅÆ„Éû„Çπ„ÇØ„Åå [[0, 1], [0, 0]] „ÅÆÂ†¥Âêà„ÄÅ0Áï™ÁõÆ„ÅÆÈÅ∏ÊäûËÇ¢„ÅÆ1Áï™ÁõÆ„ÅÆ„Éà„Éº„ÇØ„É≥„ÇíÊãÖÂΩì\n",
    "        # (2, 2) -> (2, 2, 8) -> (8, 2, 2)\n",
    "        expert_mask = torch.nn.functional.one_hot(\n",
    "            selected_experts,\n",
    "            num_classes=self.num_experts\n",
    "        ).permute(2, 1, 0)\n",
    "        logger.debug(f\"„Ç®„Ç≠„Çπ„Éë„Éº„Éà„Éû„Çπ„ÇØ„Çí‰ΩúÊàê {expert_mask.shape=}\")\n",
    "\n",
    "        # „Ç®„Ç≠„Çπ„Éë„Éº„Éà„Éí„ÉÉ„ÉàÔºàÂ∞ë„Å™„Åè„Å®„ÇÇ1„Å§„ÅÆ„Éà„Éº„ÇØ„É≥„ÇíÊãÖÂΩì„Åô„Çã„Ç®„Ç≠„Çπ„Éë„Éº„ÉàÔºâ„ÇíÂèñÂæó\n",
    "        # „Ç®„Ç≠„Çπ„Éë„Éº„Éà„Åî„Å®„ÅÆ„Éà„Éº„ÇØ„É≥ÊãÖÂΩìÊï∞„ÇíÈõÜË®à„Åó„ÄÅ0„Çà„ÇäÂ§ß„Åç„ÅÑ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÇíÂèñÂæó\n",
    "        # (8, 2, 2) -> (8,) -> (8,) -> (4, 1)\n",
    "        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n",
    "        logger.debug(f\"„Ç®„Ç≠„Çπ„Éë„Éº„Éà„Éí„ÉÉ„Éà„ÇíË®àÁÆó {expert_hit.shape=}\")\n",
    "\n",
    "        # „Ç®„Ç≠„Çπ„Éë„Éº„Éà„Éí„ÉÉ„Éà„Åî„Å®„Å´Âá¶ÁêÜ„ÇíÂÆüË°å\n",
    "        for expert_idx in expert_hit:\n",
    "            # „Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÇíÂèñÂæó\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "\n",
    "            # „Åì„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅåÊãÖÂΩì„Åô„ÇãÈÅ∏ÊäûËÇ¢„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„Å®„Éà„Éº„ÇØ„É≥„ÅÆ„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÇíÂèñÂæó\n",
    "            # ‰æã: „Ç®„Ç≠„Çπ„Éë„Éº„Éà0„ÅÆ„Éû„Çπ„ÇØ„Åå [[0, 1], [0, 0]] „ÅÆÂ†¥Âêà„ÄÅ\n",
    "            # 0Áï™ÁõÆ„ÅÆÈÅ∏ÊäûËÇ¢(idx=0)„ÅÆ1Áï™ÁõÆ„ÅÆ„Éà„Éº„ÇØ„É≥(top_x=1)„ÇíÊãÖÂΩì\n",
    "            # (1,), (1,)\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n",
    "            logger.debug(f\"„Éû„Çπ„ÇØ„ÅÆÂÄ§„ÇíÂèñÂæó {expert_idx[0]=} {idx=} {top_x=}\")\n",
    "\n",
    "            # „Åì„ÅÆ„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅåÊãÖÂΩì„Åô„Çã„Éà„Éº„ÇØ„É≥„ÇíÊäú„ÅçÂá∫„Åô\n",
    "            # (2, 4096) -> (1, 4096) or (2, 4096)\n",
    "            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n",
    "            logger.debug(f\"„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆÁèæÂú®„ÅÆhidden_states„ÇíÂèñÂæó {current_state.shape=}\")\n",
    "\n",
    "            # „Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆÈ†Ü‰ºùÊí≠„Çí„Éê„ÉÉ„ÉÅ„ÅßÂÆüË°å„Åó„ÄÅÈáç„Åø„ÇíÊéõ„Åë„Çã\n",
    "            # (1, 4096) -> (1, 4096) or (2, 4096) -> (2, 4096)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n",
    "            logger.debug(f\"„Ç®„Ç≠„Çπ„Éë„Éº„Éà„ÅÆÈ†Ü‰ºùÊí≠„ÇíÂÆüË°å {current_hidden_states.shape=}\")\n",
    "\n",
    "            # ÂÖÉ„ÅÆ„Éà„Éº„ÇØ„É≥„Åå„ÅÇ„Å£„ÅüÂ†¥ÊâÄ„Å´Âä†ÁÆó\n",
    "            # (2, 4096)\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n",
    "            logger.debug(f\"ÊúÄÁµÇÁöÑ„Å™hidden_states„Å´Âä†ÁÆó {final_hidden_states.shape=}\")\n",
    "\n",
    "        # ÂΩ¢Áä∂„ÇíÂÖÉ„Å´Êàª„Åô\n",
    "        # (2, 4096) -> (1, 2, 4096)\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "        logger.debug(f\"hidden_states„ÅÆÂΩ¢Áä∂„ÇíÂÖÉ„Å´Êàª„Åô {final_hidden_states.shape=}\")\n",
    "\n",
    "        logger.info(f\"MixtralSparseMoeBlock„ÅÆÈ†Ü‰ºùÊí≠ÂÆå‰∫Ü {final_hidden_states.shape=} {router_logits.shape=}\")\n",
    "        return final_hidden_states, router_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d84a40",
   "metadata": {},
   "source": [
    "### MixtralRMSNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e970939",
   "metadata": {},
   "source": [
    "MixtralRMSNorm„ÅØ„ÄÅ‰∫å‰πóÂπ≥ÂùáÂπ≥ÊñπÊ†πÊ≠£Ë¶èÂåñÔºàRoot Mean Square Layer NormalizationÔºâ„ÅÆ„ÇØ„É©„Çπ\n",
    "\n",
    "„Éá„Éº„Çø„ÅÆÂàÜÂ∏É„ÅÆÊã°Â§ß„ÇÑÁ∏ÆÂ∞è„ÇíÊäëÂà∂„Åô„Çã„Åì„Å®„Åß„ÄÅÂ≠¶Áøí„ÇíÂÆâÂÆöÂåñ„Åï„Åõ„Çã\n",
    "\n",
    "„É¨„Ç§„É§„ÉºÊ≠£Ë¶èÂåñ„ÅÆÁ∞°ÊòìÁâà„Åß„ÄÅ‰∏≠ÂøÉÂåñ„Çí„Çπ„Ç≠„ÉÉ„Éó„Åó„ÅüËªΩÈáèÁâà\n",
    "\n",
    "$$\n",
    "y_i = \\frac{x_i}{\\sqrt{\\frac{1}{n} \\sum_{j=1}^{n} x_j^2 + \\epsilon}} \\cdot g_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f080a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_kernel_forward_from_hub(\"RMSNorm\")\n",
    "class MixtralRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        MixtralRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        logger.info(f\"MixtralRMSNorm„ÅÆÂàùÊúüÂåñÈñãÂßã {hidden_size=}, {eps=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # (4096,)\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "\n",
    "        # 1e-05\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "        logger.info(\"MixtralRMSNorm„ÅÆÂàùÊúüÂåñÂÆå‰∫Ü\")\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        logger.info(f\"MixtralRMSNorm„ÅÆÈ†Ü‰ºùÊí≠ÈñãÂßã {hidden_states.shape=}\")\n",
    "\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "\n",
    "        # (1, 2, 1)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        logger.debug(f\"ÂàÜÊï£„ÇíË®àÁÆó {variance.shape=}\")\n",
    "\n",
    "        # (1, 2, 4096)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        logger.debug(f\"Ê≠£Ë¶èÂåñ„ÇíÈÅ©Áî® {hidden_states.shape=}\")\n",
    "\n",
    "        # (1, 2, 4096)\n",
    "        res = self.weight * hidden_states.to(input_dtype)\n",
    "        logger.debug(f\"„Ç≤„Ç§„É≥„ÇíÈÅ©Áî® {res.shape=}\")\n",
    "\n",
    "        logger.info(f\"MixtralRMSNorm„ÅÆÈ†Ü‰ºùÊí≠ÂÆå‰∫Ü {res.shape=}\")\n",
    "        return res\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c2f76",
   "metadata": {},
   "source": [
    "### MixtralRotaryEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a3c430",
   "metadata": {},
   "source": [
    "MixtralRotaryEmbedding„ÅØ„ÄÅÂõûËª¢‰ΩçÁΩÆÂüã„ÇÅËæº„ÅøÔºàRotary Position Embedding: RoPEÔºâ„ÅÆÈÅ©Áî®„Å´ÂøÖË¶Å„Å™„Çµ„Ç§„É≥„Å®„Ç≥„Çµ„Ç§„É≥„ÅÆÂÄ§„ÇíË®àÁÆó„Åô„Çã„ÇØ„É©„Çπ\n",
    "\n",
    "RoPE„ÅØ„ÄÅ„Éà„Éº„ÇØ„É≥„ÅÆ‰ΩçÁΩÆ„Å´Âøú„Åò„Å¶„ÇØ„Ç®„É™„Å®„Ç≠„Éº„ÇíÂ§âË™ø„Åï„Åõ„Çã„Åì„Å®„Åß„ÄÅ‰ΩçÁΩÆÊÉÖÂ†±„ÇíËøΩÂä†„Åô„ÇãÊâãÊ≥ï\n",
    "\n",
    "„ÇØ„Ç®„É™„ÅÆÂ§âË™ø„ÅÆÊµÅ„Çå:\n",
    "\n",
    "1.  128ÂÄã„ÅÆË¶ÅÁ¥†„ÇíÊåÅ„Å§„ÇØ„Ç®„É™$Q$„Çí„ÄÅÂâçÂçä64ÂÄã$Q_1$„Å®ÂæåÂçä64ÂÄã„ÅÆ$Q_2$„Å´ÂàÜ„Åë„Çã\n",
    "2. $Q_1$„Å®$Q_2$„Åã„Çâ‰∏Ä„Å§„Åö„Å§Ë¶ÅÁ¥†„ÇíÂèñ„Çä„ÄÅ$(q1, q2)$„ÅÆ„Éö„Ç¢„Çí‰Ωú„ÇãÔºà=2Ê¨°ÂÖÉÂπ≥Èù¢„ÇíÊßãÊàêÔºâ\n",
    "3. „Éö„Ç¢„Å´ÂØæ„ÅóËßíÂ∫¶$\\theta$„ÅßÂõûËª¢„ÇíÈÅ©Áî®„Åô„Çã\n",
    "\n",
    "2Ê¨°ÂÖÉÂõûËª¢„ÅÆÂÖ¨Âºè:\n",
    "\n",
    "$$\n",
    "(q_1 \\cos{\\theta} - q_2 \\sin{\\theta}, q_2 \\cos{\\theta} + q_1 \\sin{\\theta})\n",
    "$$\n",
    "\n",
    "ÂõûËª¢„ÅÆËßíÂ∫¶:\n",
    "\n",
    "$$\n",
    "\\theta_{m, i} = m\\cdot b^{-\\frac{2i}{d}}\n",
    "$$\n",
    "\n",
    "- $m$: „Ç∑„Éº„Ç±„É≥„ÇπÂÜÖ„Åß„ÅÆ„Éà„Éº„ÇØ„É≥„ÅÆ‰ΩçÁΩÆÔºà$0, 1, 2,...$Ôºâ\n",
    "- $b^{-\\frac{2i}{d}}$: Âë®Ê≥¢Êï∞„ÅÆÈÄÜÊï∞\n",
    "    - $b$: Âü∫Êï∞Ôºà$10000$Ôºâ\n",
    "    - $i$: Ê¨°ÂÖÉ„Ç§„É≥„Éá„ÉÉ„ÇØ„ÇπÔºà$0, 1, 2, ..., d/2-1$Ôºâ\n",
    "    - $d$: „Éô„ÇØ„Éà„É´„ÅÆÁ∑èÊ¨°ÂÖÉÊï∞\n",
    "\n",
    "„Éà„Éº„ÇØ„É≥„ÅÆ‰ΩçÁΩÆ„ÅåÈÅ†„ÅÑÔºàÂõûËª¢ËßíÂ∫¶„ÅÆÂ∑Æ„ÅåÂ§ß„Åç„ÅÑÔºâ„Åª„Å©Âêë„Åç„ÅåÊèÉ„Çè„Å™„Åè„Å™„Çä„ÄÅ„ÇØ„Ç®„É™„Å®„Ç≠„Éº„ÅÆË°åÂàóÁ©ç„ÅØÂ∞è„Åï„Åè„Å™„Çã\n",
    "\n",
    "2Ê¨°ÂÖÉÂõûËª¢„ÅÆÂÆüË£Ö„ÅØ„ÄÅÂÖ¨Âºè„ÇíÂ±ïÈñã„Åó„Å¶ÂäπÁéáÁöÑ„Å´Ë®àÁÆó„ÇíË°å„ÅÜÔºàapply_rotate_pos_embÈñ¢Êï∞Ôºâ:\n",
    "\n",
    "$$\n",
    "(q_1 \\cos{\\theta} - q_2 \\sin{\\theta}, q_2 \\cos{\\theta} + q_1 \\sin{\\theta})\n",
    "= [q_1, q_2] \\cdot \\cos{\\theta} + [-q_2, q_1] \\cdot \\sin{\\theta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(k_1 \\cos{\\theta} - k_2 \\sin{\\theta}, k_2 \\cos{\\theta} + k_1 \\sin{\\theta})\n",
    "= [k_1, k_2] \\cdot \\cos{\\theta} + [-k_2, k_1] \\cdot \\sin{\\theta}\n",
    "$$\n",
    "\n",
    "$x$„Çí$[-x_2, x_1]$„Å´Â§âÊèõ„Åô„Çãrotate_halÈñ¢Êï∞„Çí‰ΩøÁî®„Åó„ÄÅÊõ¥„Å´ÂÆüË£Ö„ÇíÁ∞°Âçò„Å´„Åô„Çã:\n",
    "\n",
    "\n",
    "$$\n",
    "[q_1, q_2] \\cdot \\cos{\\theta} + [-q_2, q_1] \\cdot \\sin{\\theta}\n",
    "= Q \\cdot \\cos{\\theta} + \\text{rotate\\_half}(Q) \\cdot \\sin{\\theta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "[k_1, k_2] \\cdot \\cos{\\theta} + [-k_2, k_1] \\cdot \\sin{\\theta}\n",
    "= K \\cdot \\cos{\\theta} + \\text{rotate\\_half}(K) \\cdot \\sin{\\theta}\n",
    "$$GemmaRotaryEmbedding„ÅØ„ÄÅÂõûËª¢‰ΩçÁΩÆÂüã„ÇÅËæº„ÅøÔºàRotary Position Embedding: RoPEÔºâ„ÅÆÈÅ©Áî®„Å´ÂøÖË¶Å„Å™„Çµ„Ç§„É≥„Å®„Ç≥„Çµ„Ç§„É≥„ÅÆÂÄ§„ÇíË®àÁÆó„Åô„Çã„ÇØ„É©„Çπ\n",
    "\n",
    "RoPE„ÅØ„ÄÅ„Éà„Éº„ÇØ„É≥„ÅÆ‰ΩçÁΩÆ„Å´Âøú„Åò„Å¶„ÇØ„Ç®„É™„Å®„Ç≠„Éº„ÇíÂ§âË™ø„Åï„Åõ„Çã„Åì„Å®„Åß„ÄÅ‰ΩçÁΩÆÊÉÖÂ†±„ÇíËøΩÂä†„Åô„ÇãÊâãÊ≥ï\n",
    "\n",
    "„ÇØ„Ç®„É™„ÅÆÂ§âË™ø„ÅÆÊµÅ„Çå:\n",
    "\n",
    "1.  128ÂÄã„ÅÆË¶ÅÁ¥†„ÇíÊåÅ„Å§„ÇØ„Ç®„É™$Q$„Çí„ÄÅÂâçÂçä64ÂÄã$Q_1$„Å®ÂæåÂçä64ÂÄã„ÅÆ$Q_2$„Å´ÂàÜ„Åë„Çã\n",
    "2. $Q_1$„Å®$Q_2$„Åã„Çâ‰∏Ä„Å§„Åö„Å§Ë¶ÅÁ¥†„ÇíÂèñ„Çä„ÄÅ$(q1, q2)$„ÅÆ„Éö„Ç¢„Çí‰Ωú„ÇãÔºà=2Ê¨°ÂÖÉÂπ≥Èù¢„ÇíÊßãÊàêÔºâ\n",
    "3. „Éö„Ç¢„Å´ÂØæ„ÅóËßíÂ∫¶$\\theta$„ÅßÂõûËª¢„ÇíÈÅ©Áî®„Åô„Çã\n",
    "\n",
    "2Ê¨°ÂÖÉÂõûËª¢„ÅÆÂÖ¨Âºè:\n",
    "\n",
    "$$\n",
    "(q_1 \\cos{\\theta} - q_2 \\sin{\\theta}, q_2 \\cos{\\theta} + q_1 \\sin{\\theta})\n",
    "$$\n",
    "\n",
    "ÂõûËª¢„ÅÆËßíÂ∫¶:\n",
    "\n",
    "$$\n",
    "\\theta_{m, i} = m\\cdot b^{-\\frac{2i}{d}}\n",
    "$$\n",
    "\n",
    "- $m$: „Ç∑„Éº„Ç±„É≥„ÇπÂÜÖ„Åß„ÅÆ„Éà„Éº„ÇØ„É≥„ÅÆ‰ΩçÁΩÆÔºà$0, 1, 2,...$Ôºâ\n",
    "- $b^{-\\frac{2i}{d}}$: Âë®Ê≥¢Êï∞„ÅÆÈÄÜÊï∞\n",
    "    - $b$: Âü∫Êï∞Ôºà$10000$Ôºâ\n",
    "    - $i$: Ê¨°ÂÖÉ„Ç§„É≥„Éá„ÉÉ„ÇØ„ÇπÔºà$0, 1, 2, ..., d/2-1$Ôºâ\n",
    "    - $d$: „Éô„ÇØ„Éà„É´„ÅÆÁ∑èÊ¨°ÂÖÉÊï∞\n",
    "\n",
    "„Éà„Éº„ÇØ„É≥„ÅÆ‰ΩçÁΩÆ„ÅåÈÅ†„ÅÑÔºàÂõûËª¢ËßíÂ∫¶„ÅÆÂ∑Æ„ÅåÂ§ß„Åç„ÅÑÔºâ„Åª„Å©Âêë„Åç„ÅåÊèÉ„Çè„Å™„Åè„Å™„Çä„ÄÅ„ÇØ„Ç®„É™„Å®„Ç≠„Éº„ÅÆË°åÂàóÁ©ç„ÅØÂ∞è„Åï„Åè„Å™„Çã\n",
    "\n",
    "2Ê¨°ÂÖÉÂõûËª¢„ÅÆÂÆüË£Ö„ÅØ„ÄÅÂÖ¨Âºè„ÇíÂ±ïÈñã„Åó„Å¶ÂäπÁéáÁöÑ„Å´Ë®àÁÆó„ÇíË°å„ÅÜÔºàapply_rotate_pos_embÈñ¢Êï∞Ôºâ:\n",
    "\n",
    "$$\n",
    "(q_1 \\cos{\\theta} - q_2 \\sin{\\theta}, q_2 \\cos{\\theta} + q_1 \\sin{\\theta})\n",
    "= [q_1, q_2] \\cdot \\cos{\\theta} + [-q_2, q_1] \\cdot \\sin{\\theta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(k_1 \\cos{\\theta} - k_2 \\sin{\\theta}, k_2 \\cos{\\theta} + k_1 \\sin{\\theta})\n",
    "= [k_1, k_2] \\cdot \\cos{\\theta} + [-k_2, k_1] \\cdot \\sin{\\theta}\n",
    "$$\n",
    "\n",
    "$x$„Çí$[-x_2, x_1]$„Å´Â§âÊèõ„Åô„Çãrotate_halÈñ¢Êï∞„Çí‰ΩøÁî®„Åó„ÄÅÊõ¥„Å´ÂÆüË£Ö„ÇíÁ∞°Âçò„Å´„Åô„Çã:\n",
    "\n",
    "\n",
    "$$\n",
    "[q_1, q_2] \\cdot \\cos{\\theta} + [-q_2, q_1] \\cdot \\sin{\\theta}\n",
    "= Q \\cdot \\cos{\\theta} + \\text{rotate\\_half}(Q) \\cdot \\sin{\\theta}\n",
    "$$\n",
    "\n",
    "$$\n",
    "[k_1, k_2] \\cdot \\cos{\\theta} + [-k_2, k_1] \\cdot \\sin{\\theta}\n",
    "= K \\cdot \\cos{\\theta} + \\text{rotate\\_half}(K) \\cdot \\sin{\\theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralRotaryEmbedding(nn.Module):\n",
    "    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n",
    "\n",
    "    def __init__(self, config: MixtralConfig, device=None):\n",
    "        logger.info(f\"MixtralRotaryEmbedding„ÅÆÂàùÊúüÂåñÈñãÂßã {config.max_position_embeddings=}\")\n",
    "        super().__init__()\n",
    "\n",
    "        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n",
    "            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n",
    "        else:\n",
    "            self.rope_type = \"default\"\n",
    "\n",
    "        # 32768\n",
    "        self.max_seq_len_cached = config.max_position_embeddings\n",
    "\n",
    "        # 32768\n",
    "        self.original_max_seq_len = config.max_position_embeddings\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # default\n",
    "        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n",
    "        logger.debug(f\"{self.rope_type=}\")\n",
    "\n",
    "        # (64,), 1.0\n",
    "        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n",
    "        logger.debug(f\"{inv_freq.shape=}, {self.attention_scaling=}\")\n",
    "\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "        logger.info(\"MixtralRotaryEmbedding„ÅÆÂàùÊúüÂåñÂÆå‰∫Ü\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n",
    "    def forward(self, x, position_ids):\n",
    "        logger.info(f\"MixtralRotaryEmbedding„ÅÆÈ†Ü‰ºùÊí≠ÈñãÂßã {x.shape=} {position_ids.shape=}\")\n",
    "\n",
    "        # (1, 64, 1)\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
    "        logger.debug(f\"{inv_freq_expanded.shape=}\")\n",
    "\n",
    "        # (1, 1, 2)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        logger.debug(f\"{position_ids_expanded.shape=}\")\n",
    "\n",
    "        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n",
    "\n",
    "        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n",
    "            # Âë®Ê≥¢Êï∞„Å®‰ΩçÁΩÆID„ÅÆË°åÂàóÁ©ç„ÇíË®àÁÆó„ÅóÂõûËª¢ËßíÂ∫¶„ÇíÊ±Ç„ÇÅ„ÄÅËª¢ÁΩÆ\n",
    "            # (1, 64, 1) @ (1, 1, 2) -> (1, 64, 2) -> (1, 2, 64)\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            logger.debug(f\"{freqs.shape=}\")\n",
    "\n",
    "            # „Éö„Ç¢„ÅßÂêå„ÅòËßíÂ∫¶„Çí‰ΩøÁî®„Åô„Çã„Åü„ÇÅË§áË£Ω\n",
    "            # (1, 2, 128)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            logger.debug(f\"{emb.shape=}\")\n",
    "\n",
    "            # „Ç≥„Çµ„Ç§„É≥„ÇíË®àÁÆó\n",
    "            # (1, 2, 128)\n",
    "            cos = emb.cos() * self.attention_scaling\n",
    "\n",
    "            # „Çµ„Ç§„É≥„ÇíË®àÁÆó\n",
    "            # (1, 2, 128)\n",
    "            sin = emb.sin() * self.attention_scaling\n",
    "\n",
    "        logger.info(f\"MixtralRotaryEmbedding„ÅÆÈ†Ü‰ºùÊí≠ÂÆå‰∫Ü {cos.shape=} {sin.shape=}\")\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    logger.info(f\"rotate_half„ÅÆÈñãÂßã {x.shape=}\")\n",
    "\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    logger.debug(f\"„Éô„ÇØ„Éà„É´„ÅÆÂâçÂçä„ÇíÊäΩÂá∫ {x1.shape=}\")\n",
    "\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    logger.debug(f\"„Éô„ÇØ„Éà„É´„ÅÆÂæåÂçä„ÇíÊäΩÂá∫ {x2.shape=}\")\n",
    "\n",
    "    res = torch.cat((-x2, x1), dim=-1)\n",
    "    logger.debug(f\"ÂæåÂçä„ÇíË≤†„Å´„Åó„Å¶ÂâçÂçä„Å®ÈÄ£Áµê {res.shape=}\")\n",
    "\n",
    "    logger.info(f\"rotate_half„ÅÆÂÆå‰∫Ü {res.shape=}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7729e3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    logger.info(f\"apply_rotary_pos_emb„ÅÆÈñãÂßã {q.shape=} {k.shape=} {cos.shape=} {sin.shape=} {unsqueeze_dim=}\")\n",
    "\n",
    "    # (1, 2, 128) -> (1, 1, 2, 128)\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "\n",
    "    # (1, 2, 128) -> (1, 1, 2, 128)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "\n",
    "    # „ÇØ„Ç®„É™„Å´ÂõûËª¢‰ΩçÁΩÆÂüã„ÇÅËæº„Åø„ÇíÈÅ©Áî®\n",
    "    # (q1, q2) * cos + (-q2, q1) * sin\n",
    "    # (1, 32, 2, 128) * (1, 1, 2, 128) + (1, 32, 2, 128) * (1, 1, 2, 128) -> (1, 32, 2, 128)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "\n",
    "    # „Ç≠„Éº„Å´ÂõûËª¢‰ΩçÁΩÆÂüã„ÇÅËæº„Åø„ÇíÈÅ©Áî®\n",
    "    # (k1, k2) * cos + (-k2, k1) * sin\n",
    "    # (1, 8, 2, 128) * (1, 1, 2, 128) + (1, 8, 2, 128) * (1, 1, 2, 128) -> (1, 8, 2, 128)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "\n",
    "    logger.info(f\"apply_rotary_pos_emb„ÅÆÂÆå‰∫Ü {q_embed.shape=} {k_embed.shape=}\")\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c06577",
   "metadata": {},
   "source": [
    "### MixtralAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1033481",
   "metadata": {},
   "source": [
    "MixtralAttention„ÅØ„ÄÅGQAÔºàGrouped Query AttentionÔºâ„Å®RoPE„Å´ÂØæÂøú„Åó„ÅüPyTorch„ÅÆ[SDPA][1]„ÅÆ„É©„ÉÉ„Éë„Éº„ÇØ„É©„Çπ\n",
    "\n",
    "[1]: https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95727363",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: MixtralConfig, layer_idx: int):\n",
    "        logger.info(f\"MixtralAttention„ÅÆÂàùÊúüÂåñÈñãÂßã {config.hidden_size=}, {config.num_attention_heads=}, {config.num_key_value_heads=}, {getattr(config, 'head_dim', None) or config.hidden_size // config.num_attention_heads=}, {config.attention_dropout=}, {layer_idx=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # 0, 1, 2, ..., 31\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        # 4096 // 32 = 128\n",
    "        self.head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n",
    "        logger.debug(f\"{self.head_dim=}\")\n",
    "\n",
    "        # 32 // 8 = 4\n",
    "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
    "        logger.debug(f\"{self.num_key_value_groups=}\")\n",
    "\n",
    "        # 0.088388347\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        logger.debug(f\"{self.scaling=}\")\n",
    "\n",
    "        # 0.0\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        logger.debug(f\"{self.attention_dropout=}\")\n",
    "\n",
    "        self.is_causal = True\n",
    "\n",
    "        # 4096 -> 32 * 128 = 4096\n",
    "        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n",
    "\n",
    "        # 4096 -> 8 * 128 = 1024\n",
    "        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n",
    "\n",
    "        # 4096 -> 8 * 128 = 1024\n",
    "        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n",
    "\n",
    "        # 32 * 128 = 4096 -> 4096\n",
    "        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n",
    "        logger.info(\"MixtralAttention„ÅÆÂàùÊúüÂåñÂÆå‰∫Ü\")\n",
    "\n",
    "    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[FlashAttentionKwargs],\n",
    "    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        logger.info(f\"MixtralAttention„ÅÆÈ†Ü‰ºùÊí≠ÈñãÂßã {hidden_states.shape=} {attention_mask.shape if attention_mask is not None else None} {past_key_values is not None=} {cache_position.shape if cache_position is not None else None}\")\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2)\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, -1, 128)\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "        logger.debug(f\"{hidden_shape=}\")\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 8192) -> (1, 2, 32, 128) -> (1, 32, 2, 128)\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        logger.debug(f\"{query_states.shape=}\")\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 1024) -> (1, 2, 8, 128) -> (1, 8, 2, 128)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        logger.debug(f\"{key_states.shape=}\")\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 1024) -> (1, 2, 8, 128) -> (1, 8, 2, 128)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        logger.debug(f\"{value_states.shape=}\")\n",
    "\n",
    "        # (1, 2, 128), (1, 2, 128)\n",
    "        cos, sin = position_embeddings\n",
    "\n",
    "        # (1, 32, 2, 128), (1, 8, 2, 128)\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if past_key_values is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            # (1, 8, 2, 128), (1, 8, 2, 128)\n",
    "            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "            logger.debug(f\"ÈÅéÂéª„ÅÆ„Ç≠„Éº„Éª„Éê„É™„É•„Éº„ÇíÊõ¥Êñ∞ {key_states.shape=} {value_states.shape=}\")\n",
    "\n",
    "        # SDPA\n",
    "        attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
    "        logger.debug(f\"„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„ÅÆÂÆüË£Ö„ÇíÈÅ∏Êäû {attention_interface=}\")\n",
    "\n",
    "        logger.debug(f\"„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„ÇíË®àÁÆóÈñãÂßã {query_states.shape=} {key_states.shape=} {value_states.shape=} {attention_mask.shape if attention_mask is not None else None} {self.scaling=} sliding_window={getattr(self.config, 'sliding_window', None)} dropout={0.0 if not self.training else self.attention_dropout}\")\n",
    "\n",
    "        # (1, 32, 2, 128) -> (1, 32, 2, 128)\n",
    "        attn_output, attn_weights = attention_interface(\n",
    "            self,\n",
    "            query_states, # (1, 32, 2, 128)\n",
    "            key_states, # (1, 8, 2, 128)\n",
    "            value_states, # (1, 8, 2, 128)\n",
    "            attention_mask, # None\n",
    "            dropout=0.0 if not self.training else self.attention_dropout, # 0.0\n",
    "            scaling=self.scaling, # 0.088388347\n",
    "            sliding_window=getattr(self.config, \"sliding_window\", None), # None\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        logger.debug(f\"„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„ÇíË®àÁÆóÂÆå‰∫Ü {attn_output.shape=} {attn_weights.shape if attn_weights is not None else None}\")\n",
    "\n",
    "        # (1, 32, 2, 128) -> (1, 2, 4096)\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        logger.debug(f\"„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥Âá∫Âäõ„ÇíÊï¥ÂΩ¢ {attn_output.shape=}\")\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 4096)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        logger.debug(f\"Âá∫Âäõ„ÅÆ„Éó„É≠„Ç∏„Çß„ÇØ„Ç∑„Éß„É≥„ÇíÈÅ©Áî® {attn_output.shape=}\")\n",
    "\n",
    "        logger.info(f\"MixtralAttention„ÅÆÈ†Ü‰ºùÊí≠ÂÆå‰∫Ü {attn_output.shape=}\")\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f94fb11",
   "metadata": {},
   "source": [
    "### MixtralDecoderLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892814ff",
   "metadata": {},
   "source": [
    "MixtralDecoderLayer„ÅØ„ÄÅ„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„Éñ„É≠„ÉÉ„ÇØ„Å®SMoE„Éñ„É≠„ÉÉ„ÇØ„Åã„Çâ„Å™„ÇãTransformer„ÅÆ„Éá„Ç≥„Éº„ÉÄ„Éº„ÇØ„É©„Çπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7edf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralDecoderLayer(GradientCheckpointingLayer):\n",
    "    def __init__(self, config: MixtralConfig, layer_idx: int):\n",
    "        logger.info(f\"MixtralDecoderLayer„ÅÆÂàùÊúüÂåñÈñãÂßã {config.hidden_size=}, {config.num_attention_heads=}, {config.intermediate_size=}, {layer_idx=}\")\n",
    "        super().__init__()\n",
    "\n",
    "        # 4096\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        # „Çª„É´„Éï„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„ÇíÂàùÊúüÂåñ\n",
    "        self.self_attn = MixtralAttention(config, layer_idx)\n",
    "\n",
    "        # MoE„Éñ„É≠„ÉÉ„ÇØ„ÇíÂàùÊúüÂåñ\n",
    "        self.block_sparse_moe = MixtralSparseMoeBlock(config)\n",
    "\n",
    "        # „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„ÅÆÂâç„ÅÆ„É¨„Ç§„É§„ÉºÊ≠£Ë¶èÂåñ\n",
    "        self.input_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        # „Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„ÅÆÂæå„ÅÆ„É¨„Ç§„É§„ÉºÊ≠£Ë¶èÂåñ\n",
    "        self.post_attention_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        logger.info(\"MixtralDecoderLayer„ÅÆÂàùÊúüÂåñÂÆå‰∫Ü\")\n",
    "\n",
    "    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> torch.FloatTensor:\n",
    "        logger.info(f\"MixtralDecoderLayer„ÅÆÈ†Ü‰ºùÊí≠ÈñãÂßã {hidden_states.shape=} {attention_mask.shape if attention_mask is not None else None} {position_ids.shape if position_ids is not None else None} {past_key_values is not None=} {cache_position.shape if cache_position is not None else None}\")\n",
    "\n",
    "        ###################\n",
    "        # „Çª„É´„Éï„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥ #\n",
    "        ###################\n",
    "\n",
    "        # ÊÆãÂ∑ÆÊé•Á∂öÁî®„Å´„Ç≥„Éî„Éº\n",
    "        # (1, 2, 4096)\n",
    "        residual = hidden_states\n",
    "\n",
    "        # „É¨„Ç§„É§„ÉºÊ≠£Ë¶èÂåñ„ÇíÈÅ©Áî®\n",
    "        # (1, 2, 4096) -> (1, 2, 4096)\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # „Çª„É´„Éï„Ç¢„ÉÜ„É≥„Ç∑„Éß„É≥„ÇíÈÅ©Áî®\n",
    "        hidden_states, _ = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            position_embeddings=position_embeddings,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            cache_position=cache_position,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # ÊÆãÂ∑ÆÊé•Á∂ö„ÇíÈÅ©Áî®\n",
    "        # (1, 2, 4096) + (1, 2, 4096) -> (1, 2, 4096)\n",
    "        hidden_states = residual + hidden_states\n",
    "        logger.debug(f\"ÊÆãÂ∑ÆÊé•Á∂ö„ÇíÈÅ©Áî® {hidden_states.shape=}\")\n",
    "\n",
    "        ####################\n",
    "        # SMoE„Éñ„É≠„ÉÉ„ÇØ„ÇíÈÅ©Áî® #\n",
    "        ####################\n",
    "\n",
    "        # ÊÆãÂ∑ÆÊé•Á∂öÁî®„Å´„Ç≥„Éî„Éº\n",
    "        # (1, 2, 4096)\n",
    "        residual = hidden_states\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 4096)\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 4096)\n",
    "        hidden_states, _ = self.block_sparse_moe(hidden_states)\n",
    "\n",
    "        # (1, 2, 4096) + (1, 2, 4096) -> (1, 2, 4096)\n",
    "        hidden_states = residual + hidden_states\n",
    "        logger.debug(f\"ÊÆãÂ∑ÆÊé•Á∂ö„ÇíÈÅ©Áî® {hidden_states.shape=}\")\n",
    "\n",
    "        logger.info(f\"MixtralDecoderLayer„ÅÆÈ†Ü‰ºùÊí≠ÂÆå‰∫Ü {hidden_states.shape=}\")\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947bfdec",
   "metadata": {},
   "source": [
    "### MixtralPreTrainedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1267b",
   "metadata": {},
   "source": [
    "MixtralPreTrainedModel„ÅØ„ÄÅMixtralModel„ÅÆË®≠ÂÆö„ÇØ„É©„Çπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c321bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralPreTrainedModel(PreTrainedModel):\n",
    "    config: MixtralConfig\n",
    "    base_model_prefix = \"model\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _no_split_modules = [\"MixtralDecoderLayer\"]\n",
    "    _skip_keys_device_placement = [\"past_key_values\"]\n",
    "    _supports_flash_attn = True\n",
    "    _supports_sdpa = True\n",
    "    _supports_flex_attn = True\n",
    "    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n",
    "    _supports_attention_backend = True\n",
    "    _can_record_outputs = {\n",
    "        \"router_logits\": OutputRecorder(MixtralSparseMoeBlock, index=1),\n",
    "        \"hidden_states\": MixtralDecoderLayer,\n",
    "        \"attentions\": MixtralAttention,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3509154b",
   "metadata": {},
   "source": [
    "### MixtralModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8497398",
   "metadata": {},
   "source": [
    "MixtralModel„ÅØ„ÄÅ32Â±§„ÅÆ„Éá„Ç≥„Éº„ÉÄ„Éº„É¨„Ç§„É§„Éº„ÇíÂÆüË£Ö„Åó„Åü‰∏ä‰Ωç„ÇØ„É©„Çπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31654fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralModel(MixtralPreTrainedModel):\n",
    "    def __init__(self, config: MixtralConfig):\n",
    "        logger.info(f\"MixtralModel„ÅÆÂàùÊúüÂåñÈñãÂßã {config.vocab_size=}, {config.hidden_size=}, {config.num_hidden_layers=}, {config.pad_token_id=}\")\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.padding_idx = config.pad_token_id\n",
    "\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "\n",
    "        # 32Â±§„ÅÆ„Éá„Ç≥„Éº„ÉÄ„Éº„É¨„Ç§„É§„Éº„ÇíÂàùÊúüÂåñ\n",
    "        self.layers = nn.ModuleList(\n",
    "            [MixtralDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "\n",
    "        self.norm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        self.rotary_emb = MixtralRotaryEmbedding(config=config)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "        logger.info(\"MixtralModel„ÅÆÂàùÊúüÂåñÂÆå‰∫Ü\")\n",
    "\n",
    "    @check_model_inputs\n",
    "    @auto_docstring\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> MoeModelOutputWithPast:\n",
    "        logger.info(f\"MixtralModel„ÅÆÈ†Ü‰ºùÊí≠ÈñãÂßã {input_ids.shape=} {attention_mask.shape=} {position_ids.shape=} {past_key_values=} {inputs_embeds.shape if inputs_embeds is not None else None} {use_cache=} {cache_position=}\")\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "\n",
    "        if use_cache and past_key_values is None:\n",
    "            past_key_values = DynamicCache(config=self.config)\n",
    "            logger.debug(\"DynamicCache„ÇíÂàùÊúüÂåñ\")\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            # (1, 2, 4096)\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "            logger.debug(f\"ÂÖ•Âäõ„ÅÆÂüã„ÇÅËæº„Åø„ÇíÂèñÂæó {inputs_embeds.shape=}\")\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        # create_causal_mask\n",
    "        mask_function = create_causal_mask if self.config.sliding_window is None else create_sliding_window_causal_mask\n",
    "        logger.debug(f\"„Éû„Çπ„ÇØÈñ¢Êï∞„ÇíÈÅ∏Êäû {mask_function=}\")\n",
    "\n",
    "        # None\n",
    "        causal_mask = mask_function(\n",
    "            config=self.config,\n",
    "            input_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            cache_position=cache_position,\n",
    "            past_key_values=past_key_values,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "        logger.debug(f\"Âõ†Êûú„Éû„Çπ„ÇØ„Çí‰ΩúÊàê {causal_mask=}\")\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        # (1, 2, 128), (1, 2, 128)\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "        logger.debug(f\"‰ΩçÁΩÆÂüã„ÇÅËæº„Åø„Çí‰ΩúÊàê {position_embeddings[0].shape=} {position_embeddings[1].shape=}\")\n",
    "\n",
    "        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n",
    "            hidden_states = decoder_layer(\n",
    "                hidden_states,\n",
    "                position_embeddings=position_embeddings,\n",
    "                attention_mask=causal_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 4096)\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        res = MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=past_key_values,\n",
    "        )\n",
    "        logger.info(f\"MixtralModel„ÅÆÈ†Ü‰ºùÊí≠ÂÆå‰∫Ü {hidden_states.shape=}\")\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb9c6c",
   "metadata": {},
   "source": [
    "### MixtralForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f2df93",
   "metadata": {},
   "source": [
    "MixtralForCausalLM„ÅØ„ÄÅÂá∫Âäõ„ÅÆÊ¨°ÂÖÉÊï∞Ôºà4096Ôºâ„ÇíË™ûÂΩô„ÅÆ„É≠„Ç∏„ÉÉ„ÉàÔºà32000Ôºâ„Å´Â∞ÑÂΩ±„Åô„ÇãÊúÄ‰∏ä‰Ωç„ÇØ„É©„Çπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtralForCausalLM(MixtralPreTrainedModel, GenerationMixin):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n",
    "    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n",
    "\n",
    "    def __init__(self, config):\n",
    "        logger.info(\"MixtralForCausalLM„ÅÆÂàùÊúüÂåñÈñãÂßã\")\n",
    "        super().__init__(config)\n",
    "        self.model = MixtralModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.router_aux_loss_coef = config.router_aux_loss_coef\n",
    "        self.num_experts = config.num_local_experts\n",
    "        self.num_experts_per_tok = config.num_experts_per_tok\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "        logger.info(\"MixtralForCausalLM„ÅÆÂàùÊúüÂåñÂÆå‰∫Ü\")\n",
    "\n",
    "    @can_return_tuple\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_router_logits: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> MoeCausalLMOutputWithPast:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, MixtralForCausalLM\n",
    "\n",
    "        >>> model = MixtralForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "\n",
    "        logger.info(f\"MixtralForCausalLM„ÅÆÈ†Ü‰ºùÊí≠ÈñãÂßã {input_ids.shape if input_ids is not None else None} {attention_mask.shape if attention_mask is not None else None} {position_ids.shape if position_ids is not None else None} {past_key_values is not None=} {inputs_embeds.shape if inputs_embeds is not None else None} {labels.shape if labels is not None else None} {use_cache=} {output_router_logits=} {cache_position.shape if cache_position is not None else None} {logits_to_keep=}\")\n",
    "\n",
    "        output_router_logits = (\n",
    "            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n",
    "        )\n",
    "        logger.debug(f\"{output_router_logits=}\")\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs: MoeModelOutputWithPast = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_router_logits=output_router_logits,\n",
    "            cache_position=cache_position,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
    "        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
    "\n",
    "        # (1, 2, 4096) -> (1, 2, 32000)\n",
    "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n",
    "\n",
    "        aux_loss = None\n",
    "        if output_router_logits:\n",
    "            aux_loss = load_balancing_loss_func(\n",
    "                outputs.router_logits,\n",
    "                self.num_experts,\n",
    "                self.num_experts_per_tok,\n",
    "                attention_mask,\n",
    "            )\n",
    "            if labels is not None:\n",
    "                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n",
    "\n",
    "        res = MoeCausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            aux_loss=aux_loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            router_logits=outputs.router_logits,\n",
    "        )\n",
    "        logger.info(f\"MixtralForCausalLM„ÅÆÈ†Ü‰ºùÊí≠ÂÆå‰∫Ü {logits.shape=}\")\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b9bd30",
   "metadata": {},
   "source": [
    "### Êé®Ë´ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67238978",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fffc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MixtralForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\", dtype=torch.bfloat16, load_in_4bit=True, device_map=\"auto\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaef015",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"Hello\"\n",
    "logger.info(f\"ÂÖ•Âäõ„Éó„É≠„É≥„Éó„Éà {text=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df13cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd48eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**model_inputs, max_new_tokens=1)\n",
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c147da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "logger.info(f\"ÁîüÊàê„Åï„Çå„Åü„ÉÜ„Ç≠„Çπ„Éà {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cf29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.WARN)\n",
    "text= \"Fukuoka is\"\n",
    "model_inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
